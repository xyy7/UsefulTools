@article{20225113283452 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {An unsupervised learning algorithm for computer vision-based blind modal parameters identification of output-only structures from video measurements},
journal = {Structure and Infrastructure Engineering},
author = {Allada, Vishal and Saravanan, T. Jothi},
year = {2022},
issn = {15732479},
abstract = {<div data-language="eng" data-ev-field="abstract">Operational modal analysis (OMA) is required to maintain large-scale and necessary civil infrastructures. The non-contact method of OMA using digital image correlation and point tracking algorithms requires a speckle pattern placed on the structure. Alternatively, advanced computer vision methods like optical flow and phase-based video motion magnification (PBVMM) techniques are used to measure modal parameters. Despite the importance of PBVMM, the users should know the range of frequencies in which the natural structure frequency lies. A methodology based on an unsupervised machine learning technique is developed to extract the modal parameters blindly from its recorded digital video. The proposed methodology uses complex steerable pyramids and an unsupervised machine learning technique, also known as principal component analysis, and analytical mode decomposition with a random decrement technique to blindly extract the modal parameters of a structure. This study validated the proposed methodology using a multi-degree of freedom (DOF) numerical model. The results are compared with theoretical and estimated values and are in good agreement. Furthermore, it is implemented on a laboratory benchmark SDOF, MDOF, and real-time videos of the London Millennium and Tacoma Narrows bridges for blindly extracting the modal frequencies and damping ratios.<br/></div> © 2022 Informa UK Limited, trading as Taylor & Francis Group.},
key = {Degrees of freedom (mechanics)},
keywords = {Computer graphics;Computer vision;Damping;Learning algorithms;Machine learning;Modal analysis;Multimedia systems;Speckle;Ultrasonic devices;Vibration analysis;Vibration measurement;},
note = {Damping ratio;Images processing;Machine-learning;Modal frequency;Modal parameters;Operational modal analysis;Phase based;Principal-component analysis;Unsupervised machine learning;Video motion;},
URL = {http://dx.doi.org/10.1080/15732479.2022.2157844},
} 


@unpublished{20230115027 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Robust Multi-Modal Pedestrian Detection Using Deep Convolutional Neural Network with Ensemble Learning Model},
journal = {SSRN},
author = {Jain, Deepak Kumar and Zhao, Xudong and Garcia, Salvador and Neelakandan, Subramani},
year = {2023},
issn = {15565068},
abstract = {<div data-language="eng" data-ev-field="abstract">In the realm of video surveillance systems, pedestrian detection (PD) is a crucial task that enables the identification and tracking of individuals, crowd monitoring, and people counting. However, accurate PD is a challenging task due to variations in the appearance and pose of pedestrians. With the advent of deep learning (DL) techniques, particularly convolutional neural networks (CNNs), computer vision (CV) methods for object classification and detection have been revolutionized. Successful PD depends on supervised learning, which necessitates large labeled datasets. This paper proposes a Robust Multi-modal Pedestrian Detection using a Deep Convolutional Neural Network with an Ensemble Learning (RMPD-DCNNEL) model to address the challenges of PD. This model leverages computer vision and ensemble learning to achieve accurate pedestrian detection. The SimAM-EfficientNet model with a stochastic gradient descent (SGD) optimizer is utilized for feature extraction, while ensemble classification is performed using three DL models: nested long short-term memory (NLSTM), deep belief networks (DBNs), and extreme learning machine (ELM).To evaluate the effectiveness of the proposed approach, we use the INRIA dataset, and the experimental results demonstrate the superior performance of RMPD-DCNNEL over other DL approaches. The proposed technique enables robust pedestrian detection despite variations in appearance and pose, making it an effective solution for video surveillance systems in smart transportation environments.<br/></div> © 2023, The Authors. All rights reserved.},
key = {Learning systems},
keywords = {Computer vision;Convolution;Convolutional neural networks;Deep neural networks;Gradient methods;Large dataset;Object detection;Security systems;Stochastic models;Stochastic systems;},
note = {Convolutional neural network;Deep learning;Ensemble learning;Learning models;Learning techniques;Multi-modal;Pedestrian detection;People counting;Video surveillance;Video surveillance systems;},
URL = {http://dx.doi.org/10.2139/ssrn.4411855},
} 


@article{20241015665694 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Robust multi-modal pedestrian detection using deep convolutional neural network with ensemble learning model},
journal = {Expert Systems with Applications},
author = {Kumar Jain, Deepak and Zhao, Xudong and Garcia, Salvador and Neelakandan, Subramani},
volume = {249},
year = {2024},
issn = {09574174},
abstract = {<div data-language="eng" data-ev-field="abstract">In the realm of video surveillance systems, pedestrian detection (PD) is a crucial task that enables the identification and tracking of individuals, crowd monitoring, and people counting. However, accurate PD is a challenging task due to variations in the appearance and pose of pedestrians. With the advent of deep learning (DL) techniques, particularly convolutional neural networks (CNNs), computer vision (CV) methods for object classification and detection have been revolutionized. Effective PD depends on supervised learning, which requires large labeled datasets. This paper, we present pedestrian detection (PD) using a Robust Multi-modal Pedestrian Detection using a Deep Convolutional Neural Network with an Ensemble Learning (RMPD-DCNN-EL) model. This model effects computer vision and ensemble learning to accomplish precise pedestrian detection. The SimAM EfficientNet model with a stochastic gradient descent (SGD) optimizer is used for feature extraction, while ensemble classification is performed using three DL models are Nested Long Short-Term Memory (NLSTM), deep belief networks (DBNs), and extreme learning machine (ELM). To evaluate the effectiveness of tettapproach, we use the INRIA dataset, and the experimental results prove the greater performance of RMPD-DCNN-EL over other DL methods. The proposed method enables robust pedestrian detection despite modifications in arrival and pose, making it an operative solution for video surveillance systems in smart transport environments. Experimental results show that the detection accuracy is 99.30% of the proposed algorithm is considerably higher than that of the traditional convolutional neural network algorithm.<br/></div> © 2024 Elsevier Ltd},
key = {Learning systems},
keywords = {Computer vision;Convolution;Convolutional neural networks;Deep neural networks;Gradient methods;Large datasets;Object detection;Security systems;Stochastic models;Stochastic systems;},
note = {Convolutional neural network;Deep learning;Ensemble learning;Learning models;Learning techniques;Multi-modal;Pedestrian detection;People counting;Video surveillance;Video surveillance systems;},
URL = {http://dx.doi.org/10.1016/j.eswa.2024.123527},
} 


@inproceedings{20230513471686 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Data Augmentation for RFID-based 3D Human Pose Tracking},
journal = {IEEE Vehicular Technology Conference},
author = {Wang, Ziqi and Yang, Chao and Mao, Shiwen},
volume = {2022-September},
year = {2022},
issn = {15502252},
address = {London, United kingdom},
abstract = {<div data-language="eng" data-ev-field="abstract">Interest in Radio Frequency (RF) based 3D human pose tracking has skyrocketed in the age of Artificial Intelligence of Things (AIoT). Compared to Computer Vision (CV) based methods, RF-based approaches are more resilient to lighting and non-line-of-sight conditions, and can better preserve user privacy. However, the majority of the current RF-based methods rely on a vision-aided multi-modal learning approach. An extensive amount of paired training data, i.e., Radio-Frequency Identification (RFID) data and vision data, must be collected, to achieve an adequate performance with the supervised-learning network. In order to mitigate such time-consuming and costly tasks, we propose a data augmentation method based on Generative Adversarial Network (GAN), named RFPose-GAN, to generate synthesized RFID data to alleviate the complications of using commodity RFID tags and receivers. In this paper, a forward kinematic layer is incorporated to generate simulated vision pose data, thus eliminating the need of using a Kinect 2.0 device in RFPose-GAN. Experiments conducted demonstrate that the synthesized data achieves accurate pose estimation performance.<br/></div> © 2022 IEEE.},
key = {Radio waves},
keywords = {Computer vision;Generative adversarial networks;Gesture recognition;Learning systems;Radio frequency identification (RFID);},
note = {Condition;Data augmentation;Frequency-based approaches;Human pose tracking;Identification data;Nonline of sight;Radio-frequency-identification;Radiofrequencies;Synthesised;Vision-based methods;},
URL = {http://dx.doi.org/10.1109/VTC2022-Fall57202.2022.10013052},
} 


@inproceedings{20240615503495 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the 22nd International Conference on Image Analysis and Processing, ICIAP 2023},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {14365 LNCS},
year = {2024},
issn = {03029743},
address = {Udine, Italy},
abstract = {<div data-language="eng" data-ev-field="abstract">The proceedings contain 92 papers. The special focus in this conference is on Image Analysis and Processing. The topics include: An Effective CNN-Based Super Resolution Method for Video Coding; medical Transformers for Boosting Automatic Grading of Colon Carcinoma in Histological Images; FERMOUTH: Facial Emotion Recognition from the MOUTH Region; consensus Ranking for Efficient Face Image Retrieval: A Novel Method for Maximising Precision and Recall; towards Explainable Navigation and Recounting; towards Facial Expression Robustness in Multi-scale Wild Environments; depth Camera Face Recognition by Normalized Fractal Encodings; automatic Generation of Semantic Parts for Face Image Synthesis; improved Bilinear Pooling for Real-Time Pose Event Camera Relocalisation; continual Source-Free Unsupervised Domain Adaptation; End-to-End Asbestos Roof Detection on Orthophotos Using Transformer-Based YOLO Deep Neural Network; OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data; UAV Multi-object Tracking by Combining Two Deep Neural Architectures; GLR: Gradient-Based Learning Rate Scheduler; a Large-scale Analysis of Athletes’ Cumulative Race Time in Running Events; uncovering Lies: Deception Detection in a Rolling-Dice Experiment; active Class Selection for Dataset Acquisition in Sign Language Recognition; MC-GTA: A Synthetic Benchmark for Multi-Camera Vehicle Tracking; a Differentiable Entropy Model for Learned Image Compression; learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation; self-Similarity Block for Deep Image Denoising; SCENE-pathy: Capturing the Visual Selective Attention of People Towards Scene Elements; not with My Name! Inferring Artists’ Names of Input Strings Employed by Diffusion Models; benchmarking of Blind Video Deblurring Methods on Long Exposure and Resource Poor Settings; LieToMe: An LSTM-Based Method for Deception Detection by Hand Movements; spatial Transformer Generative Adversarial Network for Image Super-Resolution; Real-Time GAN-Based Model for Underwater Image Enhancement; HERO: A Multi-modal Approach on Mobile Devices for Visual-Aware Conversational Assistance in Industrial Domains; a Computer Vision-Based Water Level Monitoring System for Touchless and Sustainable Water Dispensing.<br/></div>},
} 


@article{20230513514829 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {3D Object Detection and Tracking for Autonomous Vehicles},
journal = {ProQuest Dissertations and Theses Global},
author = {Pang, Su},
year = {2022},
abstract = {<div data-language="eng" data-ev-field="abstract">Autonomous driving systems require accurate 3D object detection and tracking to achieve reliable path planning and navigation. For object detection, there have been significant advances in neural networks for single-modality approaches. However, it has been surprisingly difficult to train networks to use multiple modalities in a way that demonstrates gain over single-modality networks. In this dissertation, we first propose three networks for Camera-LiDAR and Camera-Radar fusion. For Camera-LiDAR fusion, CLOCs (Camera-LiDAR Object Candidates fusion) and Fast-CLOCs are presented. CLOCs fusion provides a multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate 3D detection results. Fast-CLOCs can run in near real-time with less computational requirements compared to CLOCs. Fast-CLOCs eliminates the separate heavy 2D detector, and instead uses a 3D detector-cued 2D image detector (3D-Q-2D) to reduce memory and computation. For Camera-Radar fusion, we propose TransCAR, a Transformer-based Camera-And-Radar fusion solution for 3D object detection. The cross-attention layer within the transformer decoder can adaptively learn the soft-association between the radar features and vision queries instead of hard-association based on sensor calibration only. Then, we propose to solve the 3D multiple object tracking (MOT) problem for autonomous driving applications using a random finite set-based (RFS) Multiple Measurement Models filter (RFS-M3). In particular, we propose multiple measurement models for a Poisson multi-Bernoulli mixture (PMBM) filter in support of different application scenarios. Our RFS-M3 filter can naturally model these uncertainties accurately and elegantly. We combine learning-based detections with our RFS-M3 tracker by incorporating the detection confidence score into the PMBM prediction and update step. We have evaluated our CLOCs, Fast-CLOCs and TransCAR fusion-based 3D detector and RFS-M3 3D tracker using challenging datasets including KITTI, nuScenes, Argoverse and Waymo that are released by academia and industry leaders. Superior experimental results demonstrated the effectiveness of the proposed approaches. ProQuest Subject Headings: Electrical engineering.<br/></div>  © Citation reproduced with permission of ProQuest LLC.},
key = {Cameras},
keywords = {Motion planning;Object detection;Object recognition;Optical radar;Semantics;Three dimensional computer graphics;},
note = {3-D detectors;3D object;Autonomous driving;Fast cameras;Measurement model;Multi-Bernoulli;Multiple measurements;Object detection and tracking;Objects detection;Random finite sets;},
} 


@inproceedings{20233914809341 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the 22nd International Conference on Image Analysis and Processing, ICIAP 2023},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {14233 LNCS},
year = {2023},
issn = {03029743},
address = {Udine, Italy},
abstract = {<div data-language="eng" data-ev-field="abstract">The proceedings contain 92 papers. The special focus in this conference is on Image Analysis and Processing. The topics include: An Effective CNN-Based Super Resolution Method for Video Coding; medical Transformers for Boosting Automatic Grading of Colon Carcinoma in Histological Images; FERMOUTH: Facial Emotion Recognition from the MOUTH Region; consensus Ranking for Efficient Face Image Retrieval: A Novel Method for Maximising Precision and Recall; towards Explainable Navigation and Recounting; towards Facial Expression Robustness in Multi-scale Wild Environments; depth Camera Face Recognition by Normalized Fractal Encodings; automatic Generation of Semantic Parts for Face Image Synthesis; improved Bilinear Pooling for Real-Time Pose Event Camera Relocalisation; continual Source-Free Unsupervised Domain Adaptation; End-to-End Asbestos Roof Detection on Orthophotos Using Transformer-Based YOLO Deep Neural Network; OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data; UAV Multi-object Tracking by Combining Two Deep Neural Architectures; GLR: Gradient-Based Learning Rate Scheduler; a Large-scale Analysis of Athletes’ Cumulative Race Time in Running Events; uncovering Lies: Deception Detection in a Rolling-Dice Experiment; active Class Selection for Dataset Acquisition in Sign Language Recognition; MC-GTA: A Synthetic Benchmark for Multi-Camera Vehicle Tracking; a Differentiable Entropy Model for Learned Image Compression; learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation; self-Similarity Block for Deep Image Denoising; SCENE-pathy: Capturing the Visual Selective Attention of People Towards Scene Elements; not with My Name! Inferring Artists’ Names of Input Strings Employed by Diffusion Models; benchmarking of Blind Video Deblurring Methods on Long Exposure and Resource Poor Settings; LieToMe: An LSTM-Based Method for Deception Detection by Hand Movements; spatial Transformer Generative Adversarial Network for Image Super-Resolution; Real-Time GAN-Based Model for Underwater Image Enhancement; HERO: A Multi-modal Approach on Mobile Devices for Visual-Aware Conversational Assistance in Industrial Domains; a Computer Vision-Based Water Level Monitoring System for Touchless and Sustainable Water Dispensing.<br/></div>},
} 


@inproceedings{20234114863219 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Proceedings of the 22nd International Conference on Image Analysis and Processing, ICIAP 2023},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
volume = {14234 LNCS},
year = {2023},
issn = {03029743},
address = {Udine, Italy},
abstract = {<div data-language="eng" data-ev-field="abstract">The proceedings contain 92 papers. The special focus in this conference is on Image Analysis and Processing. The topics include: An Effective CNN-Based Super Resolution Method for Video Coding; medical Transformers for Boosting Automatic Grading of Colon Carcinoma in Histological Images; FERMOUTH: Facial Emotion Recognition from the MOUTH Region; consensus Ranking for Efficient Face Image Retrieval: A Novel Method for Maximising Precision and Recall; towards Explainable Navigation and Recounting; towards Facial Expression Robustness in Multi-scale Wild Environments; depth Camera Face Recognition by Normalized Fractal Encodings; automatic Generation of Semantic Parts for Face Image Synthesis; improved Bilinear Pooling for Real-Time Pose Event Camera Relocalisation; continual Source-Free Unsupervised Domain Adaptation; End-to-End Asbestos Roof Detection on Orthophotos Using Transformer-Based YOLO Deep Neural Network; OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data; UAV Multi-object Tracking by Combining Two Deep Neural Architectures; GLR: Gradient-Based Learning Rate Scheduler; a Large-scale Analysis of Athletes’ Cumulative Race Time in Running Events; uncovering Lies: Deception Detection in a Rolling-Dice Experiment; active Class Selection for Dataset Acquisition in Sign Language Recognition; MC-GTA: A Synthetic Benchmark for Multi-Camera Vehicle Tracking; a Differentiable Entropy Model for Learned Image Compression; learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation; self-Similarity Block for Deep Image Denoising; SCENE-pathy: Capturing the Visual Selective Attention of People Towards Scene Elements; not with My Name! Inferring Artists’ Names of Input Strings Employed by Diffusion Models; benchmarking of Blind Video Deblurring Methods on Long Exposure and Resource Poor Settings; LieToMe: An LSTM-Based Method for Deception Detection by Hand Movements; spatial Transformer Generative Adversarial Network for Image Super-Resolution; Real-Time GAN-Based Model for Underwater Image Enhancement; HERO: A Multi-modal Approach on Mobile Devices for Visual-Aware Conversational Assistance in Industrial Domains; a Computer Vision-Based Water Level Monitoring System for Touchless and Sustainable Water Dispensing.<br/></div>},
} 


@article{20233514663839 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {How Good is Google Bard’s Visual Understanding? An Empirical Study on Open Challenges},
journal = {Machine Intelligence Research},
author = {Qin, Haotong and Ji, Ge-Peng and Khan, Salman and Fan, Deng-Ping and Khan, Fahad Shahbaz and Gool, Luc Van},
volume = {20},
number = {5},
year = {2023},
pages = {605 - 613},
issn = {2731538X},
abstract = {<div data-language="eng" data-ev-field="abstract">Google’s Bard has emerged as a formidable competitor to OpenAI’s ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard’s impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard’s performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data. Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand .<br/></div> © 2023, Institute of Automation, Chinese Academy of Sciences and Springer-Verlag GmbH Germany, part of Springer Nature.},
key = {Remote sensing},
keywords = {Visual languages;},
note = {Chatbots;Conversational AI;Empirical studies;Google bard;Google+;Language model;Large language model;Multi-modal;Multi-modal understanding;Visual comprehension;},
URL = {http://dx.doi.org/10.1007/s11633-023-1469-x},
} 


@article{20240515455977 ,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Survey on autonomous navigation systems of bionic flapping-wing flying robot},
title = {仿生扑翼飞行机器人自主导航系统研究进展},
journal = {Yi Qi Yi Biao Xue Bao/Chinese Journal of Scientific Instrument},
author = {Jiang, Jizhou and Xu, Wenfu and Pan, Erzhen},
volume = {44},
number = {11},
year = {2023},
pages = {66 - 84},
issn = {02543087},
abstract = {<div data-language="eng" data-ev-field="abstract">Bio-inspired Flapping-Wing Flying Robot (FWFR) mimicking the flight mechanism of animals like birds and insects in nature has the advantages of high safety and good disorientation, which has extremely significant application prospects in disaster rescue, anti-riot and other aspects. In order to realize autonomous flight and perform detection tasks in unknown complex environments, a navigation system integrating functional modules such as perception and mapping, planning and obstacle avoidance, and flight control guidance, etc. need to be developed. This paper firstly gives an overview of the FWFR′s navigation system by classifying the robot into two categories of micro and medium-large according to the wingspan size, and then compares and analyzes the main research progresses in the navigation system of both FWFRs at home and abroad. Then, the key technologies involved in autonomous navigation systems such as perceptual localization and mapping, planning and obstacle avoidance, trajectory tracking and flight control guidance, etc. are analyzed and reviewed, the limitations and urgent problems are also pointed out. Finally, an outlook on future research trends is provided, including enhancing onboard embedded computing capabilities, multi-modal information fusion, multi-modal flight flow field perception, research of biomimetic vision systems, and swarm perception and formation control etc.<br/></div> © 2023 Science Press. All rights reserved.},
key = {Mapping},
keywords = {Biomimetics;Embedded systems;Navigation systems;Robot programming;Wings;},
note = {Autonomous navigation systems;Flapping-wing;Flapping-wing flying robot;Flight control;Flight control and guidance;Flying robots;High safety;Obstacles avoidance;Perception and mapping;Planning and obstacle avoidance;},
URL = {http://dx.doi.org/10.19650/j.cnki.cjsi.J2311124},
} 


@article{20235015193823 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {A Data-Driven Framework for Direct Local Tensile Property Prediction of Laser Powder Bed Fusion Parts},
journal = {Materials},
author = {Scime, Luke and Joslin, Chase and Collins, David A. and Sprayberry, Michael and Singh, Alka and Halsey, William and Duncan, Ryan and Snow, Zackary and Dehoff, Ryan and Paquit, Vincent},
volume = {16},
number = {23},
year = {2023},
issn = {19961944},
abstract = {<div data-language="eng" data-ev-field="abstract">This article proposes a generalizable, data-driven framework for qualifying laser powder bed fusion additively manufactured parts using part-specific in situ data, including powder bed imaging, machine health sensors, and laser scan paths. To achieve part qualification without relying solely on statistical processes or feedstock control, a sequence of machine learning models was trained on 6299 tensile specimens to locally predict the tensile properties of stainless-steel parts based on fused multi-modal in situ sensor data and a priori information. A cyberphysical infrastructure enabled the robust spatial tracking of individual specimens, and computer vision techniques registered the ground truth tensile measurements to the in situ data. The co-registered 230 GB dataset used in this work has been publicly released and is available as a set of HDF5 files. The extensive training data requirements and wide range of size scales were addressed by combining deep learning, machine learning, and feature engineering algorithms in a relay. The trained models demonstrated a 61% error reduction in ultimate tensile strength predictions relative to estimates made without any in situ information. Lessons learned and potential improvements to the sensors and mechanical testing procedure are discussed.<br/></div> © 2023 by the authors.},
key = {Mechanical testing},
keywords = {Deep learning;Forecasting;Learning systems;Tensile strength;},
note = {Data driven;Imaging machines;In-situ data;In-situ monitoring;Laser powder bed fusion;Laser powders;Machine health;Machine-learning;Powder bed;Property predictions;},
URL = {http://dx.doi.org/10.3390/ma16237293},
} 


@unpublished{20230083306 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {UAV Tracking with Lidar as a Camera Sensor in GNSS-Denied Environments},
journal = {arXiv},
author = {Sier, Ha and Yu, Xianjia and Catalano, Iacopo and Queralta, Jorge Pena and Zou, Zhuo and Westerlund, Tomi},
year = {2023},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">Light detection and ranging (LiDAR) sensor has become one of the primary sensors in robotics and autonomous system for high-accuracy situational awareness. In recent years, multi-modal LiDAR systems emerged, and among them, LiDAR-as-a-camera sensors provide not only 3D point clouds but also fixed-resolution 360°panoramic images by encoding either depth, reflectivity, or near-infrared light in the image pixels. This potentially brings computer vision capabilities on top of the potential of LiDAR itself. In this paper, we are specifically interested in utilizing LiDARs and LiDAR-generated images for tracking Unmanned Aerial Vehicles (UAVs) in real-time which can benefit applications including docking, remote identification, or counter-UAV systems, among others. This is, to the best of our knowledge, the first work that explores the possibility of fusing the images and point cloud generated by a single LiDAR sensor to track a UAV without a priori known initialized position. We trained a custom YOLOv5 model for detecting UAVs based on the panoramic images collected in an indoor experiment arena with a motion capture (MOCAP) system. By integrating with the point cloud, we are able to continuously provide the position of the UAV. Our experiment demonstrated the effectiveness of the proposed UAV tracking approach compared with methods based only on point clouds or images. Additionally, we evaluated the real-time performance of our approach on the Nvidia Jetson Nano, a popular mobile computing platform.<br/></div> Copyright © 2023, The Authors. All rights reserved.},
key = {Cameras},
keywords = {Aircraft detection;Antennas;Drones;Infrared devices;Optical radar;Real time systems;},
note = {Aerial vehicle;Drone tracking;Light detection and ranging;Light detection and ranging detection;Light detection and ranging tracking;Light detection and ranging-as-a-camera;Unmanned aerial vehicle;YOLOV5;},
URL = {http://dx.doi.org/10.48550/arXiv.2303.00277},
} 


@inproceedings{20232714358154 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {UAV Tracking with Lidar as a Camera Sensor in GNSS-Denied Environments},
journal = {2023 International Conference on Localization and GNSS, ICL-GNSS 2023 - Proceedings},
author = {Sier, Ha and Yu, Xianjia and Catalano, Iacopo and Queralta, Jorge Pena and Zou, Zhuo and Westerlund, Tomi},
year = {2023},
pages = {Tampere University (TAU) - },
address = {Castellon, Spain},
abstract = {<div data-language="eng" data-ev-field="abstract">Light detection and ranging (LiDAR) sensor has become one of the primary sensors in robotics and autonomous system for high-accuracy situational awareness. In recent years, multi-modal LiDAR systems emerged, and among them, LiDAR-as-a-camera sensors provide not only 3D point clouds but also fixed-resolution 360°panoramic images by encoding either depth, reflectivity, or near-infrared light in the image pixels. This potentially brings computer vision capabilities on top of the potential of LiDAR itself. In this paper, we are specifically interested in utilizing LiDARs and LiDAR-generated images for tracking Unmanned Aerial Vehicles (UAVs) in real-time which can benefit applications including docking, remote identification, or counter-UAV systems, among others. This is, to the best of our knowledge, the first work that explores the possibility of fusing the images and point cloud generated by a single LiDAR sensor to track a UAV without a priori known initialized position. We trained a custom YOLOv5 model for detecting UAVs based on the panoramic images collected in an indoor experiment arena with a motion capture (MOCAP) system. By integrating with the point cloud, we are able to continuously provide the position of the UAV. Our experiment demonstrated the effectiveness of the proposed UAV tracking approach compared with methods based only on point clouds or images. Additionally, we evaluated the real-time performance of our approach on the Nvidia Jetson Nano, a popular mobile computing platform.<br/></div> © 2023 IEEE.},
key = {Cameras},
keywords = {Aircraft detection;Antennas;Drones;Infrared devices;Optical radar;Real time systems;},
note = {Aerial vehicle;Drone tracking;Light detection and ranging;Light detection and ranging detection;Light detection and ranging tracking;Light detection and ranging-as-a-camera;Unmanned aerial vehicle;YOLOV5;},
URL = {http://dx.doi.org/10.1109/ICL-GNSS57829.2023.10148919},
} 


@inproceedings{20233714730988 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Appearance Label Balanced Triplet Loss for Multi-modal Aerial View Object Classification},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
author = {Puttagunta, Raghunath Sai and Li, Zhu and Bhattacharyya, Shuvra and York, George},
volume = {2023-June},
year = {2023},
pages = {534 - 542},
issn = {21607508},
address = {Vancouver, BC, Canada},
abstract = {<div data-language="eng" data-ev-field="abstract">Automatic target recognition (ATR) using image data is an important computer vision task with widespread applications in remote sensing for surveillance, object tracking, urban planning, agriculture, and more. Although there have been continuous advancements in this task, there is still significant room for further advancements, particularly with aerial images. This work extracts rich information from multimodal synthetic aperture radar (SAR) and electro-optical (EO) aerial images to perform object classification.Compared to EO images, the advantages of SAR images are that they can be captured at night and in any weather condition. Compared to EO images, the disadvantage of SAR images is that they are noisy. Overcoming the noise inherent to SAR images is a challenging, but worthwhile, task because of the additional information SAR images provide the model.This work proposes a training strategy that involves the creation of appearance labels to generate triplet pairs for training the network with both triplet loss and cross-entropy loss. During the development phase of the 2023 Perception Beyond Visual Spectrum (PBVS) Multi-modal Aerial Image Object Classification (MAVOC) challenge, our ResNet-34 model achieved a top-1 accuracy of 64.29% for Track 1 and our ensemble learning model achieved a top-1 accuracy 75.84% for Track 2. These values are 542% and 247% higher than the baseline values. Overall, this work ranked 3rd in both Track 1 and Track 2.<br/></div> © 2023 IEEE.},
key = {Automatic target recognition},
keywords = {Antennas;Classification (of information);Optical remote sensing;Radar imaging;Radar target recognition;Synthetic aperture radar;},
note = {Aerial images;Condition;Electro-optical;Electro-optical images;Image data;Multi-modal;Object classification;Object Tracking;Remote-sensing;Synthetic aperture radar images;},
URL = {http://dx.doi.org/10.1109/CVPRW59228.2023.00060},
} 


@inproceedings{20240615507800 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {EAID: An Eye-Tracking Based Advertising Image Dataset with Personalized Affective Tags},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
author = {Liang, Song and Liu, Ruihang and Qian, Jiansheng},
volume = {14495},
year = {2024},
pages = {282 - 294},
issn = {03029743},
address = {Shanghai, China},
abstract = {<div data-language="eng" data-ev-field="abstract">Contrary to natural images with randomized content, advertisements contain abundant emotion-eliciting manufactured scenes and multi-modal visual elements with highly related semantics. However, little research has evaluated the interrelationships of advertising vision and affective perception. The absence of advertising data sets with affective labels and visual attention benchmarks is one of the most pressing issues that have to be addressed. Meanwhile, growing evidence indicates that eye movements can reveal the internal states of human minds. Inspired by these, we use a high-precision eye tracker to record the eye-moving data of 57 subjects when they observe 1000 advertising images. 7-score opinion ratings for the five advertising attributes (i.e., ad liking, emotional, aesthetic, functional, and brand liking) are then collected. We further make a preliminary analysis of the correlation among advertising attributes, subjects’ visual attention, eye movement characteristics, and personality traits, obtaining a series of enlightening conclusions. To our best knowledge, the proposed dataset is the largest advertising image dataset based on eye tracking and with multiple personalized affective tags. It provides a new exploration space and data foundation for multimedia visual analysis and affection computing community. The data are available at: https://github.com/lscumt/EAID.<br/></div> © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
key = {Marketing},
keywords = {Behavioral research;Eye movements;Eye tracking;Semantics;},
note = {Advertizing;Affective Computing;Data set;Eye movement analysis;Eye-tracking;Image datasets;Multi-modal;Natural images;Visual Attention;Visual elements;},
URL = {http://dx.doi.org/10.1007/978-3-031-50069-5_24},
} 


@unpublished{20230016158 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {High-temporal-resolution event-based vehicle detection and tracking},
journal = {arXiv},
author = {El Shair, Zaid and Rawashdeh, Samir},
year = {2022},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">Event-based vision has been rapidly growing in recent years justified by the unique characteristics it presents such as its high temporal resolutions (~1us), high dynamic range (>120dB), and output latency of only a few microseconds. This work further explores a hybrid, multi-modal, approach for object detection and tracking that leverages state-of-the-art frame-based detectors complemented by hand-crafted event-based methods to improve the overall tracking performance with minimal computational overhead. The methods presented include event-based bounding box (BB) refinement that improves the precision of the resulting BBs, as well as a continuous event-based object detection method, to recover missed detections and generate inter-frame detections that enable a high-temporal-resolution tracking output. The advantages of these methods are quantitatively verified by an ablation study using the higher order tracking accuracy (HOTA) metric. Results show significant performance gains resembled by an improvement in the HOTA from 56.6%, using only frames, to 64.1% and 64.9%, for the event and edge-based mask configurations combined with the two methods proposed, at the baseline framerate of 24Hz. Likewise, incorporating these methods with the same configurations has improved HOTA from 52.5% to 63.1%, and from 51.3% to 60.2% at the high-temporal-resolution tracking rate of 384Hz. Finally, a validation experiment is conducted to analyze the real-world single-object tracking performance using high-speed LiDAR. Empirical evidence shows that our approaches provide significant advantages compared to using frame-based object detectors at the baseline framerate of 24Hz and higher tracking rates of up to 500Hz.<br/></div> Copyright © 2022, The Authors. All rights reserved.},
key = {Object detection},
keywords = {Object recognition;Object tracking;},
note = {Bounding box refinement;Bounding-box;Event camera;Event-based;Event-based vision;High temporal resolution;High-order;Higher-order;Object Tracking;Objects detection;},
URL = {http://dx.doi.org/10.48550/arXiv.2212.14289},
} 


@article{20231213789060 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Inferring the 3D Information from the Outside World Using Monocular Cameras},
journal = {ProQuest Dissertations and Theses Global},
author = {Zhang, Haotian},
year = {2022},
abstract = {<div data-language="eng" data-ev-field="abstract">Technological advances have made autonomous driving more and more feasible in common driving scenarios. Many large companies such as Waymo, Tesla, GM, and Uber have tested their self-driving vehicles with success in limited capacities. These vehicles employ a combination of cameras, radar, sonar, and LiDAR sensors. Yet the high cost of LiDAR, as well as the unreliability of sonar and radar, makes them unsuitable for quick large-scale deployment. On the contrary, camera-based autonomous driving has the potential to be a cheap and reliable alternative through steadily advancing computer vision and deep learning techniques. A general autonomous driving system incorporates three correlated technologies: 3D-based object detection, tracking, and localization.While all three components are important, most relevant papers tend to only focus on one single component. In this work, we first propose a multi-stage monocular vision-based framework for 3D-based detection, tracking, and localization by effectively integrating all three tasks in a complementary manner. Our system contains an RCNN-based Localization Network (LOCNet), which works in concert with fitness evaluation score (FES) based single-frame optimization, to get more accurate and refined 3D vehicle localization. To better utilize the temporal information, we further use a multi-frame optimization technique, taking advantage of camera ego-motion and a 3D TrackletNet Tracker (3D TNT), to improve both accuracy and consistency in our 3D localization.Moreover, we propose a joint framework (JMV3D) that can effectively associates moving objects over time and estimate their 3D localization information as well as segmentation masks from a sequence of 2D images so as to compensate for the individual drawbacks of each component. We further extend the existing Localization Network (LOCNet) to become Localization for Tracking Network (Loc4Trk-Net). A spatial Attention (SA) Neck is added to highlight the foreground (target of interest) and suppress the background with the help of mask segmentation so that more concentrated appearance features can be obtained. Besides, one additional embedding head is introduced to train discriminative feature embeddings to leverage deep pairwise contrastive learning and identify objects in various poses and viewpoints with appearance cues. Then, a straightforward combination of a 3D Kalman filter and the Hungarian algorithm is further utilized for robust instance association via both feature similarity and 3D localization information. Overall, both systems outperform the state-of-the-art image-based solutions in diverse scenarios and is even comparable with LiDAR-based methods. The proposed JMV3D pipeline also ranks 1st place on the KITTI-MOTS & KITT-STEP leaderboards and also achieves impressive results among all image-based solutions on nuScenes 3D tracking benchmark.Furthermore, monocular 3D object detection requires decoding 3D predictions solely from a single 2D image. However, by formulating this problem as a region-level understanding task, previous approaches neglect the image-level understanding of depth and semantics. To address this, we present the monocular 3D object detection via coarse-to-fine training, a new transformer-based architecture with an effective two-stage training strategy that can seamlessly handle both levels of tasks: (i) coarse-grained training on the whole image based on monocular depth data; followed by (ii) fine-grained training on specific regions based on 3D bounding boxes annotations. Instead of having dedicated transformer layers for fusion after the uni-modal backbone, Mono3DCFT pushes multi-modal cross-attention fusion into both the vision and depth backbones and achieves significant gains on the KITTI benchmark coupled with two-stage training. Trained solely based on limited publicly available KITTI depth data, our Mono3DCFT performs comparably against the previous best state-of-the-art, which is pre-trained on 15M additional proprietary depth data along with a more compute-intensive architecture. Extensive ablation studies demonstrate the effectiveness of our approach and its potential to serve as a transformer baseline for future monocular 3D monocular object detection. ProQuest Subject Headings: Electrical engineering.<br/></div>  © Citation reproduced with permission of ProQuest LLC.},
key = {Object detection},
keywords = {Autonomous vehicles;Cameras;Computer vision;Deep learning;Image segmentation;Object recognition;Optical radar;Sonar;Vision;},
note = {2D images;3D information;3D localization;3D object;Autonomous driving;Image-based;Localisation;Localization information;Objects detection;State of the art;},
} 


@unpublished{20230277371 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Learning from Synthetic Human Group Activities},
journal = {arXiv},
author = {Chang, Che-Jui and Goel, Parth and Li, Danrui and Zhou, Honglu and Patel, Deep and Moon, Seonghyeon and Sohn, Samuel S. and Yoon, Sejong and Kapadia, Mubbasir and Pavlovic, Vladimir},
year = {2023},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">The study of complex human interactions and group activities has become a focal point in human-centric computer vision. However, progress in related tasks is often hindered by the challenges of obtaining large-scale labeled datasets from real-world scenarios. To address the limitation, we introduce M<sup>3</sup>Act, a synthetic data generator for multi-view multi-group multi-person human atomic actions and group activities. Powered by the Unity engine, M<sup>3</sup>Act features multiple semantic groups, highly diverse and photorealistic images, and a comprehensive set of annotations, which facilitates the learning of human-centered tasks across single-person, multi-person, and multi-group conditions. We demonstrate the advantages of M<sup>3</sup>Act across three core experiments using various input modalities. First, adding our synthetic data significantly improves the performance of MOTRv2 on DanceTrack, leading to a hop on the leaderboard from 10<sup>th</sup> to 2<sup>nd</sup> place. With M<sup>3</sup>Act, we achieve tracking results on par with MOTRv2*, which is trained with 62.5% more real-world data. Second, M<sup>3</sup>Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on human group activity and atomic action recognition accuracy respectively. Moreover, M<sup>3</sup>Act opens new research for controllable 3D group activity generation. We define multiple metrics and propose a competitive baseline for the novel task.<br/></div> © 2023, CC BY-NC-SA.},
key = {Semantics},
keywords = {Computer vision;Human computer interaction;Large dataset;},
note = {Atomic actions;Focal points;Group activities;Human-centric;Humaninteraction;Interaction activities;Large-scales;Multi-group;Performance;Synthetic data;},
URL = {http://dx.doi.org/10.48550/arXiv.2306.16772},
} 


@inproceedings{20234615072241 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Hierarchical Vision Transformers with Shuffled Local Self-Attentions},
journal = {Proceedings - IWIS 2023: 3rd International Workshop on Intelligent Systems},
author = {Vo, Xuan-Thuy and Nguyen, Duy-Linh and Priadana, Adri and Jo, Kang-Hyun},
year = {2023},
address = {Ulsan, Korea, Republic of},
abstract = {<div data-language="eng" data-ev-field="abstract">Vision Transformers have reached breakthrough improvements in addressing computer visual fields, for instance, object classification, bounding box localization, semantic/instance pixel-wise predictions, single/multiple tracking, and generative AI models such as GPT-4, SAM, and UniAD. The key success of the Transformers is derived from the flexibility in fulfilling long-range dependencies from raw data and the generalization capability of input-dependent weight adaption. With these properties, Transformer models operated with the self-attention heart and without inductive biases become the new paradigm in processing multiple-modality data. However, the main bottle-neck of the Transformer is that global multi-head self-attention layers have high computational costs with the input lengths, e.g., quadratic complexity. When exploiting Transformer-based models on pixel-wise predictions, the cost is not affordable. To deal with this issue, recent methods try to calculate attention weights in local non-overlapped areas and require extra designs that exchange information across windows, for example, window shifting, window expanding, and window sliding. Although these strategies improve accuracy, their implementation is unfriendly and produces additional inference time. Following a line of this research, this paper introduces a new block that consists of non-overlapped local self-attention and overlapped local self-attention. Non-overlapped local self-attention learns interactions inside each window and overlapped local self-attention captures relationships among non-overlapped windows to boost receptive fields and modeling abilities. To be more efficient, both layers are performed in parallel in which each half of the heads is assigned to each layer. Therefore the diversity of the model is enhanced since conventional methods treat all heads equally. Experimental results are conducted and evaluated on the medium dataset, ImageNet-1K. As a result, the proposed approach achieves 77.2% Top-1 accuracy at 5.1M parameters and 0.5 GFLOPs, surpassing lightweight models by clear rooms.<br/></div> © 2023 IEEE.},
key = {Bottles},
keywords = {Computer vision;Data handling;Image classification;Pixels;Semantics;},
note = {Bounding-box;Generalization capability;Images classification;Local self-attention;Localisation;Long-range dependencies;Multiple tracking;Object classification;Vision transformer;Visual fields;},
URL = {http://dx.doi.org/10.1109/IWIS58789.2023.10284686},
} 


@article{20231413849563 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {DAKRS: Domain Adaptive Knowledge-Based Retrieval System for Natural Language-Based Vehicle Retrieval},
journal = {IEEE Access},
author = {Ha, Synh Viet-Uyen and Le, Huy Dinh-Anh and Nguyen, Quang Qui-Vinh and Chung, Nhat Minh},
volume = {11},
year = {2023},
pages = {90951 - 90965},
issn = {21693536},
abstract = {<div data-language="eng" data-ev-field="abstract">Given Natural Language (NL) text descriptions, NL-based vehicle retrieval aims to extract target vehicles from a multi-view multi-camera traffic video pool. Solutions to the problem have been challenged by not only inherent distinctions between textual and visual domains, but also by the complexities of the high-dimensionality of visual data, the diverse range of textual descriptions, a major lack of high-volume datasets in this relatively new field, alongside prominently large domain gaps between training and test sets. To deal with these issues, existing approaches have advocated computationally expensive models to separately extract the subspaces of language and vision before blending them into the same shared representation space. Through our proposed Domain Adaptive Knowledge-based Retrieval System (DAKRS), we show that by taking advantage of multi-modal information in a pretrained model, we can better focus on training robust representations in the shared space of limited labels, rather than on robust extraction of uni-modal representations that comes with increased computational burdens. Our contributions are threefold: (i) An efficient extension of Contrastive Language-Image Pre-training (CLIP)'s transfer learning into a baseline text-to-image multi-modular vehicle retrieval framework; (ii) A data enhancement method to create pseudo-vehicle tracks from the traffic video pool by leveraging the robustness of baseline retrieval model combined with background subtraction; and (iii) A Semi-Supervised Domain Adaptation (SSDA) scheme to engineer pseudo-labels for adapting model parameters to the target domain. Experimental results are benchmarked on Cityflow-NL to obtain 63.20% MRR with 150.0 M of parameters, illustrating our competitive effectiveness and efficiency against state-of-the-arts, without ensembling.<br/></div> © 2013 IEEE.},
key = {Vehicles},
keywords = {Abstracting;Cameras;Feature extraction;Image enhancement;Image retrieval;Knowledge based systems;Natural language processing systems;Personnel training;Search engines;Supervised learning;Text processing;},
note = {Adaptation models;Background subtraction;Contrastive representation learning;Domain adaptation;Features extraction;Images processing;Representation learning;Semi-supervised learning;Text-processing;Text-to-image retrieval;Urban areas;Vehicle retrieval;},
URL = {http://dx.doi.org/10.1109/ACCESS.2023.3260149},
} 


@article{20231613894810 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Multi-sensor based object tracking using enhanced particle swarm optimized multi-cue granular fusion},
journal = {Multimedia Tools and Applications},
author = {Kapoor, Rajiv and Singh, Nikhil and Kapoor, Aarchishya},
volume = {82},
number = {27},
year = {2023},
pages = {42417 - 42438},
issn = {13807501},
abstract = {<div data-language="eng" data-ev-field="abstract">In the discipline of computer vision, object tracking is one of the progressive and prominent areas of research with its application in the field of medical imaging, vehicle navigation, surveillance etc. Many of the proposed object tracking algorithms has shown success in the recent years. In this manuscript, we introduced a novel approach for object tracking that can develop an efficient framework of various features from different sensors. If we contemplate a RGB (red–green–blue) image it has better distinction of colors from human eye standpoint but is degraded by shadows and noise caused by illumination. Unlike RGB images thermal images are less receptive to such type of noise factors yet environmental condition can alter its distinction. To overcome this distinction issue of the two sensors a fusion of these two modalities is introduced, considering their interdependent advantages. This proposed technique is focused at enhancing the information collected from the fusion of visible imaging and thermal imaging sensors. It can also be implemented if the number of sensor are increased which in turn increases the number of features. With the use of features from two different sensors the proposed scheme utilizes the six information cues for the estimation of single output. The EPSO (enhanced particle swarm optimization) based particle filtering was adjusted with the concept of using multi-cue granular computing to weigh the particles and estimate the ultimate tracking result. After conducting attribute weight adaptation, the same approach is expanded to produce source-level fusion. The experimental performance of the method has been demonstrated on publicly available standard video sequences. After comparing it against state-of-the-art approaches, the findings show that it outperforms the trackers mentioned in the literature.<br/></div> © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
key = {Particle swarm optimization (PSO)},
keywords = {Infrared imaging;Medical imaging;Monte Carlo methods;},
note = {Enhanced particle swarm optimization;Granular fusion;ITS applications;Multi cues;Multi sensor;Multi-sensor tracking;Object Tracking;Particle filter;Particle swarm;Red green blues;},
URL = {http://dx.doi.org/10.1007/s11042-023-15164-9},
} 


@article{20231213783482 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Matching and Segmentation for Multimedia Data},
journal = {ProQuest Dissertations and Theses Global},
author = {Li, Hui},
year = {2022},
abstract = {<div data-language="eng" data-ev-field="abstract">With the development of society, both industry and academia draw increasing attention to multimedia systems, which handle image/video data, audio data, and text data comprehensively and simultaneously. In this thesis, we mainly focus on multi-modality data understanding, combining the two subjects of Computer Vision (CV) and Natural Language Processing (NLP). Such a task is widely used in many real-world scenarios, including criminal search with language descriptions by the witness, robotic navigation with language instruction in the smart industry, terrorist tracking, missing person identification, and so on. However, such a multi-modality system still faces many challenges, limiting its performance and ability in real-life situations, including the domain gap between the modalities of vision and language, the request for high-quality datasets, and so on. Therefore, to better analyze and handle these challenges, this thesis focuses on the two fundamental tasks, including matching and segmentation.Image-Text Matching (ITM) aims to retrieve the texts (images) that describe the most relevant contents for a given image (text) query. Due to the semantic gap between the linguistic and visual domains, aligning and comparing feature representations for languages and images are still challenging. To overcome this limitation, we propose a new framework for the image-text matching task, which uses an auxiliary captioning step to enhance the image feature, where the image feature is fused with the text feature of the captioning output. As the downstream application of ITM, the language-person search is one of the specific cases where language descriptions are provided to retrieve person images, which also suffers the domain gap between linguistic and visual data. To handle this problem, we propose a transformer-based language-person search matching framework with matching conducted between words and image regions for better image-text interaction. However, collecting a large amount of training data is neither cheap nor reliable using human annotations. We further study the one-shot person Re-ID (re-identification) task aiming to match people by offering one labeled reference image for each person, where previous methods request a large number of ground-truth labels. We propose progressive sample mining and representation learning to fit the limited labels for the one-shot Re-ID task better.Referring Expression Segmentation (RES) aims to localize and segment the target according to the given language expression. Existing methods jointly consider the localization and segmentation steps, which rely on the fused visual and linguistic features for both steps. We argue that the conflict between the purpose of finding the object and generating the mask limits the RES performance. To solve this problem, we propose a parallel position-kernel-segmentation pipeline to better isolate then interact with the localization and segmentation steps. In our pipeline, linguistic information will not directly contaminate the visual feature for segmentation. Specifically, the localization step localizes the target object in the image based on the referring expression, then the visual kernel obtained from the localization step guides the segmentation step. This pipeline also enables us to train RES in a weakly-supervised way, where the pixel-level segmentation labels are replaced by click annotations on center and corner points. The position head is fully-supervised trained with the click annotations as supervision, and the segmentation head is trained with weakly-supervised segmentation losses.This thesis focus on the key limitations of the multimedia system, where the experiments prove that the proposed frameworks are effective for specific tasks. The experiments are easy to reproduce with clear details, and source codes are provided for future works aiming at these tasks. ProQuest Subject Headings: Computer science, Multimedia.<br/></div>  © Citation reproduced with permission of ProQuest LLC.},
key = {Multimedia systems},
keywords = {Audio systems;Computer vision;Data mining;Data visualization;Image enhancement;Image retrieval;Image segmentation;Natural language processing systems;Semantics;Visual communication;Visual languages;},
note = {Annotation;Image texts;Language description;Localisation;Matchings;Multi-media communications;Multi-modality;Multimedium;Referring expressions;Text-matching;},
} 


@unpublished{20220099246 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Single Object Tracking Research : A Survey},
journal = {arXiv},
author = {Han, Rui-Ze and Feng, Wei and Guo, Qing and Hu, Qing-Hua},
year = {2022},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">Visual object tracking is an important and fundamental task in computer vision, which has many real-world applications, e.g., video surveillance, visual navigation and robotic service. Visual object tracking also has many challenges, such as object loss, object deformation, background clutters, and object fast motion. To solve the above problems and track the target accurately and efficiently, many visual object tracking algorithms have been emerged in recent years. In this paper, we first review the two most popular tracking frameworks in the past ten years, i.e., the Correlation Filter (CF) and Siamese network based visual object tracking. We present the rationale, the improvement strategy, and the representative works of the above two frameworks in detail. Specifically, the CF technology has been used in visual object tracking for over ten years, which has a good balance between the tracking accuracy and running speed. In CF tracking, the target is located by applying a circular convolution operation on the learned filter and the current frame, which can be efficiently achieved by the Fast Fourier Transform (FFT). The Siamese network based trackers locate the target from the candidate patches through a matching function offline learned on abundant training data in terms of image pairs. The matching function is modeled by a two-branch convolutional neural network (CNN) with shared parameters to learn the similarity between the target and the candidate patches. Besides the above two frameworks, we then present some other deep learning based tracking methods categorized by different network structures, e.g., RNN (Recurrent Neural Network), GCN (Graph Convolutional Network), etc. We also introduce some classical strategies for handling the challenges in the visual object tracking problem. From the recent tracking methods, we find that the development direction of the methods shows a diversified trend. More new network structures and skills have been applied to object tracking task. Although other deep tracking methods show a diversified trend, they have not formed a complete system. In the past ten years, the mainstream frameworks were the Correlation Filter and Siamese network. For the development trend of the visual tracking in the next few years, the CF tracking method has been relatively mature, and the development space in the future is limited. The deep learning algorithm based on CNN, especially the tracking algorithm under Siamese framework, could still be the mainstream framework. Further, this paper detailedly presents and compares the benchmarks and challenges for visual object tracking task, including the OTB benchmark, LaSOT benchmark, and VOT challenges, etc. Based on the data statistics of the datasets and the performance evaluation of the algorithms, we summarize the characteristics and advantages of various visual object tracking algorithms. For the future development of visual object tracking, which would be applied in real-world scenes before some problems to be addressed, such as the problems in long-term tracking, low-power high-speed tracking and attack-robust tracking. In the future, we can consider the integration of the traditional color (RGB) image together with the multi-modal data, such as the depth image, the thermal image, for joint analysis, which will provide more solutions for the visual object tracking task. Moreover, the visual tracking task will develop together with some other related tasks for mutual promotion, e.g., the video object detection, the video object segmentation task.<br/></div> Copyright © 2022, The Authors. All rights reserved.},
key = {Surveys},
keywords = {Clutter (information theory);Convolution;Convolutional neural networks;Learning algorithms;Recurrent neural networks;Robots;Security systems;},
note = {Correlation filter based tracking;Correlation filters;Development of visual tracking;Filter-based;Network-based;Siamese network based tracking;Visual object tracking;Visual Tracking;Visual tracking benchmark;},
} 


@article{20223612701989 ,
language = {Chinese},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Single Object Tracking Research: A Survey},
title = {视频单目标跟踪研究进展综述},
journal = {Jisuanji Xuebao/Chinese Journal of Computers},
author = {Han, Rui-Ze and Feng, Wei and Guo, Qing and Hu, Qing-Hua},
volume = {45},
number = {9},
year = {2022},
pages = {1877 - 1907},
issn = {02544164},
abstract = {<div data-language="eng" data-ev-field="abstract">Visual object tracking is an important and fundamental task in computer vision, which has many real-world applications, e.g., video surveillance, visual navigation and robotic service. Visual object tracking also has many challenges, such as object loss, object deformation, background clutters, and object fast motion. To solve the above problems and track the target accurately and efficiently, many visual object tracking algorithms have been emerged in recent years. In this paper, we first review the two most popular tracking frameworks in the past ten years, i.e., the Correlation Filter(CF) and Siamese network based visual object tracking. We present the rationale, the improvement strategy, and the representative works of the above two frameworks in detail. Specifically, the CF technology has been used in visual object tracking for over ten years, which has a good balance between the tracking accuracy and running speed. In CF tracking, the target is located by applying a circular convolution operation on the learned filter and the current frame, which can be efficiently achieved by the Fast Fourier Transform(FFT). The Siamese network based trackers locate the target from the candidate patches through a matching function offline learned on abundant training data in terms of image pairs. The matching function is modeled by a two-branch convolutional neural network(CNN) with shared parameters to learn the similarity between the target and the candidate patches. Besides the above two frameworks, we then present some other deep learning based tracking methods categorized by different network structures, e.g., RNN(Recurrent Neural Network), GCN(Graph Convolutional Network), etc. We also introduce some classical strategies for handling the challenges in the visual object tracking problem. From the recent tracking methods, we find that the development direction of the methods shows a diversified trend. More new network structures and skills have been applied to object tracking task. Although other deep tracking methods show a diversified trend, they have not formed a complete system. In the past ten years, the mainstream frameworks were the Correlation Filter and Siamese network. For the development trend of the visual tracking in the next few years, the CF tracking method has been relatively mature, and the development space in the future is limited. The deep learning algorithm based on CNN, especially the tracking algorithm under Siamese framework, could still be the mainstream framework. Further, this paper detailedly presents and compares the benchmarks and challenges for visual object tracking task, including the OTB benchmark, LaSOT benchmark, and VOT challenges, etc. Based on the data statistics of the datasets and the performance evaluation of the algorithms, we summarize the characteristics and advantages of various visual object tracking algorithms. For the future development of visual object tracking, which would be applied in real-world scenes before some problems to be addressed, such as the problems in long-term tracking, low-power high-speed tracking and attack-robust tracking. In the future, we can consider the integration of the traditional color(RGB) image together with the multi-modal data, such as the depth image, the thermal image, for joint analysis, which will provide more solutions for the visual object tracking task. Moreover, the visual tracking task will develop together with some other related tasks for mutual promotion, e.g., the video object detection, the video object segmentation task.<br/></div> © 2022, Science Press. All right reserved.},
key = {Recurrent neural networks},
keywords = {Computer vision;Convolution;Convolutional neural networks;Fast Fourier transforms;Learning algorithms;Learning systems;Security systems;Target tracking;},
note = {Correlation filter based tracking;Correlation filters;Development of visual tracking;Filter-based;Network-based;Siamese network based tracking;Visual object tracking;Visual Tracking;Visual tracking benchmark;},
URL = {http://dx.doi.org/10.11897/SP.J.1016.2022.01877},
} 


@inproceedings{20223112493154 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Aerial Based Traffic Tracking and Vehicle Count Detection Using Background Subtraction},
journal = {Lecture Notes in Mechanical Engineering},
author = {Muhamad, Muhamad Zulhilmi Bin and Akhtar, Mohammad Nishat and Bakar, Elmi Abu and Zulkoffli, Zuliani Binti and Mahmod, Muhammad Faisal},
volume = {25},
year = {2022},
pages = {372 - 386},
issn = {21954356},
address = {Melaka, Malaysia},
abstract = {<div data-language="eng" data-ev-field="abstract">With the advent of increasing population, the traffic density is also increasing. Thus, it becomes deemed necessary for the urban planners to analyse the traffic condition via video surveillance for a successful improvisation to the existing town planning. The objective of this study was to develop a high altitude video surveillance setup with centroid tracker and compare different variants of background subtraction comprising of K-Nearest Neighbour, Mixture of Gaussian and Geometric Multi-Grid on a vision-based system for road vehicle counting and tracking. This project uses Python as its programming language and Open Computer Vision (OpenCV) as an open-source library for developing a high altitude video surveillance system for vehicle counting and directional motion detection. The designed system was able to achieve high count precision even in difficult scenarios related to occlusions or the presence of shadows. The principle of the system was to install a camera on the pedestrian bridges and track the vehicular traffic congestion by incorporating a unique ID. Moving objects were tracked using different background subtraction algorithm and object tracking was conducted using the centroid tracker. The video processing model was combined with a motion detection procedure, which correctly classified the positioning of moving vehicles depending on the space and time when the experiment was conducted on the site location. From the results it was revealed that both K-Nearest Neighbour and Mixture of Gaussian showed better accuracy with 93% and 100% depending upon the traffic density modalities. Using the proposed setup design, the identification of severe shadows based on solidity can be computed through the nature of the shape and this classification allows its accuracy to be estimated.<br/></div> © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
URL = {http://dx.doi.org/10.1007/978-981-16-8954-3_35},
} 


@article{20231613941861 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {StreetAware: A High-Resolution Synchronized Multimodal Urban Scene Dataset},
journal = {Sensors},
author = {Piadyk, Yurii and Rulff, Joao and Brewer, Ethan and Hosseini, Maryam and Ozbay, Kaan and Sankaradas, Murugan and Chakradhar, Srimat and Silva, Claudio},
volume = {23},
number = {7},
year = {2023},
issn = {14248220},
abstract = {<div data-language="eng" data-ev-field="abstract">Access to high-quality data is an important barrier in the digital analysis of urban settings, including applications within computer vision and urban design. Diverse forms of data collected from sensors in areas of high activity in the urban environment, particularly at street intersections, are valuable resources for researchers interpreting the dynamics between vehicles, pedestrians, and the built environment. In this paper, we present a high-resolution audio, video, and LiDAR dataset of three urban intersections in Brooklyn, New York, totaling almost 8 unique hours. The data were collected with custom Reconfigurable Environmental Intelligence Platform (REIP) sensors that were designed with the ability to accurately synchronize multiple video and audio inputs. The resulting data are novel in that they are inclusively multimodal, multi-angular, high-resolution, and synchronized. We demonstrate four ways the data could be utilized — (1) to discover and locate occluded objects using multiple sensors and modalities, (2) to associate audio events with their respective visual representations using both video and audio modes, (3) to track the amount of each type of object in a scene over time, and (4) to measure pedestrian speed using multiple synchronized camera views. In addition to these use cases, our data are available for other researchers to carry out analyses related to applying machine learning to understanding the urban environment (in which existing datasets may be inadequate), such as pedestrian-vehicle interaction modeling and pedestrian attribute recognition. Such analyses can help inform decisions made in the context of urban sensing and smart cities, including accessibility-aware urban design and Vision Zero initiatives.<br/></div> © 2023 by the authors.},
key = {Computer vision},
keywords = {Pedestrian safety;Synchronization;Urban planning;},
note = {Data synchronization;High resolution;Multi-modal;Multimedia data;Street-level imagery;Urban design;Urban environments;Urban intelligence;Urban multimedium data;Urban sensing;},
URL = {http://dx.doi.org/10.3390/s23073710},
} 


@unpublished{20220475150 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Needs, trends, and advances in scintillators for radiographic imaging and tomography},
journal = {arXiv},
author = {Wang, Zhehui and Dujardin, Christophe and Freeman, Matthew S. and Gehring, Amanda E. and Hunter, James F. and Lecoq, Paul and Liu, Wei and Melcher, Charles L. and Morris, C.L. and Nikl, Martin and Pilania, Ghanshyam and Pokharel, Reeju and Robertson, Daniel G. and Rutstrom, Daniel J. and Sjue, Sky K. and Tremsin, Anton S. and Watson, S.A. and Wiggins, Brenden W. and Winch, Nicola M. and Zhuravleva, Mariya},
year = {2022},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">Scintillators are important materials for radiographic imaging and tomography (RadIT), when ionizing radiations such as X-rays, energetic charged particles or neutrons are used to reveal optically opaque internal structures of matter. Since its discovery and invention by Röntgen, RadIT now come in many forms or modalities such as phase contrast X-ray imaging, coherent X-ray diffractive imaging, high-energy X- and γ−ray radiography at above 1 MeV, X-ray computed tomography (CT), proton imaging and tomography (IT), neutron IT, positron emission tomography (PET), high-energy electron radiography, muon tomography, etc. High spatial, temporal resolution, high sensitivity, and radiation hardness, among others, are common metrics for RadIT performance, which are enabled by, in addition to scintillators, advances in particle sources especially high-luminosity accelerators and high-power lasers, photodetectors especially complementary metal-oxide-semiconductor (CMOS) pixelated sensor arrays, and lately data science. Medical imaging, non-destructive testing, nuclear safety and safeguards are traditional applications for RadIT. Examples of rapidly growing or emerging applications include space, additive manufacturing (AM), machine vision, and virtual reality or ‘metaverse’. Scintillator metrics such as light yield, decay time and radiation hardness are discussed in light of RadIT metrics. More than 160 kinds of scintillators and applications are presented during the SCINT22 conference. Some new trends include inorganic and organic scintillator composites or heterostructures, liquid phase synthesis of perovskites and single-crystal micrometer-thick films, use of multi-physics models and data science lately to guide scintillator development and discovery, structural innovations such as photonic crystals, nano-scintillators enhanced by the Purcell effect, heterostructural scintillating fibers, and multilayer configurations. Plenty of new opportunities exist in optimization of RadIT performance with reduced radiation dose, data-driven measurements, photon/particle counting and tracking methods supplementing time-integrated measurements, multimodal RadIT, and novel applications of RadIT for scintillator development and discovery.<br/></div> Copyright © 2022, The Authors. All rights reserved.},
key = {Photons},
keywords = {Charged particles;CMOS integrated circuits;Computerized tomography;Gamma rays;Hardness;Ionization;Medical imaging;MOS devices;Oxide films;Oxide semiconductors;Photodetectors;Photonic crystals;Radiation hardening;Scintillation counters;Semiconductor detectors;Single crystals;Thick films;Timing circuits;X ray detectors;X ray radiography;},
note = {Data driven;Data-driven discovery;Dose;Fast timing;High energy physic;High-energy physics;Inorganic scintillator;Multimodal imaging;Purcell-effect;Radiographic imaging;Radiographic tomography;Structured scintillator;},
URL = {http://dx.doi.org/10.48550/arXiv.2212.10322},
} 


@article{20240815617718 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {UniMod1K: Towards a More Universal Large-Scale Dataset and Benchmark for Multi-modal Learning},
journal = {International Journal of Computer Vision},
author = {Zhu, Xue-Feng and Xu, Tianyang and Liu, Zongtao and Tang, Zhangyong and Wu, Xiao-Jun and Kittler, Josef},
year = {2024},
issn = {09205691},
abstract = {<div data-language="eng" data-ev-field="abstract">The emergence of large-scale high-quality datasets has stimulated the rapid development of deep learning in recent years. However, most computer vision tasks focus on the visual modality only, resulting in a huge imbalance in the number of annotated data for other modalities. While several multi-modal datasets have been made available, the majority of them are confined to only two modalities, serving a single specific computer vision task. To redress the data deficiency for multi-modal learning and applications, a new dataset named UniMod1K is presented in this work. UniMod1K involves three data modalities: vision, depth, and language. For the vision and depth modalities, the UniMod1K dataset contains 1050 RGB-D sequences, comprising a total of some 2.5 million frames. Regarding the language modality, the proposed dataset includes 1050 sentences describing the target object in each video. To demonstrate the advantages of training on a larger multi-modal dataset, such as UniMod1K, and to stimulate research enabled by the dataset, we address several multi-modal tasks, namely multi-modal object tracking and monocular depth estimation. To establish a performance baseline, we propose novel baseline methods for RGB-D object tracking, vision-language tracking and vision-depth-language tracking. Additionally, we conduct comprehensive experiments for each of these tasks. The results highlight the potential of the UniMod1K dataset to improve the performance of multi-modal approaches. The dataset and codes can be accessed at https://github.com/xuefeng-zhu5/UniMod1K.<br/></div> © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
key = {Computer vision},
keywords = {Deep learning;Large datasets;Modal analysis;Tracking (position);},
note = {Depth Estimation;Monocular depth estimation;Multi-modal;Multi-modal dataset;Multi-modal learning;Object Tracking;Performance;RGB-D object tracking;Vision-depth-language tracking;Vision-language tracking;},
URL = {http://dx.doi.org/10.1007/s11263-024-01999-8},
} 


@unpublished{20240010327 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Bi-directional Adapter for Multi-modal Tracking},
journal = {arXiv},
author = {Cao, Bing and Guo, Junliang and Zhu, Pengfei and Hu, Qinghua},
year = {2023},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">Due to the rapid development of computer vision, single-modal (RGB) object tracking has made significant progress in recent years. Considering the limitation of single imaging sensor, multi-modal images (RGB, Infrared, etc.) are introduced to compensate for this deficiency for all-weather object tracking in complex environments. However, as acquiring sufficient multi-modal tracking data is hard while the dominant modality changes with the open environment, most existing techniques fail to extract multi-modal complementary information dynamically, yielding unsatisfactory tracking performance. To handle this problem, we propose a novel multi-modal visual prompt tracking model based on a universal bi-directional adapter, cross-prompting multiple modalities mutually. Our model consists of a universal bi-directional adapter and multiple modality-specific transformer encoder branches with sharing parameters. The encoders extract features of each modality separately by using a frozen pre-trained foundation model. We develop a simple but effective light feature adapter to transfer modality-specific information from one modality to another, performing visual feature prompt fusion in an adaptive manner. With adding fewer (0.32M) trainable parameters, our model achieves superior tracking performance in comparison with both the full fine-tuning methods and the prompt learning-based methods. Our code is available: https://github.com/SparkTempest/BAT.<br/></div> © 2023, CC0.},
key = {Signal encoding},
keywords = {Computer vision;Tracking (position);},
note = {Bi-directional;Complex environments;Imaging sensors;Multi-modal;Multimodal images;Multiple modalities;Object Tracking;Single-modal;Tracking data;Tracking performance;},
URL = {http://dx.doi.org/10.48550/arXiv.2312.10611},
} 


@inproceedings{20221211822316 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {RGBT Tracking by Fully-Convolutional Triple Networks with Cosine Embedding Loss},
journal = {ACM International Conference Proceeding Series},
author = {Zhang, Ping and Luo, Jin and Li, Muyang and Gao, Chunming and Wu, Changke},
year = {2022},
pages = {96 - 102},
address = {Virtual, Online, Thailand},
abstract = {<div data-language="eng" data-ev-field="abstract">RGBT tracking has drawn much attention on computer vision in recent years, which aims to fuse complementary information from visible and thermal images for robust object tracking. There are a lot of works paying extensive explorations for fusing features from convolutional networks to integrate modalities, especially modality-specific properties. Although these methods have achieved quite good performance, it is insufficient to represent and interpret the commonness and specificity, as well as the relationship between them, which are significant for RGBT tracking. In this work, we propose a novel triple network to extract Modal-Common, Modal-Specific features, which interprets Modal Common-Specific information from multi-modal images. Besides, a corresponding cosine embedding loss is designed to differentiate features and make them discriminative. For the purpose of perceiving complementary information of modal-specific features, we propose a cross-modal attention-query module, which queries channel attention of another modality and enhances relevant channels itself. Moreover, we conduct an efficient tracker with fully-convolutional siamese network for real-time RGBT tracking. Extensive experiments on two RGBT benchmark datasets has proved the excellent performance and efficiency of our method compared with classic RGB trackers and other state-of-the-art RGBT tracking algorithms.<br/></div> © 2022 ACM.},
key = {Convolution},
keywords = {Network embeddings;Tracking (position);Benchmarking;},
note = {Cosine embedding loss;Embeddings;Extensive explorations;Object Tracking;Performance;RGB-T tracking;Similarity learning;Thermal images;Triple network;Visible image;},
URL = {http://dx.doi.org/10.1145/3512353.3512367},
} 


@article{20240115325735 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Multi-modal visual tracking: Review and experimental comparison},
journal = {Computational Visual Media},
author = {Zhang, Pengyu and Wang, Dong and Lu, Huchuan},
volume = {10},
number = {2},
year = {2024},
pages = {193 - 214},
issn = {20960433},
abstract = {<div data-language="eng" data-ev-field="abstract">Visual object tracking has been drawing increasing attention in recent years, as a fundamental task in computer vision. To extend the range of tracking applications, researchers have been introducing information from multiple modalities to handle specific scenes, with promising research prospects for emerging methods and benchmarks. To provide a thorough review of multi-modal tracking, different aspects of multi-modal tracking algorithms are summarized under a unified taxonomy, with specific focus on visible-depth (RGB-D) and visible-thermal (RGB-T) tracking. Subsequently, a detailed description of the related benchmarks and challenges is provided. Extensive experiments were conducted to analyze the effectiveness of trackers on five datasets: PTB, VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, various future directions, including model design and dataset construction, are discussed from different perspectives for further research.[Figure not available: see fulltext.].<br/></div> © 2023, The Author(s).},
key = {Benchmarking},
keywords = {Tracking (position);},
note = {(RGB-D) and visible-thermal tracking;Experimental comparison;Multi-modal;Multi-modal fusion;Object Tracking;RGB-D tracking;Thermal;Tracking application;Visual object tracking;Visual Tracking;},
URL = {http://dx.doi.org/10.1007/s41095-023-0345-5},
} 


@inproceedings{20232214172311 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {RGB-T object tracking with adaptive decision fusion},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
author = {Bai, Yida and Yang, Ming},
volume = {12635},
year = {2023},
pages = {Academic Exchange Information Centre (AEIC) - },
issn = {0277786X},
address = {Zhengzhou, China},
abstract = {<div data-language="eng" data-ev-field="abstract">Visual object tracking is a traditional task in computer vision, which has developed with several decades. With the development of machine learning, correlation filter (CF) has been proposed with satisfying performance and very high framerate. Though the CF framework has numerous strengths in this task, the tracker is fragile to miss the target in several scenes, including extreme illumination, target occlusion and deformation. Recently, thermal modality, which detects the target’s temperature, is robust to the night scenes and can provide a precise target contour. In this paper, we propose a CF based tracker with decision fusion strategy for visible-thermal (RGB-T) tracking. First, we introduce multi-modal KCF trackers as our baseline. Then, we design a decision fusion method considering the Peak-to-Side Rate (PSR) of the score maps, thereby achieving an adaptive fusing those modalities and avoiding model’s heterogeneity. In the experiments, our tracker has validated on the public dataset, namely GTOT. Compared with two uni-modality trackers, the proposed tracker with real-time speed has shown superior results on both target localization and scale estimation.<br/></div> © 2023 SPIE.},
key = {Target tracking},
keywords = {Computer vision;},
note = {Correlation filters;Decisions fusion;Kernelized correlation filter;Learning correlation;Machine-learning;Object Tracking;Performance;RGB-T tracking;Thermal;Visual object tracking;},
URL = {http://dx.doi.org/10.1117/12.2679107},
} 


@inproceedings{20235015224309 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment},
journal = {MM 2023 - Proceedings of the 31st ACM International Conference on Multimedia},
author = {Zhang, Chunhui and Sun, Xin and Yang, Yiqian and Liu, Li and Liu, Qiong and Zhou, Xi and Wang, Yanfeng},
year = {2023},
pages = {5552 - 5561},
address = {Ottawa, ON, Canada},
abstract = {<div data-language="eng" data-ev-field="abstract">Current mainstream vision-language (VL) tracking framework consists of three parts,i.e., a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, e.g., similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language signals to generate language-injected vision tokens, which we then concatenate before feeding into the unified backbone architecture. This approach achieves feature integration in a unified backbone, removing the need for carefully-designed fusion modules and resulting in a more effective and efficient VL tracking framework. To further improve the learning efficiency, we introduce a multi-modal alignment module based on cross-modal and intra-modal contrastive objectives, providing more reasonable representations for the unified All-in-One transformer backbone. Extensive experiments on five benchmarks, i.e., OTB99-L, TNL2K, LaSOT, LaSOTExt and WebUAV-3M, demonstrate the superiority of the proposed tracker against existing state-of-the-art (SOTA) methods on VL tracking. Codes will be available at https://github.com/983632847/All-in-One here.<br/></div> © 2023 ACM.},
key = {Alignment},
keywords = {Computer architecture;Computer vision;Extraction;Feature extraction;HTTP;Semantics;Visual languages;},
note = {'current;Feature extractor;Feature integration;Features extraction;Fusion model;Language features;Multi-modal;Multi-modal alignment;Unified vision-language tracking;Visual feature;},
URL = {http://dx.doi.org/10.1145/3581783.3611803},
} 


@article{20230613546236 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {X-MAS: Extremely Large-Scale Multi-Modal Sensor Dataset for Outdoor Surveillance in Real Environments},
journal = {IEEE Robotics and Automation Letters},
author = {Noh, DongKi and Sung, Changki and Uhm, Teayoung and Lee, WooJu and Lim, Hyungtae and Choi, Jaeseok and Lee, Kyuewang and Hong, Dasol and Um, Daeho and Chung, Inseop and Shin, Hochul and Kim, MinJung and Kim, Hyoung-Rock and Baek, SeungMin and Myung, Hyun},
volume = {8},
number = {2},
year = {2023},
pages = {1093 - 1100},
issn = {23773766},
abstract = {<div data-language="eng" data-ev-field="abstract">In robotics and computer vision communities, extensive studies have been widely conducted regarding surveillance tasks, including human detection, tracking, and motion recognition with a camera. Additionally, deep learning algorithms are widely utilized in the aforementioned tasks as in other computer vision tasks. Existing public datasets are insufficient to develop learning-based methods that handle various surveillance for outdoor and extreme situations such as harsh weather and low illuminance conditions. Therefore, we introduce a new large-scale outdoor surveillance dataset named eXtremely large-scale Multi-modAl Sensor dataset (X-MAS) containing more than 500,000 image pairs and the first-person view data annotated by well-trained annotators. Moreover, a single pair contains multi-modal data (e.g. an IR image, an RGB image, a thermal image, a depth image, and a LiDAR scan). This is the first large-scale first-person view outdoor multi-modal dataset focusing on surveillance tasks to the best of our knowledge. We present an overview of the proposed dataset with statistics and present methods of exploiting our dataset with deep learning-based algorithms.<br/></div> © 2022 IEEE.},
key = {Cameras},
keywords = {Computer vision;Deep learning;Infrared imaging;Learning algorithms;Modal analysis;Motion estimation;Robots;Security systems;},
note = {Dataset;Field robot;Large-scales;Multi modal perceptions;Multimodal sensor;Robot vision systems;Surveillance;Surveillance robots;Task analysis;Video;},
URL = {http://dx.doi.org/10.1109/LRA.2023.3236569},
} 


@unpublished{20230246864 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment},
journal = {arXiv},
author = {Zhang, Chunhui and Sun, Xin and Liu, Li and Yang, Yiqian and Liu, Qiong and Zhou, Xi and Wang, Yanfeng},
year = {2023},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">Current mainstream vision-language (VL) tracking framework consists of three parts, i.e., a visual feature extractor, a language feature extractor, and a fusion model. To pursue better performance, a natural modus operandi for VL tracking is employing customized and heavier unimodal encoders, and multi-modal fusion models. Albeit effective, existing VL trackers separate feature extraction and feature integration, resulting in extracted features that lack semantic guidance and have limited target-aware capability in complex scenarios, e.g., similar distractors and extreme illumination. In this work, inspired by the recent success of exploring foundation models with unified architecture for both natural language and computer vision tasks, we propose an All-in-One framework, which learns joint feature extraction and interaction by adopting a unified transformer backbone. Specifically, we mix raw vision and language signals to generate language-injected vision tokens, which we then concatenate before feeding into the unified backbone architecture. This approach achieves feature integration in a unified backbone, removing the need for carefully-designed fusion modules and resulting in a more effective and efficient VL tracking framework. To further improve the learning efficiency, we introduce a multimodal alignment module based on cross-modal and intra-modal contrastive objectives, providing more reasonable representations for the unified All-in-One transformer backbone. Extensive experiments on five benchmarks, i.e., OTB99-L, TNL2K, LaSOT, LaSOTExt and WebUAV-3M, demonstrate the superiority of the proposed tracker against existing state-of-the-arts on VL tracking. Codes will be made publicly available.<br/></div> © 2023, CC BY-NC-SA.},
key = {Extraction},
keywords = {Alignment;Computer architecture;Feature extraction;Semantics;Visual languages;},
note = {'current;Feature extractor;Feature integration;Features extraction;Foundation models;Fusion model;Multi-modal;Multi-modal alignment;Transformer;Unified vision-language tracking;},
} 


@article{20232414211423 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Deep Triply Attention Network for RGBT Tracking},
journal = {Cognitive Computation},
author = {Yang, Rui and Wang, Xiao and Zhu, Yabin and Tang, Jin},
volume = {15},
number = {6},
year = {2023},
pages = {1934 - 1946},
issn = {18669956},
abstract = {<div data-language="eng" data-ev-field="abstract">RGB-Thermal (RGBT) tracking has gained significant attention in the field of computer vision due to its wide range of applications in video surveillance, autonomous driving, and human-computer interaction. This paper focuses on achieving a robust fusion of different modalities for RGBT tracking through attention modeling. We propose an effective triply attentive network for robust RGBT tracking, which consists of a local attention module, a cross-modality co-attention module, and a global attention module. The local attention module enables the tracker to focus on target regions while considering background interference, generated through backpropagation of the score map with respect to the RGB and thermal image pair. To enhance the interaction of different modalities in feature learning, we introduce a co-attention module that selects more discriminative features for both the visible (RGB) and thermal modalities simultaneously. To compensate for the limitations of local sampling, we incorporate a global attention module based on multi-modal information to compute high-quality global proposals. This module not only complements the local search strategy but also re-tracks lost targets when they come back into view. Extensive experiments conducted on three RGBT tracking datasets demonstrate that our proposed method outperforms other RGBT trackers, achieving more competitive results. Specifically, on the LasHeR dataset, the precision rate, normalized precision rate, and success rate reach 57.5%, 51.6%, and 41.0%, respectively. The above state-of-the-art experimental results confirm the effectiveness of our method in exploring the complementary advantages between modalities and achieving robust visual tracking.<br/></div> © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
key = {Human computer interaction},
keywords = {Backpropagation;Security systems;},
note = {Attention mechanisms;Autonomous driving;Co-attention mechanism;Global proposal;Local attention;Precision rates;RGB-thermal tracking;Robust fusion;Thermal;Video surveillance;},
URL = {http://dx.doi.org/10.1007/s12559-023-10158-z},
} 


@inproceedings{20234715106194 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Multi-Modal Food Classification in a Diet Tracking System with Spoken and Visual Inputs},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
author = {Gowda, Shivani and Hu, Yifan and Korpusik, Mandy},
volume = {2023-June},
year = {2023},
pages = {IEEE; IEEE Signal Processing Society - },
issn = {15206149},
address = {Rhodes Island, Greece},
abstract = {<div data-language="eng" data-ev-field="abstract">In this paper, we present multi-modal approaches to diet tracking. As health and well-being become increasingly important, mobile applications for diet tracking attract much interest. However, these applications often require users to log their meals based on relatively unreliable memory recall, thereby underestimating nutritional intake and, thus, undermining the efforts of nutrition tracking. To accurately record dietary intake, there is an increasing need for image computational methods. We investigated multi-modal transfer learning approaches on a novel, food-specific image-text dataset, specifically a Vision-and-Language Transformer that achieves a held-out test set Micro-F1 score of 77.70% and Macro-F1 score of 51.43% for 696 food categories. We aim to give other researchers new insight into the process of developing domain-specific, multi-modal deep learning models with small datasets.<br/></div> © 2023 IEEE.},
URL = {http://dx.doi.org/10.1109/ICASSP49357.2023.10095762},
} 


@article{20234615060938 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {MMF-Track: Multi-Modal Multi-Level Fusion for 3D Single Object Tracking},
journal = {IEEE Transactions on Intelligent Vehicles},
author = {Li, Zhiheng and Cui, Yubo and Lin, Yu and Fang, Zheng},
volume = {9},
number = {1},
year = {2024},
pages = {1817 - 1829},
issn = {23798858},
abstract = {<div data-language="eng" data-ev-field="abstract">3D single object tracking plays an important role in computer vision and autonomous driving. The mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the mentioned limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. After that, in feature interaction level, we present a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we propose a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we introduce a Similarity Fusion Module (SFM) to aggregate geometry and texture similarity from the target. Extensive experiments show that our method achieves competitive performance on KITTI and NuScenes datasets.<br/></div> © 2016 IEEE.},
key = {Three dimensional displays},
keywords = {Association reactions;Image fusion;Image texture;Modal analysis;Semantics;Target tracking;},
note = {3d single object tracking;Features extraction;Multi-modal data;Multi-modal data fusion;Object Tracking;Point cloud compression;Point-clouds;Siamese network;Single object;Targets tracking;Three-dimensional display;Transformer;},
URL = {http://dx.doi.org/10.1109/TIV.2023.3326790},
} 


@article{20230413413815 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Fusing Wireless Communication with Visual Sensing: From Sensor Sharing to Localization},
journal = {ProQuest Dissertations and Theses Global},
author = {Liu, Hansi},
year = {2022},
abstract = {<div data-language="eng" data-ev-field="abstract">Visual and wireless sensing, two popular sensing modalities in multi-modal systems, have complementary characteristics. While providing richer and more accurate spatial measurements using RGBD information, vision sensors have drawbacks such as limited field of view, vulnerability to occlusion and poor performance in low illumination conditions. Wireless sensing, on the other hand, suffers less from appearance variation and can operate non-line-of-sight. But its ranging performance is degraded by multipath and shadowing in complex environments.Sensor fusion and association in vision-wireless systems create "reality aware network", which allows the strength of each sensing modality to complement each other. In this thesis we propose to fuse wireless communication with visual sensing to improve system sensing range, tracking and localization. We design and implement different sensor fusion and association mechanisms for systems including Vehicle-to-Vehicle, Vehicle-to-Everything as well as localization and pedestrian tracking.Advanced driver assistance systems benefit from complete understandings of traffic scenes around vehicles. Existing systems gather data through cameras and other sensors in vehicles but scene understanding can be limited due to the sensing range of sensors or occlusion from other objects. To explore how to gather information beyond the view of one vehicle, we propose a connected vehicle system that allows multiple moving vehicles to share perception data over vehicle-to-vehicle communications and collaboratively fuse the data into a more complete traffic scene.Beyond fusing vision data via wireless communication, associating vision data with wireless data is another fundamental need in multi-modal applications. Successful vision-wireless association enables use-cases such as localization by fusing camera depth measurements with wireless ranging. It can also improve tracking and re-identification since wireless transmitters provide a stable identifier. Existing approaches of visual-wireless data association rely on appearance based fingerprinting, focus on controlled scenarios where participants are always visible and no passersby exist, or formulate optimization problems based on long sequences of measurement that needs post-processing. To achieve robust association between vision and wireless data, we propose a multi-modal system that leverages users' depth measurements, smartphone WiFi Fine Timing Measurements (FTM) and inertial measurement unit (IMU) sensor data to associate users detected on a camera footage with their corresponding smartphone identifiers.Furthermore, we propose a multi-modal localization approach that leverages pedestrians' visual and phone data to accurately estimate their positions. Existing works of localization adopt filtering techniques to fuse multi-modal sensor data and produce location estimations. In our context, however, these algorithms become infeasible when a pedestrian's camera measurement is unavailable due to occlusion or camera's limited field of view. To address this limitation, we propose a Generative Adversarial Network that leverages the available data correspondences from vision and phone modalities to learn the underlined cross-modal linkage. With a pedestrian's phone measurements as input, the network is able to generate coordinate estimations that are more accurate than the phone's original GPS readings. We further show that the proposed model supports self-learning. The generated coordinates can be associated with pedestrian's bounding box coordinates to obtain additional camera-phone data correspondences during inference. ProQuest Subject Headings: Computer engineering.<br/></div>  © Citation reproduced with permission of ProQuest LLC.},
key = {Cameras},
keywords = {Advanced driver assistance systems;Automobile drivers;Pedestrian safety;Smartphones;Vehicle to vehicle communications;Vehicles;},
note = {Computer engineering;Field of views;Localisation;Multimodal system;Sensing modalities;Vision data;Visual sensing;Wireless communications;Wireless data;Wireless sensing;},
} 


@inproceedings{20240815574064 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Siamese Network with Hybrid Fusion Strategy for RGB-T Tracking},
journal = {2023 IEEE 6th International Conference on Information Systems and Computer Aided Education, ICISCAE 2023},
author = {Liang, Minjun},
year = {2023},
pages = {811 - 814},
address = {Dalian, China},
abstract = {<div data-language="eng" data-ev-field="abstract">Visual object tracking is to capture the target according to the initial target state, which can be widely used in autonomous driving, human interaction, smart surveillance. In the community of computer vision, the subject has become more popularized. With development of multi-modality vision, RGB-T tracking has been focused by researchers due to its robust performance and wider applications. Existing methods mainly focus on data fusion, which can be categorized into three manners, including image fusion, feature fusion and decision fusion. In this paper, we aim to analyze the complementary role of various fusion types. We proposed a hybrid fusion strategy, which combines those types into a framework. We select Siamese based tracker as baseline and evaluate them in GTOT dataset. Our fusion strategy obtains the best performance against all other fusion modules, which achieves 65.3% and 77.8% success rate and precision rate. Our method can validate the complementary role of different fusion methods.<br/></div> © 2023 IEEE.},
URL = {http://dx.doi.org/10.1109/ICISCAE59047.2023.10392771},
} 


@article{20232414231382 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey},
journal = {Machine Intelligence Research},
author = {Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao, Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian, Yonghong and Gao, Wen},
volume = {20},
number = {4},
year = {2023},
pages = {447 - 482},
issn = {2731538X},
abstract = {<div data-language="eng" data-ev-field="abstract">With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as bidirectional encoder representations (BERT), vision transformer (ViT), generative pre-trained transformers (GPT), etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the validation of large-scale MM-PTMs, including generative, classification, and regression tasks. We also give visualization and analysis of the model parameters and results on representative downstream tasks. Finally, we point out possible research directions for this topic that may benefit future works. In addition, we maintain a continuously updated paper list for large-scale pre-trained multi-modal big models: https://github.com/wangxiao5791509/MultiModal_BigModels_Survey .<br/></div> © 2023, The Author(s).},
key = {Computer vision},
keywords = {Deep learning;Learning algorithms;Learning systems;Natural language processing systems;Network architecture;},
note = {Deep learning;Down-stream;Large-scales;Multi-modal;Natural languages;Pre-trained model;Pre-training;Representation learning;Single domains;},
URL = {http://dx.doi.org/10.1007/s11633-022-1410-8},
} 


@unpublished{20230180496 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking},
journal = {arXiv},
author = {Li, Zhiheng and Cui, Yubo and Lin, Yu and Fang, Zheng},
year = {2023},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">3D single object tracking plays an important role in computer vision and autonomous driving. The mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the mentioned limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. After that, in feature interaction level, we present a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we propose a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we introduce a Similarity Fusion Module (SFM) to aggregate geometry and texture similarity from the target. Extensive experiments show that our method achieves competitive performance on KITTI and NuScenes datasets. The code will be opened soon in https://github.com/LeoZhiheng/MMF-Tracker.git.<br/></div> Copyright © 2023, The Authors. All rights reserved.},
key = {Data fusion},
keywords = {Image texture;Modal analysis;Semantics;Target tracking;Textures;},
note = {3d single object tracking;Feature interactions;Multi level fusion;Multi-modal data;Multi-modal data fusion;Object Tracking;Point-clouds;Siamese network;Single object;Transformer;},
URL = {http://dx.doi.org/10.48550/arXiv.2305.06794},
} 


@unpublished{20230018011 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {X-MAS: Extremely Large-Scale Multi-Modal Sensor Dataset for Outdoor Surveillance in Real Environments},
journal = {arXiv},
author = {Noh, DongKi and Sung, Changki and Uhm, Teayoung and Lee, WooJu and Lim, Hyungtae and Choi, Jaeseok and Lee, Kyuewang and Hong, Dasol and Um, Daeho and Chung, Inseop and Shin, Hochul and Kim, MinJung and Kim, Hyoung-Rock and Baek, SeungMin and Myung, Hyun},
year = {2022},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">In robotics and computer vision communities, extensive studies have been widely conducted regarding surveillance tasks, including human detection, tracking, and motion recognition with a camera. Additionally, deep learning algorithms are widely utilized in the aforementioned tasks as in other computer vision tasks. Existing public datasets are insufficient to develop learning-based methods that handle various surveillance for outdoor and extreme situations such as harsh weather and low illuminance conditions. Therefore, we introduce a new large-scale outdoor surveillance dataset named eXtremely large-scale Multi-modAl Sensor dataset (XMAS) containing more than 500,000 image pairs and the first-person view data annotated by well-trained annotators. Moreover, a single pair contains multi-modal data (e.g. an IR image, an RGB image, a thermal image, a depth image, and a LiDAR scan). This is the first large-scale first-person view outdoor multi-modal dataset focusing on surveillance tasks to the best of our knowledge. We present an overview of the proposed dataset with statistics and present methods of exploiting our dataset with deep learning-based algorithms. The latest information on the dataset and our study are available at https://github.com/lge-robot-navi, and the dataset will be available for download through a server.<br/></div> © 2022, CC BY.},
key = {Computer vision},
keywords = {Cameras;Deep learning;Infrared imaging;Large dataset;Learning algorithms;Modal analysis;Motion estimation;Robots;Security systems;},
note = {Dataset;Field robot;First person;Large-scales;Multi modal perceptions;Multimodal sensor;Real environments;Surveillance robots;Surveillance task;Vision communities;},
URL = {http://dx.doi.org/10.48550/arXiv.2212.14574},
} 


@unpublished{20230060713 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey},
journal = {arXiv},
author = {Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao, Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian, Yonghong and Gao, Wen},
year = {2023},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pretraining models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the validation of large-scale MM-PTMs, including generative, classification, and regression tasks. We also give visualization and analysis of the model parameters and results on representative downstream tasks. Finally, we point out possible research directions for this topic that may benefit future works. In addition, we maintain a continuously updated paper list for large-scale pre-trained multi-modal big models: https://github.com/wangxiao5791509/MultiModal BigModels Survey.<br/></div> Copyright © 2023, The Authors. All rights reserved.},
key = {Network architecture},
keywords = {Computer vision;Deep learning;Learning algorithms;Learning systems;Natural language processing systems;},
note = {Deep learning;Down-stream;Language processing;Large-scales;Multi-modal;Natural languages;Pre-trained model;Pre-training;Representation learning;Single domains;},
URL = {http://dx.doi.org/10.48550/arXiv.2302.10035},
} 


@article{20232814393485 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Exploring the potential of Siamese network for RGBT object tracking},
journal = {Journal of Visual Communication and Image Representation},
author = {Feng, Liangliang and Song, Kechen and Wang, Junyi and Yan, Yunhui},
volume = {95},
year = {2023},
issn = {10473203},
abstract = {<div data-language="eng" data-ev-field="abstract">Siamese tracking is one of the most promising object tracking methods today due to its balance of performance and speed. However, it still performs poorly when faced with some challenges such as low light or extreme weather. This is caused by the inherent limitations of visible images, and a common way to cope with it is to introduce infrared data as an aid to improve the robustness of tracking. However, most of the existing RGBT trackers are variants of MDNet (Hyeonseob Nam and Bohyung Han, Learning multi-domain convolutional neural networks for visual tracking, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 4293–4302.), which have significant limitations in terms of operational efficiency. On the contrary, the potential of Siamese tracking in the field of RGBT tracking has not been effectively exploited due to the reliance on large-scale training data. To solve this dilemma, in this paper, we propose an end-to-end Siamese RGBT tracking framework that is based on cross-modal feature enhancement and self-attention (SiamFEA). We draw on the idea of migration learning and employ local fine-tuning to reduce the dependence on large-scale RGBT data and verify the feasibility of this approach, and then we propose a reliable fusion approach to efficiently fuse the features of different modalities. Specifically, we first propose a cross-modal feature enhancement module to exploit the complementary properties of dual-modality, followed by capturing non-local attention in channel and spatial dimensions for adaptive weighted fusion, respectively. Our network was trained end-to-end on the LasHeR (Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, Jin Tang, LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking, CoRR abs/2104.13202, 2021) training set and reached new SOTAs on GTOT (C. Li, H. Cheng, S. Hu, X. Liu, J. Tang, L. Lin, Learning collaborative sparse representation for grayscale-thermal tracking, IEEE Trans. Image Process, 25 (12) (2016) 5743–5756.), RGBT234 (C. Li, X. Liang, Y. Lu, N. Zhao, and J. Tang, "Rgb-t object tracking: Benchmark and baseline," Pattern Recognition, vol. 96, p. 106977, 2019.), and LasHeR (Chenglong Li, Wanlin Xue, Yaqing Jia, Zhichen Qu, Bin Luo, Jin Tang, LasHeR: A Large-scale High-diversity Benchmark for RGBT Tracking, CoRR abs/2104.13202, 2021) while running in real-time.<br/></div> © 2023 Elsevier Inc.},
key = {Image enhancement},
keywords = {Convolutional neural networks;Object tracking;},
note = {Attention mechanisms;Cross-modal;End to end;Feature enhancement;Large-scales;Multi-modal fusion;Object Tracking;RGBT tracking;Siamese network;Tracking method;},
URL = {http://dx.doi.org/10.1016/j.jvcir.2023.103882},
} 


@unpublished{20240071496 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {A Survey for Foundation Models in Autonomous Driving},
journal = {arXiv},
author = {Gao, Haoxiang and Li, Yaqian and Long, Kaiwen and Yang, Ming and Shen, Yiqing},
year = {2024},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.<br/></div> © 2024, CC BY.},
key = {Autonomous vehicles},
keywords = {Foundations;Modeling languages;Natural language processing systems;Object detection;Well testing;},
note = {Autonomous driving;Code translation;Codegeneration;Foundation models;Language model;Language processing;Natural languages;Parallel vision;Planning and simulations;Research papers;},
URL = {http://dx.doi.org/10.48550/arXiv.2402.01105},
} 


@article{20230913640189 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Visual–auditory learning network for construction equipment action detection},
journal = {Computer-Aided Civil and Infrastructure Engineering},
author = {Jung, Seunghoon and Jeoung, Jaewon and Lee, Dong-Eun and Jang, Hyounseung and Hong, Taehoon},
volume = {38},
number = {14},
year = {2023},
pages = {1916 - 1934},
issn = {10939687},
abstract = {<div data-language="eng" data-ev-field="abstract">Action detection of construction equipment is critical for tracking project performance, facilitating construction automation, and fostering construction efficiency in terms of construction site monitoring. Particularly, the auditory signal can provide additional information on computer vision-based action detection of various types of construction equipment. Therefore, this study aims to develop a visual–auditory learning network model for the action detection of construction equipment based on two modalities (i.e., vision and audition). To this end, both visual and auditory features are extracted from the multi-modal feature extractor. In addition, the multi-head attention and detection module is designed to conduct the localization and classification tasks in separate heads in which different attention mechanisms for each task are applied. Particularly, the content-based attention mechanism and the dot-product attention mechanism are, respectively, adopted for spatial attention in the localization head and channel attention in the classification head. The evaluation results show that the precision and recall of the proposed model can reach 86.92% and 84.00% with the adoption of the multi-head attention and detection module, which has proven to improve overall detection performance by utilizing different correlations of visual and auditory features for localization and classification, respectively.<br/></div> © 2023 Computer-Aided Civil and Infrastructure Engineering.},
key = {Construction equipment},
keywords = {Audition;Learning systems;},
note = {Attention mechanisms;Auditory feature;Auditory learning;Construction automation;Construction efficiency;Detection modules;Learning network;Localisation;Project performance;Visual feature;},
URL = {http://dx.doi.org/10.1111/mice.12983},
} 


@article{20235015209801 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {A Particle Filter Based Visual Object Tracking: A Systematic Review of Current Trends and Research Challenges},
journal = {International Journal of Advanced Computer Science and Applications},
author = {Awal, Md Abdul and Refat, Md Abu Rumman and Naznin, Feroza and Islam, Md Zahidul},
volume = {14},
number = {11},
year = {2023},
pages = {1291 - 1301},
issn = {2158107X},
abstract = {<div data-language="eng" data-ev-field="abstract">Visual object tracking is a crucial research area in computer vision because it can simulate a dynamic environment with non-linear motions and multi-modal non-Gaussian noises. However, This paper presents an overview of the recent developments in particle filter-based visual object tracking algorithms and discusses the pros and cons of particle filters, respectively. There are presentations of many different methodologies and algorithms in the research literature. The majority of visual object tracking research at present is on particle filters. In addition, the most advanced technique for visual object tracking has also been developed by combining the convolutional neural network (CNN) and the particle filter. The advantage of particle filters is that they can handle nonlinear models and non-Gaussian advancements, sequentially concentrating on the areas of the state space with higher densities, primarily parallelization, and simplicity of implementation. Despite this, it offers a robust framework for visual object tracking because it incorporates uncertainty and outperforms other filters like the Kalman filter, Kernelized correlation filter, optical filter, mean shift filter, and extended Kalman filter in recognition tests. In contrast, this study provided information on various particle filter features and classifiers.<br/></div> © (2023), (Science and Information Organization). All Rights Reserved.},
key = {Kalman filters},
keywords = {Classification (of information);Convolutional neural networks;Gaussian distribution;Gaussian noise (electronic);Monte Carlo methods;Optical correlation;},
note = {'current;A-particles;Convolutional neural network;Filter-based;Gaussians;On-gaussian noise;Particle filter;Research challenges;Systematic Review;Visual object tracking;},
URL = {http://dx.doi.org/10.14569/IJACSA.2023.01411131},
} 


@inproceedings{20231814049923 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Analysis of simultaneous localization and mapping technology for mobile robot based on binocular vision},
journal = {2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms, EEBDA 2023},
author = {Xue, Ziheng},
year = {2023},
pages = {1735 - 1739},
address = {Changchun, China},
abstract = {<div data-language="eng" data-ev-field="abstract">With the increasing demand for intelligent robot applications over the years, SLAM technology is also developing rapidly as an essential part of robot perception modality. SLAM is a vast system that involves multi-domain knowledge such as image pre-processing, matching, and camera pose estimation. In this paper, the basic architecture of SLAM based on binocular vision is elaborated, and some applications of its methods are presented in conjunction with new methods in recent years. The primary approach is to separate the front-end and back-end and achieve good tracking and map-building results using a multi-threaded approach. The front-end uses the SHI-Tomasi algorithm in OpenCV to detect the feature points, triangulation to calculate the landmark depth, and Lucas-Kanade optical flow method for tracking from one frame to the next and computing the pose transformation. G2O is used in the SLAM back-end to optimize the poses and nonlinearities. The analysis of SLAM based on binocular vision also provides an essential reference for people new to visual SLAM.<br/></div> © 2023 IEEE.},
key = {Stereo vision},
keywords = {Binocular vision;Intelligent robots;Robot applications;Robot vision;Stereo image processing;},
note = {Domain knowledge;Front end;G2O;Image preprocessing;Matchings;Multi-domains;Robot perception;Simultaneous localization and mapping technologies;SLAM;Triangulate;},
URL = {http://dx.doi.org/10.1109/EEBDA56825.2023.10090692},
} 


@inproceedings{20232214154688 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {IPSN 2023 - Proceedings of the 2023 22nd International Conference on Information Processing in Sensor Networks},
journal = {IPSN 2023 -  Proceedings of the 2023 22nd International Conference on Information Processing in Sensor Networks},
year = {2023},
pages = {ACM SIGBED; IEEE SPS - },
address = {San Antonio, TX, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">The proceedings contain 58 papers. The topics discussed include: POS: an operator scheduling framework for multi-model inference on edge intelligent computing; CoEdge: a cooperative edge system for distributed real-time deep learning tasks; PointSplit: towards on-device 3D object detection with heterogenous low-power accelerators; addressing practical challenges in acoustic sensing to enable fast motion tracking; CMA: cross-modal association between wearable and structural vibration signal segments for indoor occupant sensing; interpersonal distance tracking with mmWave radar and IMUs; mmRipple: communicating with mmWave radars through smartphone vibration; DeepGANTT: a scalable deep learning scheduler for backscatter networks; ARSteth: enabling home self-screening with AR-assisted intelligent stethoscopes; hydra: concurrent coordination for fault-tolerant networking; MicroDeblur: image motion deblurring on microcontroller-based vision systems; and mosaic: extremely low-resolution RFID vision for visually-anonymized action recognition.<br/></div>},
} 


@article{20240515455591 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Fusion of VMD-AR with adaptive Gaussian mixture particle filtering for pedestrian trajectory tracking},
journal = {Digital Signal Processing: A Review Journal},
author = {Luo, Yuheng and Xu, Jingyun and Cai, Zhiduan and Jiang, Dongming},
volume = {146},
year = {2024},
issn = {10512004},
abstract = {<div data-language="eng" data-ev-field="abstract">Pedestrian target tracking is a key area in computer vision. This paper tackles the problem of low accuracy in conventional pedestrian target tracking by introducing an Adaptive Mixture Gaussian Particle Filter (AMGPF), which integrates Variational Mode Decomposition and autoregressive models. This method builds upon the traditional particle filter algorithm and introduces several improvements. First, we enhance the particle filter by introducing a mixture Gaussian model to obtain the posterior probability distribution of particles. This representation allows each particle to capture a mixture of possible states, alleviating the problem of particle degeneracy commonly encountered in traditional particle filters. Next, we refine the iterative process of the mixture Gaussian model by employing Bayesian criteria and likelihood function convergence to adaptively control the number of iterations and the Gaussian component count. This optimization helps reduce computational load while improving the model's goodness of fit. Lastly, we introduce variational mode decomposition to process the residual signals. By decomposing and reconstructing these signals, we extract multi-modal motion features. Based on this, we employ an autoregressive model for prediction, combining the predicted trajectory with the original one to effectively enhance tracking accuracy. Experimental results demonstrate that the proposed method significantly improves the precision of pedestrian trajectory prediction and exhibits strong robustness, outperforming traditional methods.<br/></div> © 2024 Elsevier Inc.},
key = {Variational mode decomposition},
keywords = {Adaptive filtering;Adaptive filters;Clutter (information theory);Gaussian distribution;Iterative methods;Monte Carlo methods;Target tracking;Trajectories;},
note = {Adaptive Gaussian mixture;Adaptive mixture gaussian;Autoregressive modelling;Gaussians;Mixture Gaussian model;Particle filter;Particle Filtering;Pedestrian target tracking;Pedestrian trajectories;Targets tracking;},
URL = {http://dx.doi.org/10.1016/j.dsp.2024.104386},
} 


@inproceedings{20232214175108 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2024 Elsevier Inc.},
copyright = {Compendex},
title = {Automated registration of multi-mode nondestructive evaluation data},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
author = {Spaeth, Peter W. and Frankforter, Erik L. and Schneck, William C. and Webster, Matthew R. and Godbold, Matthew and Perey, Daniel F.},
volume = {12487},
year = {2023},
pages = {The Society of Photo-Optical Instrumentation Engineers (SPIE) - },
issn = {0277786X},
address = {Long Beach, CA, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">Registration techniques play a central role in applications of image processing to computer vision, medical imaging, and automatic target tracking. Feature-based techniques such as scale-invariant feature transform (SIFT) and speeded up robust features (SURF) are commonly used to register images derived from a single modality. However, SIFT and SURF struggle to register images from different modalities because the features tend to manifest rather differently and at sometimes very different length-scales. The most successful methods that have been developed to register multi-modal data use information-theoretic approaches. These methods play a key part in nondestructive evaluation scenarios where data that is collected by sensors of different modalities must be registered to be fused. In this paper, automated registration based on normalized mutual information is applied to align data derived from ultrasonic and radiographic inspections of (i) additively manufactured titanium alloy test coupons, and (ii) thin, lithium metal pouch-cell batteries. The quality of the registration is quantified in terms of computational resources and spatial accuracy. In the first case the X-ray computed tomography (XCT) data is captured on a region corresponding to a small subset of the ultrasonic data, while in the case of the lithium batteries the digital radiography (DR) captures a larger region of interest than the ultrasonic data. In both cases the radiographic data resolution is much higher than for ultrasound, but interestingly, in both cases the accuracy of the registration is approximately equal to two-to-three-pixel lengths in the ultrasonic images.<br/></div> © 2023 SPIE.},
key = {Medical imaging},
keywords = {Computerized tomography;Image segmentation;Imaging systems;Lithium batteries;Modal analysis;Nondestructive examination;Sensor data fusion;Ultrasonics;X ray radiography;},
note = {Automated registration;Data registration;Digital radiography;Features extraction;Invariant feature transforms;Mutual information-based registration;Mutual informations;Non destructive evaluation;Scale invariant features;X-ray computed tomography;},
URL = {http://dx.doi.org/10.1117/12.2657439},
} 



