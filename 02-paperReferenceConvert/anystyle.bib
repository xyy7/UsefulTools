@inproceedings{2018_CVPR_Mentzer_Conditional,
  language  = {English},
  copyright = {Compendex},
  title     = {Conditional Probability Models for Deep Image Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Mentzer, Fabian and Agustsson, Eirikur and Tschannen, Michael and Timofte, Radu and Gool, Luc Van},
  year      = {2018},
  pages     = {4394-4402},
  issn      = {10636919},
  address   = {Salt Lake City, UT, United states},
  publisher = {IEEE},
  abstract  = {Deep Neural Networks trained as image auto-encoders have recently emerged as a promising direction for advancing the state-of-the-art in image compression. The key challenge in learning such networks is twofold: To deal with quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent image representation. In this paper, we focus on the latter challenge and propose a new technique to navigate the rate-distortion trade-off for an image compression auto-encoder. The main idea is to directly model the entropy of the latent representation by using a context model: A 3D-CNN which learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder makes use of the context model to estimate the entropy of its representation, and the context model is concurrently updated to learn the dependencies between the symbols in the latent representation. Our experiments show that this approach, when measured in MS-SSIM, yields a state-of-the-art image compression system based on a simple convolutional auto-encoder.<br/> &copy; 2018 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR.2018.00462},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2020_CVPR_Cheng_Learned,
  language  = {English},
  copyright = {Compendex},
  title     = {Learned image compression with discretized gaussian mixture likelihoods and attention modules},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
  year      = {2020},
  pages     = {7936-7945},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  publisher = {IEEE},
  abstract  = {Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM.<br/> &copy; 2020 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR42600.2020.00796},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2020_NIPS_Mentzer_High-fidelity,
  language  = {English},
  copyright = {Compendex},
  title     = {High-fidelity generative image compression},
  booktitle = {Advances in Neural Information Processing Systems},
  author    = {Mentzer, Fabian and Toderici, George and Tschannen, Michael and Agustsson, Eirikur},
  volume    = {2020-December},
  year      = {2020},
  pages     = {11913-11924},
  issn      = {10495258},
  address   = {Virtual, Online},
  publisher = {MIT Press},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We extensively study how to combine Generative Adversarial Networks and learned compression to obtain a state-of-the-art generative lossy compression system. In particular, we investigate normalization layers, generator and discriminator architectures, training strategies, as well as perceptual losses. In contrast to previous work, i) we obtain visually pleasing reconstructions that are perceptually similar to the input, ii) we operate in a broad range of bitrates, and iii) our approach can be applied to high-resolution images. We bridge the gap between rate-distortion-perception theory and practice by evaluating our approach both quantitatively with various perceptual metrics, and with a user study. The study shows that our method is preferred to previous approaches even if they use more than 2&times; the bitrate.<br/></div> &copy; 2020 Neural information processing systems foundation. All rights reserved.},
  journal   = {Advances in Neural Information Processing Systems}
}

@inproceedings{2021_CVPR_He_Checkerboard,
  language  = {English},
  copyright = {Compendex},
  title     = {Checkerboard context model for efficient learned image compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {He, Dailan and Zheng, Yaoyan and Sun, Baocheng and Wang, Yan and Qin, Hongwei},
  year      = {2021},
  pages     = {14766-14775},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">For learned image compression, the autoregressive context model is proved effective in improving the rate-distortion (RD) performance. Because it helps remove spatial redundancies among latent representations. However, the decoding process must be done in a strict scan order, which breaks the parallelization. We propose a parallelizable checkerboard context model (CCM) to solve the problem. Our two-pass checkerboard context calculation eliminates such limitations on spatial locations by re-organizing the decoding order. Speeding up the decoding process more than 40 times in our experiments, it achieves significantly improved computational efficiency with almost the same rate-distortion performance. To the best of our knowledge, this is the first exploration on parallelization-friendly spatial context model for learned image compression.<br/></div> &copy; 2021 IEEE},
  url       = {http://dx.doi.org/10.1109/CVPR46437.2021.01453},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@article{2021_arXiv_Dupont_COIN,
  language  = {English},
  copyright = {Compendex},
  title     = {COIN: Compression with implicit neural representations},
  journal   = {arXiv preprint arXiv:2103.03123},
  year      = {2021},
  publisher = {arXiv},
  author    = {Dupont, Emilien and Goliski, Adam and Alizadeh, Milad and The, Yee Whye and Doucet, Arnaud},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We propose a new simple approach for image compression: instead of storing the RGB values for each pixel of an image, we store the weights of a neural network overfitted to the image. Specifically, to encode an image, we fit it with an MLP which maps pixel locations to RGB values. We then quantize and store the weights of this MLP as a code for the image. To decode the image, we simply evaluate the MLP at every pixel location. We found that this simple approach outperforms JPEG at low bit-rates, even without entropy coding or learning a distribution over weights. While our framework is not yet competitive with state of the art compression methods, we show that it has various attractive properties which could make it a viable alternative to other neural data compression approaches.<br/></div> Copyright &copy; 2021, The Authors. All rights reserved.}
}

@inproceedings{2021_MM_Xie_Enhanced,
  language  = {English},
  copyright = {Compendex},
  title     = {Enhanced Invertible Encoding for Learned Image Compression},
  booktitle = {MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia},
  author    = {Xie, Yueqi and Cheng, Ka Leong and Chen, Qifeng},
  year      = {2021},
  pages     = {162-170},
  address   = {Virtual, Online, China},
  publisher = {ACM},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Although deep learning based image compression methods have achieved promising progress these days, the performance of these methods still cannot match the latest compression standard Versatile Video Coding (VVC). Most of the recent developments focus on designing a more accurate and flexible entropy model that can better parameterize the distributions of the latent features. However, few efforts are devoted to structuring a better transformation between the image space and the latent feature space. In this paper, instead of employing previous autoencoder style networks to build this transformation, we propose an enhanced Invertible Encoding Network with invertible neural networks (INNs) to largely mitigate the information loss problem for better compression. Experimental results on the Kodak, CLIC, and Tecnick datasets show that our method outperforms the existing learned image compression methods and compression standards, including VVC (VTM 12.1), especially for high-resolution images. Our source code is available at https://github.com/xyq7/InvCompress.<br/></div> &copy; 2021 ACM.},
  url       = {http://dx.doi.org/10.1145/3474085.3475213},
  journal   = {MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia}
}

@article{2020_arXiv_Liu_A,
  language  = {English},
  copyright = {Compendex},
  title     = {A unified end-to-end framework for efficient deep image compression},
  booktitle = {arXiv preprint arXiv:2002.03370},
  publisher = {arXiv},
  author    = {Liu, Jiaheng and Lu, Guo and Hu, Zhihao and Xu, Dong},
  year      = {2020},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Image compression is a widely used technique to reduce the spatial redundancy in images. Recently, learning based image compression has achieved significant progress by using the powerful representation ability from neural networks. However, the current state-of-the-art learning based image compression methods suffer from the huge computational complexity, which limits their capacity for practical applications. In this paper, we propose a unified framework called Efficient Deep Image Compression (EDIC) based on three new technologies, including a channel attention module, a Gaussian mixture model and a decoder-side enhancement module. Specifically, we design an auto-encoder style network for learning based image compression. To improve the coding efficiency, we exploit the channel relationship between latent representations by using the channel attention module. Besides, the Gaussian mixture model is introduced for the entropy model and improves the accuracy for bitrate estimation. Furthermore, we introduce the decoder-side enhancement module to further improve image compression performance. Our EDIC method can also be readily incorporated with the Deep Video Compression (DVC) framework [1] to further improve the video compression performance. Simultaneously, our EDIC method boosts the coding performance significantly while bringing slightly increased computational complexity. More importantly, experimental results demonstrate that the proposed approach outperforms the current state-of-the-art image compression methods and is up to more than 150 times faster in terms of decoding speed when compared with Minnen's method [2]. The proposed framework also successfully improves the performance of the recent deep video compression system DVC [1].<br/></div> Copyright &copy; 2020, The Authors. All rights reserved.},
  journal   = {arXiv preprint arXiv:2002.03370}
}

@inproceedings{2020_AAAI_Hu_Coarse-to-fine,
  language  = {English},
  copyright = {Compendex},
  title     = {Coarse-to-fine hyper-prior modeling for learned image compression},
  booktitle = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
  publisher = {AAAI press},
  author    = {Hu, Yueyu and Yang, Wenhan and Liu, Jiaying},
  year      = {2020},
  pages     = {11013-11020},
  address   = {New York, NY, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Approaches to image compression with machine learning now achieve superior performance on the compression rate compared to existing hybrid codecs. The conventional learning-based methods for image compression exploits hyper-prior and spatial context model to facilitate probability estimations. Such models have limitations in modeling long-term dependency and do not fully squeeze out the spatial redundancy in images. In this paper, we propose a coarseto- fine framework with hierarchical layers of hyper-priors to conduct comprehensive analysis of the image and more effectively reduce spatial redundancy, which improves the ratedistortion performance of image compression significantly. Signal Preserving Hyper Transforms are designed to achieve an in-depth analysis of the latent representation and the Information Aggregation Reconstruction sub-network is proposed to maximally utilize side-information for reconstruction. Experimental results show the effectiveness of the proposed network to efficiently reduce the redundancies in images and improve the rate-distortion performance, especially for high-resolution images. Our project is publicly available at https://huzi96.github.io/coarse-to-fine-compression.html.<br/></div> Copyright &copy; 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  url       = {https://doi.org/10.1609/aaai.v34i07.6736 },
  journal   = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence}
}

@inproceedings{2018_NIPS_Minnen_Joint,
  language  = {English},
  copyright = {Compendex},
  title     = {Joint autoregressive and hierarchical priors for learned image compression},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {MIT Press},
  author    = {Minnen, David and Balle, Johannes and Toderici, George},
  volume    = {2018-December},
  year      = {2018},
  pages     = {10771-10780},
  issn      = {10495258},
  address   = {Montreal, QC, Canada},
  abstract  = {Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate-distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.<br/> &copy; 2018 Curran Associates Inc.All rights reserved.},
  journal   = {Advances in Neural Information Processing Systems}
}

@article{2018_arXiv_Balle_Variational,
  language  = {English},
  copyright = {Compendex},
  title     = {Variational image compression with a scale hyperprior},
  booktitle = {arXiv preprint arXiv:1802.01436},
  publisher = {arXiv},
  author    = {Balle, Johannes and Minnen, David and Singh, Saurabh and Hwang, Sung Jin and Johnston, Nick},
  year      = {2018},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate&ndash;distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.<br/></div> Copyright &copy; 2018, The Authors. All rights reserved.},
  journal   = {arXiv preprint arXiv:1802.01436}
}

@inproceedings{2019_ICLR_Lee_Context-adaptive,
  language  = {English},
  copyright = {Compendex},
  title     = {Context-adaptive entropy model for end-to-end optimized image compression},
  booktitle = {7th International Conference on Learning Representations, ICLR 2019},
  publisher = {ICLR},
  author    = {Lee, Jooyoung and Cho, Seunghyun and Beack, Seung-Kwon},
  year      = {2019},
  address   = {New Orleans, LA, United states},
  abstract  = {We propose a context-adaptive entropy model for use in end-to-end optimized image compression. Our model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required. Based on these contexts, we allow the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an enhanced compression performance. Based on the experimental results, the proposed method outperforms the traditional image codecs, such as BPG and JPEG2000, as well as other previous artificial-neural-network (ANN) based approaches, in terms of the peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) index. The test code is publicly available at https://github.com/JooyoungLeeETRI/CA_Entropy_Model.<br/> &copy; 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved.},
  journal   = {7th International Conference on Learning Representations, ICLR 2019}
}

@inproceedings{2017_CVPR_Toderici_Full,
  language  = {English},
  copyright = {Compendex},
  title     = {Full resolution image compression with recurrent neural networks},
  booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  publisher = {IEEE},
  author    = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
  volume    = {2017-January},
  year      = {2017},
  pages     = {5435-5443},
  address   = {Honolulu, HI, United states},
  abstract  = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.<br/> &copy; 2017 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR.2017.577},
  journal   = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017}
}

@article{2022_arXiv_Yang_Lossy,
  language  = {English},
  copyright = {Compendex},
  title     = {Lossy Image Compression with Conditional Diffusion Models},
  booktitle = {arXiv preprint arXiv:2209.06950},
  publisher = {arXiv},
  author    = {Yang, Ruihan and Mandt, Stephan},
  year      = {2022},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" latent variables characterizing the diffusion process are synthesized (stochastically or deterministically) at decoding time. We show that the model&rsquo;s performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving five datasets and sixteen image quality assessment metrics show that our approach yields the strongest reported FID scores while also yielding competitive performance with state-of-the-art models in several SIM-based reference metrics.<br/></div> Copyright &copy; 2022, The Authors. All rights reserved.},
  url       = {http://dx.doi.org/10.48550/arXiv.2209.06950},
  journal   = {arXiv preprint arXiv:2209.06950}
}

@inproceedings{2017_ICLR_Balle_End-to-end,
  language  = {English},
  copyright = {Compendex},
  title     = {End-to-end optimized image compression},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  publisher = {ICLR},
  author    = {Balle, Johannes and Laparra, Valero and Simoncelli, Eero P.},
  year      = {2017},
  address   = {Toulon, France},
  journal   = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings}
}

@article{2022_TPAMI_Ma_End-to-End,
  language  = {English},
  copyright = {Compendex},
  title     = {End-to-End Optimized Versatile Image Compression with Wavelet-Like Transform},
  booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE},
  author    = {Ma, Haichuan and Liu, Dong and Yan, Ning and Li, Houqiang and Wu, Feng},
  volume    = {44},
  number    = {3},
  year      = {2022},
  pages     = {1247-1263},
  issn      = {01628828},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Built on deep networks, end-to-end optimized image compression has made impressive progress in the past few years. Previous studies usually adopt a compressive auto-encoder, where the encoder part first converts image into latent features, and then quantizes the features before encoding them into bits. Both the conversion and the quantization incur information loss, resulting in a difficulty to optimally achieve arbitrary compression ratio. We propose iWave++ as a new end-to-end optimized image compression scheme, in which iWave, a trained wavelet-like transform, converts images into coefficients without any information loss. Then the coefficients are optionally quantized and encoded into bits. Different from the previous schemes, iWave++ is versatile: a single model supports both lossless and lossy compression, and also achieves arbitrary compression ratio by simply adjusting the quantization scale. iWave++ also features a carefully designed entropy coding engine to encode the coefficients progressively, and a de-quantization module for lossy compression. Experimental results show that lossy iWave++ achieves state-of-the-art compression efficiency compared with deep network-based methods; on the Kodak dataset, lossy iWave++ leads to 17.34 percent bits saving over BPG; lossless iWave++ achieves comparable or better performance than FLIF. Our code and models are available at https://github.com/mahaichuan/Versatile-Image-Compression.<br/></div> &copy; 1979-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/TPAMI.2020.3026003},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@inproceedings{2019_ICCV_Habibian_Video,
  language  = {English},
  copyright = {Compendex},
  title     = {Video compression with rate-distortion autoencoders},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  author    = {Habibian, Amirhossein and Rozendaal, Ties Van and Tomczak, Jakub and Cohen, Taco},
  volume    = {2019-October},
  year      = {2019},
  pages     = {7032-7041},
  issn      = {15505499},
  address   = {Seoul, Korea, Republic of},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {In this paper we present a a deep generative model for lossy video compression. We employ a model that consists of a 3D autoencoder with a discrete latent space and an autoregressive prior used for entropy coding. Both autoencoder and prior are trained jointly to minimize a rate-distortion loss, which is closely related to the ELBO used in variational autoencoders. Despite its simplicity, we find that our method outperforms the state-of-the-art learned video compression networks based on motion compensation or interpolation. We systematically evaluate various design choices, such as the use of frame-based or spatio-temporal autoencoders, and the type of autoregressive prior. In addition, we present three extensions of the basic method that demonstrate the benefits over classical approaches to compression. First, we introduce emph{semantic compression}, where the model is trained to allocate more bits to objects of interest. Second, we study emph{adaptive compression}, where the model is adapted to a domain with limited variability, eg videos taken from an autonomous car, to achieve superior compression on that domain. Finally, we introduce emph{multimodal compression}, where we demonstrate the effectiveness of our model in joint compression of multiple modalities captured by non-standard imaging sensors, such as quad cameras. We believe that this opens up novel video compression applications, which have not been feasible with classical codecs.<br/> &copy; 2019 IEEE.},
  url       = {http://dx.doi.org/10.1109/ICCV.2019.00713},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision}
}

@inproceedings{2020_CVPR_Yang_Learning,
  language  = {English},
  copyright = {Compendex},
  title     = {Learning for video compression with hierarchical quality and recurrent enhancement},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Yang, Ren and Mentzer, Fabian and Van Gool, Luc and Timofte, Radu},
  year      = {2020},
  pages     = {6627-6636},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {In this paper, we propose a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. The frames in the first layer are compressed by an image compression method with the highest quality. Using these frames as references, we propose the Bi-Directional Deep Compression (BDDC) network to compress the second layer with relatively high quality. Then, the third layer frames are compressed with the lowest quality, by the proposed Single Motion Deep Compression (SMDC) network, which adopts a single motion map to estimate the motions of multiple frames, thus saving bits for motion information. In our deep decoder, we develop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement. In our HLVC approach, the hierarchical quality benefits the coding efficiency, since the high quality information facilitates the compression and enhancement of low quality frames at encoder and decoder sides, respectively. Finally, the experiments validate that our HLVC approach advances the state-of-the-art of deep video compression methods, and outperforms the "Low-Delay P (LDP) very fast" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at https://github.com/RenYang-home/HLVC.<br/> &copy; 2020 IEEE},
  url       = {http://dx.doi.org/10.1109/CVPR42600.2020.00666},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2020_CVPR_Lin_M-LVC,
  language  = {English},
  copyright = {Compendex},
  title     = {M-LVC: Multiple frames prediction for learned video compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Lin, Jianping and Liu, Dong and Li, Houqiang and Wu, Feng},
  year      = {2020},
  pages     = {3543-3551},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {We propose an end-to-end learned video compression scheme for low-latency scenarios. Previous methods are limited in using the previous one frame as reference. Our method introduces the usage of the previous multiple frames as references. In our scheme, the motion vector (MV) field is calculated between the current frame and the previous one. With multiple reference frames and associated multiple MV fields, our designed network can generate more accurate prediction of the current frame, yielding less residual. Multiple reference frames also help generate MV prediction, which reduces the coding cost of MV field. We use two deep auto-encoders to compress the residual and the MV, respectively. To compensate for the compression error of the auto-encoders, we further design a MV refinement network and a residual refinement network, taking use of the multiple reference frames as well. All the modules in our scheme are jointly optimized through a single rate-distortion loss function. We use a step-by-step training strategy to optimize the entire scheme. Experimental results show that the proposed method outperforms the existing learned video compression methods for low-latency mode. Our method also performs better than H.265 in both PSNR and MS-SSIM. Our code and models are publicly available.<br/> &copy;2020 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR42600.2020.00360},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2020_CVPR_Agustsson_Scale-space,
  language  = {English},
  copyright = {Compendex},
  title     = {Scale-space flow for end-to-end optimized video compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Agustsson, Eirikur and Minnen, David and Johnston, Nick and Balle, Johannes and Hwang, Sung Jin and Toderici, George},
  year      = {2020},
  pages     = {8500-8509},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {Despite considerable progress on end-to-end optimized deep networks for image compression, video coding remains a challenging task. Recently proposed methods for learned video compression use optical flow and bilinear warping for motion compensation and show competitive rate-distortion performance relative to hand-engineered codecs like H.264 and HEVC. However, these learning-based methods rely on complex architectures and training schemes including the use of pre-trained optical flow networks, sequential training of sub-networks, adaptive rate control, and buffering intermediate reconstructions to disk during training. In this paper, we show that a generalized warping operator that better handles common failure cases, e.g. disocclusions and fast motion, can provide competitive compression results with a greatly simplified model and training procedure. Specifically, we propose scale-space flow, an intuitive generalization of optical flow that adds a scale parameter to allow the network to better model uncertainty. Our experiments show that a low-latency video compression model (no B-frames) using scale-space flow for motion compensation can outperform analogous state-of-the art learned video compression models while being trained using a much simpler procedure and without any pre-trained optical flow networks.<br/> &copy; 2020 IEEE},
  url       = {http://dx.doi.org/10.1109/CVPR42600.2020.00853},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2020_Lecture_Lu_Content,
  language  = {English},
  copyright = {Compendex},
  title     = {Content Adaptive and Error Propagation Aware Deep Video Compression},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author    = {Lu, Guo and Cai, Chunlei and Zhang, Xiaoyun and Chen, Li and Ouyang, Wanli and Xu, Dong and Gao, Zhiyong},
  volume    = {12347 LNCS},
  year      = {2020},
  pages     = {456-472},
  issn      = {03029743},
  address   = {Glasgow, United kingdom},
  publisher = {Berlin: Springer},
  abstract  = {Recently, learning based video compression methods attract increasing attention. However, the previous works suffer from error propagation due to the accumulation of reconstructed error in inter predictive coding. Meanwhile, the previous learning based video codecs are also not adaptive to different video contents. To address these two problems, we propose a content adaptive and error propagation aware video compression system. Specifically, our method employs a joint training strategy by considering the compression performance of multiple consecutive frames instead of a single frame. Based on the learned long-term temporal information, our approach effectively alleviates error propagation in reconstructed frames. More importantly, instead of using the hand-crafted coding modes in the traditional compression systems, we design an online encoder updating scheme in our system. The proposed approach updates the parameters for encoder according to the rate-distortion criterion but keeps the decoder unchanged in the inference stage. Therefore, the encoder is adaptive to different video contents and achieves better compression performance by reducing the domain gap between the training and testing datasets. Our method is simple yet effective and outperforms the state-of-the-art learning based video codecs on benchmark datasets without increasing the model size or decreasing the decoding speed.<br/> &copy; 2020, Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-030-58536-5_27},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2020_Lecture_Hu_Improving,
  language  = {English},
  copyright = {Compendex},
  title     = {Improving Deep Video Compression by Resolution-Adaptive Flow Coding},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author    = {Hu, Zhihao and Chen, Zhenghao and Xu, Dong and Lu, Guo and Ouyang, Wanli and Gu, Shuhang},
  volume    = {12347 LNCS},
  year      = {2020},
  pages     = {193-209},
  issn      = {03029743},
  address   = {Glasgow, United kingdom},
  publisher = {Berlin: Springer},
  abstract  = {In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression.<br/> &copy; 2020, Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-030-58536-5_12},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{2021_JSTSP_Yang_Learning,
  language  = {English},
  copyright = {Compendex},
  title     = {Learning for Video Compression with Recurrent Auto-Encoder and Recurrent Probability Model},
  booktitle = {IEEE Journal on Selected Topics in Signal Processing},
  author    = {Yang, Ren and Mentzer, Fabian and Van Gool, Luc and Timofte, Radu},
  volume    = {15},
  number    = {2},
  year      = {2021},
  pages     = {388-401},
  issn      = {19324553},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {The past few years have witnessed increasing interests in applying deep learning to video compression. However, the existing approaches compress a video frame with only a few number of reference frames, which limits their ability to fully exploit the temporal correlation among video frames. To overcome this shortcoming, this paper proposes a Recurrent Learned Video Compression (RLVC) approach with the Recurrent Auto-Encoder (RAE) and Recurrent Probability Model (RPM). Specifically, the RAE employs recurrent cells in both the encoder and decoder. As such, the temporal information in a large range of frames can be used for generating latent representations and reconstructing compressed outputs. Furthermore, the proposed RPM network recurrently estimates the Probability Mass Function (PMF) of the latent representation, conditioned on the distribution of previous latent representations. Due to the correlation among consecutive frames, the conditional cross entropy can be lower than the independent cross entropy, thus reducing the bit-rate. The experiments show that our approach achieves the state-of-the-art learned video compression performance in terms of both PSNR and MS-SSIM. Moreover, our approach outperforms the default Low-Delay P (LDP) setting of x265 on PSNR, and also has better performance on MS-SSIM than the SSIM-tuned x265 and the slowest setting of x265. The codes are available at https://github.com/RenYang-home/RLVC.git.<br/> &copy; 2007-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/JSTSP.2020.3043590},
  journal   = {IEEE Journal on Selected Topics in Signal Processing}
}

@inproceedings{2021_NIPS_Li_Deep,
  language  = {English},
  copyright = {Compendex},
  title     = {Deep Contextual Video Compression},
  booktitle = {Advances in Neural Information Processing Systems},
  author    = {Li, Jiahao and Li, Bin and Lu, Yan},
  volume    = {22},
  year      = {2021},
  pages     = {18114-18125},
  issn      = {10495258},
  address   = {Virtual, Online},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Most of the existing neural video compression methods adopt the predictive coding framework, which first generates the predicted frame and then encodes its residue with the current frame. However, as for compression ratio, predictive coding is only a sub-optimal solution as it uses simple subtraction operation to remove the redundancy across frames. In this paper, we propose a deep contextual video compression framework to enable a paradigm shift from predictive coding to conditional coding. In particular, we try to answer the following questions: how to define, use, and learn condition under a deep video compression framework. To tap the potential of conditional coding, we propose using feature domain context as condition. This enables us to leverage the high dimension context to carry rich information to both the encoder and the decoder, which helps reconstruct the high-frequency contents for higher video quality. Our framework is also extensible, in which the condition can be flexibly designed. Experiments show that our method can significantly outperform the previous state-of-the-art (SOTA) deep video compression methods. When compared with x265 using veryslow preset, we can achieve 26.0% bitrate saving for 1080P standard test videos.<br/></div> &copy; 2021 Neural information processing systems foundation. All rights reserved.},
  journal   = {Advances in Neural Information Processing Systems}
}

@inproceedings{2021_ICCV_Rippel_ELF-VC,
  language  = {English},
  copyright = {Compendex},
  title     = {ELF-VC: Efficient Learned Flexible-Rate Video Coding},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  author    = {Rippel, Oren and Anderson, Alexander G. and Tatwawadi, Kedar and Nair, Sanjay and Lytle, Craig and Bourdev, Lubomir},
  year      = {2021},
  pages     = {14459-14468},
  issn      = {15505499},
  address   = {Virtual, Online, Canada},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {<div data-language="eng" data-ev-field="abstract">While learned video codecs have demonstrated great promise, they have yet to achieve sufficient efficiency for practical deployment. In this work, we propose several novel ideas for learned video compression which allow for improved performance for the low-latency mode (I- and P-frames only) along with a considerable increase in computational efficiency. In this setting, for natural videos our approach compares favorably across the entire R-D curve under metrics PSNR, MS-SSIM and VMAF against all mainstream video standards (H.264, H.265, AV1) and all ML codecs. At the same time, our approach runs at least 5x faster and has fewer parameters than all ML codecs which report these figures. Our contributions include a flexible-rate framework allowing a single model to cover a large and dense range of bitrates, at a negligible increase in computation and parameter count; an efficient backbone optimized for ML-based codecs; and a novel in-loop flow prediction scheme which leverages prior information towards more efficient compression. We benchmark our method, which we call ELF-VC (Efficient, Learned and Flexible Video Coding) on popular video test sets UVG and MCL-JCV under metrics PSNR, MS-SSIM and VMAF. For example, on UVG under PSNR, it reduces the BD-rate by 44% against H.264, 26% against H.265, 15% against AV1, and 35% against the current best ML codec. At the same time, on an NVIDIA Titan V GPU our approach encodes/decodes VGA at 49/91 FPS, HD 720 at 19/35 FPS, and HD 1080 at 10/18 FPS.<br/></div> &copy; 2021 IEEE},
  url       = {http://dx.doi.org/10.1109/ICCV48922.2021.01421},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision}
}

@inproceedings{2021_CVPR_Hu_FVC,
  language  = {English},
  copyright = {Compendex},
  title     = {FVC: A New Framework Towards Deep Video Compression in Feature Space},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Hu, Zhihao and Lu, Guo and Xu, Dong},
  year      = {2021},
  pages     = {1502-1511},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  publisher = {Piscataway, NJ: IEEE},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Learning based video compression attracts increasing attention in the past few years. The previous hybrid coding approaches rely on pixel space operations to reduce spatial and temporal redundancy, which may suffer from inaccurate motion estimation or less effective motion compensation. In this work, we propose a feature-space video coding network (FVC) by performing all major operations (i.e., motion estimation, motion compression, motion compensation and residual compression) in the feature space. Specifically, in the proposed deformable compensation module, we first apply motion estimation in the feature space to produce motion information (i.e., the offset maps), which will be compressed by using the auto-encoder style network. Then we perform motion compensation by using deformable convolution and generate the predicted feature. After that, we compress the residual feature between the feature from the current frame and the predicted feature from our deformable compensation module. For better frame reconstruction, the reference features from multiple previous reconstructed frames are also fused by using the non-local attention mechanism in the multi-frame feature fusion module. Comprehensive experimental results demonstrate that the proposed framework achieves the state-of-the-art performance on four benchmark datasets including HEVC, UVG, VTL and MCL-JCV.<br/></div> &copy; 2021 IEEE},
  url       = {http://dx.doi.org/10.1109/CVPR46437.2021.00155},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2021_NIPS_Chen_NeRV,
  language  = {English},
  copyright = {Compendex},
  title     = {NeRV: Neural Representations for Videos},
  booktitle = {Advances in Neural Information Processing Systems},
  author    = {Chen, Hao and He, Bo and Wang, Hanyu and Ren, Yixuan and Lim, Ser-Nam and Shrivastava, Abhinav},
  volume    = {26},
  year      = {2021},
  pages     = {21557-21568},
  issn      = {10495258},
  address   = {Virtual, Online},
  publisher = {Cambridge: MIT Press},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We propose a novel neural representation for videos (NeRV) which encodes videos in neural networks. Unlike conventional representations that treat videos as frame sequences, we represent videos as neural networks taking frame index as input. Given a frame index, NeRV outputs the corresponding RGB image. Video encoding in NeRV is simply fitting a neural network to video frames and decoding process is a simple feedforward operation. As an image-wise implicit representation, NeRV output the whole image and shows great efficiency compared to pixel-wise implicit representation, improving the encoding speed by 25&times; to 70&times;, the decoding speed by 38&times; to 132&times;, while achieving better video quality. With such a representation, we can treat videos as neural networks, simplifying several video-related tasks. For example, conventional video compression methods are restricted by a long and complex pipeline, specifically designed for the task. In contrast, with NeRV, we can use any neural network compression method as a proxy for video compression, and achieve comparable performance to traditional frame-based video compression approaches (H.264, HEVC etc.). Besides compression, we demonstrate the generalization of NeRV for video denoising. The source code and pre-trained model can be found at https://github.com/haochen-rye/NeRV.git.<br/></div> &copy; 2021 Neural information processing systems foundation. All rights reserved.},
  journal   = {Advances in Neural Information Processing Systems}
}

@inproceedings{2022_NIPS_Mentzer_VCT,
  language  = {English},
  copyright = {Compendex},
  title     = {VCT: A Video Compression Transformer},
  booktitle = {Advances in Neural Information Processing Systems},
  author    = {Mentzer, Fabian and Toderici, George and Minnen, David and Hwang, Sung Jin and Caelles, Sergi and Lucic, Mario and Agustsson, Eirikur},
  volume    = {35},
  year      = {2022},
  issn      = {10495258},
  address   = {New Orleans, LA, United states},
  publisher = {Cambridge: MIT Press},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We show how transformers can be used to vastly simplify neural video compression. Previous methods have been relying on an increasing number of architectural biases and priors, including motion prediction and warping operations, resulting in complex models. Instead, we independently map input frames to representations and use a transformer to model their dependencies, letting it predict the distribution of future representations given the past. The resulting video compression transformer outperforms previous methods on standard video compression data sets. Experiments on synthetic data show that our model learns to handle complex motion patterns such as panning, blurring and fading purely from data. Our approach is easy to implement, and we release code to facilitate future research.<br/></div> &copy; 2022 Neural information processing systems foundation. All rights reserved.},
  journal   = {Advances in Neural Information Processing Systems}
}

@inproceedings{2022_Lecture_Ho_CANF-VC,
  language  = {English},
  copyright = {Compendex},
  title     = {CANF-VC: Conditional Augmented Normalizing Flows forVideo Compression},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author    = {Ho, Yung-Han and Chang, Chih-Peng and Chen, Peng-Yu and Gnutti, Alessandro and Peng, Wen-Hsiao},
  volume    = {13676 LNCS},
  year      = {2022},
  pages     = {207-223},
  issn      = {03029743},
  address   = {Tel Aviv, Israel},
  publisher = {Berlin: Springer},
  abstract  = {<div data-language="eng" data-ev-field="abstract">This paper presents an end-to-end learning-based video compression system, termed CANF-VC, based on conditional augmented normalizing flows (CANF). Most learned video compression systems adopt the same hybrid-based coding architecture as the traditional codecs. Recent research on conditional coding has shown the sub-optimality of the hybrid-based coding and opens up opportunities for deep generative models to take a key role in creating new coding frameworks. CANF-VC represents a new attempt that leverages the conditional ANF to learn a video generative model for conditional inter-frame coding. We choose ANF because it is a special type of generative model, which includes variational autoencoder as a special case and is able to achieve better expressiveness. CANF-VC also extends the idea of conditional coding to motion coding, forming a purely conditional coding framework. Extensive experimental results on commonly used datasets confirm the superiority of CANF-VC to the state-of-the-art methods. The source code of CANF-VC is available at https://github.com/NYCU-MAPL/CANF-VC.<br/></div> &copy; 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-031-19787-1_12},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2022_Lecture_Li_E-NeRV,
  language  = {English},
  copyright = {Compendex},
  title     = {E-NeRV: Expedite Neural Video Representation withDisentangled Spatial-Temporal Context},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author    = {Li, Zizhang and Wang, Mengmeng and Pi, Huaijin and Xu, Kechun and Mei, Jianbiao and Liu, Yong},
  volume    = {13695 LNCS},
  year      = {2022},
  pages     = {267-284},
  issn      = {03029743},
  address   = {Tel Aviv, Israel},
  publisher = {Berlin: Springer},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Recently, the image-wise implicit neural representation of videos, NeRV, has gained popularity for its promising results and swift speed compared to regular pixel-wise implicit representations. However, the redundant parameters within the network structure can cause a large model size when scaling up for desirable performance. The key reason of this phenomenon is the coupled formulation of NeRV, which outputs the spatial and temporal information of video frames directly from the frame index input. In this paper, we propose E-NeRV, which dramatically expedites NeRV by decomposing the image-wise implicit neural representation into separate spatial and temporal context. Under the guidance of this new formulation, our model greatly reduces the redundant model parameters, while retaining the representation ability. We experimentally find that our method can improve the performance to a large extent with fewer parameters, resulting in a more than 8 &times; faster speed on convergence. Code is available at https://github.com/kyleleey/E-NeRV.<br/></div> &copy; 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-031-19833-5_16},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2023_arXiv_Chen_HNeRV,
  language  = {English},
  copyright = {Compendex},
  title     = {HNeRV: A Hybrid Neural Representation for Videos},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author    = {Chen, Hao and Gwilliam, Matthew and Lim, Ser-Nam and Shrivastava, Abhinav},
  year      = {2023},
  issn      = {23318422},
  pages     = {10270-10279},
  publisher = {IEEE},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Implicit neural representations store videos as neural networks and have performed well for various vision tasks such as video compression and denoising. With frame index or positional index as input, implicit representations (NeRV, E-NeRV, etc.) reconstruct video frames from fixed and content-agnostic embeddings. Such embedding largely limits the regression capacity and internal generalization for video interpolation. In this paper, we propose a Hybrid Neural Representation for Videos (HNeRV), where a learnable encoder generates content-adaptive embeddings, which act as the decoder input. Besides the input embedding, we introduce HNeRV blocks, which ensure model parameters are evenly distributed across the entire network, such that higher layers (layers near the output) can have more capacity to store high-resolution content and video details. With content-adaptive embeddings and redesigned architecture, HNeRV outperforms implicit methods in video regression tasks for both reconstruction quality (+4.7 PSNR) and convergence speed (16&times; faster), and shows better internal generalization. As a simple and efficient video representation, HNeRV also shows decoding advantages for speed, flexibility, and deployment, compared to traditional codecs (H.264, H.265) and learning-based compression methods. Finally, we explore the effectiveness of HNeRV on downstream tasks such as video compression and video inpainting.<br/></div> &copy; 2023, CC BY-NC-SA.},
  url       = {http://dx.doi.org/10.48550/arXiv.2304.02633},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2019_ICCV_Djelouah_Neural,
  language  = {English},
  copyright = {Compendex},
  title     = {Neural inter-frame compression for video coding},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  author    = {Djelouah, Abdelaziz and Campos, Joaquim and Schaub-Meyer, Simone and Schroers, Christopher},
  volume    = {2019-October},
  year      = {2019},
  pages     = {6420-6428},
  issn      = {15505499},
  address   = {Seoul, Korea, Republic of},
  abstract  = {While there are many deep learning based approaches for single image compression, the field of end-to-end learned video coding has remained much less explored. Therefore, in this work we present an inter-frame compression approach for neural video coding that can seamlessly build up on different existing neural image codecs. Our end-to-end solution performs temporal prediction by optical flow based motion compensation in pixel space. The key insight is that we can increase both decoding efficiency and reconstruction quality by encoding the required information into a latent representation that directly decodes into motion and blending coefficients. In order to account for remaining prediction errors, residual information between the original image and the interpolated frame is needed. We propose to compute residuals directly in latent space instead of in pixel space as this allows to reuse the same image compression network for both key frames and intermediate frames. Our extended evaluation on different datasets and resolutions shows that the rate-distortion performance of our approach is competitive with existing state-of-the-art codecs.<br/> &copy; 2019 IEEE.},
  url       = {http://dx.doi.org/10.1109/ICCV.2019.00652},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision}
}

@inproceedings{2019_ICCV_Rippel_Learned,
  language  = {English},
  copyright = {Compendex},
  title     = {Learned video compression},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  author    = {Rippel, Oren and Nair, Sanjay and Lew, Carissa and Branson, Steve and Anderson, Alexander and Bourdev, Lubomir},
  volume    = {2019-October},
  year      = {2019},
  pages     = {3453-3462},
  issn      = {15505499},
  address   = {Seoul, Korea, Republic of},
  abstract  = {We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so. We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial codecs in the low-latency mode. On standard-definition videos, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger than our algorithm. On high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer from blocking artifacts and pixelation, and thus produces videos that are more visually pleasing. We propose two main contributions. The first is a novel architecture for video compression, which (1) generalizes motion estimation to perform any learned compensation beyond simple translations, (2) rather than strictly relying on previously transmitted reference frames, maintains a state of arbitrary information learned by the model, and (3) enables jointly compressing all transmitted signals (such as optical flow and residual). Secondly, we present a framework for ML-based spatial rate control - - a mechanism for assigning variable bitrates across space for each frame. This is a critical component for video coding, which to our knowledge had not been developed within a machine learning setting.<br/> &copy; 2019 IEEE.},
  url       = {http://dx.doi.org/10.1109/ICCV.2019.00355},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision}
}

@inproceedings{2019_CVPR_Lu_Dvc,
  language  = {English},
  copyright = {Compendex},
  title     = {Dvc: An end-to-end deep video compression framework},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Lu, Guo and Ouyang, Wanli and Xu, Dong and Zhang, Xiaoyun and Cai, Chunlei and Gao, Zhiyong},
  volume    = {2019-June},
  year      = {2019},
  pages     = {10998-11007},
  issn      = {10636919},
  address   = {Long Beach, CA, United states},
  abstract  = {Conventional video compression approaches use the predictive coding architecture and encode the corresponding motion information and residual information. In this paper, taking advantage of both classical architecture in the conventional video compression method and the powerful non-linear representation ability of neural networks, we propose the first end-to-end video compression deep model that jointly optimizes all the components for video compression. Specifically, learning based optical flow estimation is utilized to obtain the motion information and reconstruct the current frames. Then we employ two auto-encoder style neural networks to compress the corresponding motion and residual information. All the modules are jointly learned through a single loss function, in which they collaborate with each other by considering the trade-off between reducing the number of compression bits and improving quality of the decoded video. Experimental results show that the proposed approach can outperform the widely used video coding standard H.264 in terms of PSNR and be even on par with the latest standard H.265 in terms of MS-SSIM. Code is released at https://github.com/GuoLusjtu/DVC.<br/> &copy; 2019 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR.2019.01126},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2018_Lecture_Wu_Video,
  language  = {English},
  copyright = {Compendex},
  title     = {Video compression through image interpolation},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author    = {Wu, Chao-Yuan and Singhal, Nayan and Krahenbuhl, Philipp},
  volume    = {11212 LNCS},
  year      = {2018},
  pages     = {425-440},
  issn      = {03029743},
  address   = {Munich, Germany},
  abstract  = {An ever increasing amount of our digital communication, media consumption, and content creation revolves around videos. We share, watch, and archive many aspects of our lives through them, all of which are powered by strong video compression. Traditional video compression is laboriously hand designed and hand optimized. This paper presents an alternative in an end-to-end deep learning codec. Our codec builds on one simple idea: Video compression is repeated image interpolation. It thus benefits from recent advances in deep image interpolation and generation. Our deep video codec outperforms today&rsquo;s prevailing codecs, such as H.261, MPEG-4 Part 2, and performs on par with H.264.<br/> &copy; Springer Nature Switzerland AG 2018.},
  url       = {http://dx.doi.org/10.1007/978-3-030-01237-3_26},
  publisher = {Springer},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{2023_TCSVT_Zhang_Rethinking,
  language  = {English},
  copyright = {Compendex},
  title     = {Rethinking Semantic Image Compression: Scalable Representation with Cross-modality Transfer},
  booktitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  author    = {Zhang, Pingping and Wang, Shiqi and Wang, Meng and Li, Jiguo and Wang, Xu and Kwong, Sam},
  year      = {2023},
  pages     = {1-1},
  issn      = {10518215},
  abstract  = {<div data-language="eng" data-ev-field="abstract">This article proposes the &lt;italic&gt;scalable cross-modality compression&lt;/italic&gt; (SCMC) paradigm, in which the image compression problem is further cast into a representation task by hierarchically sketching the image with different modalities. Herein, we adopt the conceptual organization philosophy to model the overwhelmingly complicated visual patterns, based upon the semantic, structure, and signal level representation accounting for different tasks. The SCMC paradigm that incorporates the representation at different granularities supports diverse application scenarios, such as high-level semantic communication and low-level image reconstruction. The decoder, which enables the recovery of the visual information, benefits from the scalable coding based upon the semantic, structure, and signal layers. Qualitative and quantitative results demonstrate that the SCMC can convey accurate semantic and perceptual information of images, especially at low bitrates, and promising rate-distortion performance has been achieved compared to state-of-the-art methods. The code will be available online https://github.com/ppingzhang/SCMC.<br/></div> IEEE},
  url       = {http://dx.doi.org/10.1109/TCSVT.2023.3241225},
  publisher = {IEEE},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology}
}

@inproceedings{2022_ICLR_Zhu_TRANSFORMER-BASED,
  language  = {English},
  copyright = {Compendex},
  title     = {TRANSFORMER-BASED TRANSFORM CODING},
  booktitle = {ICLR 2022 - 10th International Conference on Learning Representations},
  author    = {Zhu, Yinhao and Yang, Yang and Cohen, Taco},
  year      = {2022},
  address   = {Virtual, Online},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Neural data compression based on nonlinear transform coding has made great progress over the last few years, mainly due to improvements in prior models, quantization methods and nonlinear transforms. A general trend in many recent works pushing the limit of rate-distortion performance is to use ever more expensive prior models that can lead to prohibitively slow decoding. Instead, we focus on more expressive transforms that result in a better rate-distortion-computation trade-off. Specifically, we show that nonlinear transforms built on Swin-transformers can achieve better compression efficiency than transforms built on convolutional neural networks (ConvNets), while requiring fewer parameters and shorter decoding time. Paired with a compute-efficient Channel-wise AutoRegressive Model prior, our SwinT-ChARM model outperforms VTM-12.1 by 3.68% in BD-rate on Kodak with comparable decoding speed. In P-frame video compression setting, we are able to outperform the popular ConvNet-based scale-space-flow model by 12.35% in BD-rate on UVG. We provide model scaling studies to verify the computational efficiency of the proposed solutions and conduct several analyses to reveal the source of coding gain of transformers over ConvNets, including better spatial decorrelation, flexible effective receptive field, and more localized response of latent pixels during progressive decoding.<br/></div> &copy; 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.},
  publisher = {ICLR},
  journal   = {ICLR 2022 - 10th International Conference on Learning Representations}
}

@inproceedings{2022_Lecture_Strumpler_Implicit,
  language  = {English},
  copyright = {Compendex},
  title     = {Implicit Neural Representations for Image Compression},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  author    = {Strumpler, Yannick and Postels, Janis and Yang, Ren and Gool, Luc Van and Tombari, Federico},
  volume    = {13686 LNCS},
  year      = {2022},
  pages     = {74-91},
  issn      = {03029743},
  address   = {Tel Aviv, Israel},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Implicit Neural Representations (INRs) gained attention as a novel and effective representation for various data types. Recently, prior work applied INRs to image compressing. Such compression algorithms are promising candidates as a general purpose approach for any coordinate-based data modality. However, in order to live up to this promise current INR-based compression algorithms need to improve their rate-distortion performance by a large margin. This work progresses on this problem. First, we propose meta-learned initializations for INR-based compression which improves rate-distortion performance. As a side effect it also leads to leads to faster convergence speed. Secondly, we introduce a simple yet highly effective change to the network architecture compared to prior work on INR-based compression. Namely, we combine SIREN networks with positional encodings which improves rate distortion performance. Our contributions to source compression with INRs vastly outperform prior work. We show that our INR-based compression algorithm, meta-learning combined with SIREN and positional encodings, outperforms JPEG2000 and Rate-Distortion Autoencoders on Kodak with 2x reduced dimensionality for the first time and closes the gap on full resolution images. To underline the generality of INR-based source compression, we further perform experiments on 3D shape compression where our method greatly outperforms Draco - a traditional compression algorithm.<br/></div> &copy; 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-031-19809-0_5},
  publisher = {Springer},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2022_CVPRW_Jiang_Online,
  language  = {English},
  copyright = {Compendex},
  title     = {Online Meta Adaptation for Variable-Rate Learned Image Compression},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  author    = {Jiang, Wei and Wang, Wei and Li, Songnan and Liu, Shan},
  volume    = {2022-June},
  year      = {2022},
  pages     = {497-505},
  issn      = {21607508},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">This work addresses two major issues of end-to-end learned image compression (LIC) based on deep neural networks: variable-rate learning where separate networks are required to generate compressed images with varying qualities, and the train-test mismatch between differentiable approximate quantization and true hard quantization. We introduce an online meta-learning (OML) setting for LIC, which combines ideas from meta learning and online learning in the conditional variational auto-encoder (CVAE) framework. By treating the conditional variables as meta parameters and treating the generated conditional features as meta priors, the desired reconstruction can be controlled by the meta parameters to accommodate compression with variable qualities. The online learning framework is used to update the meta parameters so that the conditional reconstruction is adaptively tuned for the current image. Through the OML mechanism, the meta parameters can be effectively updated through SGD. The conditional reconstruction is directly based on the quantized latent representation in the decoder network, and therefore helps to bridge the gap between the training estimation and true quantized latent distribution. Experiments demonstrate that our OML approach can be flexibly applied to different state-of-the-art LIC methods to achieve additional performance improvements with little computation and transmission overhead.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPRW56347.2022.00065},
  publisher = {IEEE},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}
}

@inproceedings{2022_CVPR_Zou_The,
  language  = {English},
  copyright = {Compendex},
  title     = {The Devil Is in the Details: Window-based Attention for Image Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Zou, Renjie and Song, Chunfeng and Zhang, Zhaoxiang},
  volume    = {2022-June},
  year      = {2022},
  pages     = {17471-17480},
  issn      = {10636919},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Learned image compression methods have exhibited superior rate-distortion performance than classical image compression standards. Most existing learned image compression models are based on Convolutional Neural Networks (CNNs). Despite great contributions, a main drawback of CNN based model is that its structure is not designed for capturing local redundancy, especially the nonrepetitive textures, which severely affects the reconstruction quality. Therefore, how to make full use of both global structure and local texture becomes the core problem for learning-based image compression. Inspired by recent progresses of Vision Transformer (ViT) and Swin Transformer, we found that combining the local-aware attention mechanism with the global-related feature learning could meet the expectation in image compression. In this paper, we first extensively study the effects of multiple kinds of attention mechanisms for local features learning, then introduce a more straightforward yet effective window-based local attention block. The proposed window-based attention is very flexible which could work as a plug-and-play component to enhance CNN and Transformer models. Moreover, we propose a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in the down-sampling encoder and up-sampling decoder. Extensive experimental evaluations have shown that the proposed method is effective and outperforms the state-of-the-art methods. The code is publicly available at https://github.com/Googolxx/STF.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.01697},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2022_CVPR_Kim_Joint,
  language  = {English},
  copyright = {Compendex},
  title     = {Joint Global and Local Hierarchical Priors for Learned Image Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Kim, Jun-Hyuk and Heo, Byeongho and Lee, Jong-Seok},
  volume    = {2022-June},
  year      = {2022},
  pages     = {5982-5991},
  issn      = {10636919},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Recently, learned image compression methods have out-performed traditional hand-crafted ones including BPG. One of the keys to this success is learned entropy models that estimate the probability distribution of the quantized latent representation. Like other vision tasks, most recent learned entropy models are based on convolutional neural networks (CNNs). However, CNNs have a limitation in modeling long-range dependencies due to their nature of local connectivity, which can be a significant bottleneck in image compression where reducing spatial redundancy is a key point. To overcome this issue, we propose a novel entropy model called Information Transformer (Informer) that exploits both global and local information in a content-dependent manner using an attention mechanism. Our experiments show that Informer improves rate-distortion performance over the state-of-the-art methods on the Kodak and Tecnick datasets without the quadratic computational complexity problem. Our source code is available at https://github.com/naver-ai/informer.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.00590},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2017_ICLR_Theis_Lossy,
  language  = {English},
  copyright = {Compendex},
  title     = {Lossy image compression with compressive autoencoders},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  publisher = {ICLR},
  author    = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszar, Ferenc},
  year      = {2017},
  address   = {Toulon, France},
  abstract  = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.<br/> &copy; ICLR 2019 - Conference Track Proceedings. All rights reserved.},
  journal   = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings}
}

@inproceedings{2022_CVPR_He_ELIC,
  language  = {English},
  copyright = {Compendex},
  title     = {ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {He, Dailan and Yang, Ziming and Peng, Weikun and Ma, Rui and Qin, Hongwei and Wang, Yan},
  volume    = {2022-June},
  year      = {2022},
  pages     = {5708-5717},
  issn      = {10636919},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Recently, learned image compression techniques have achieved remarkable performance, even surpassing the best manually designed lossy image coders. They are promising to be large-scale adopted. For the sake of practicality, a thorough investigation of the architecture design of learned image compression, regarding both compression performance and running speed, is essential. In this paper, we first propose uneven channel-conditional adaptive coding, motivated by the observation of energy compaction in learned image compression. Combining the proposed uneven grouping model with existing context models, we obtain a spatial-channel contextual adaptive model to improve the coding performance without damage to running speed. Then we study the structure of the main transform and propose an efficient model, ELIC, to achieve state-of-the-art speed and compression ability. With superior performance, the proposed model also supports extremely fast preview decoding and progressive decoding, which makes the coming application of learning-based image compression more promising.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.00563},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2022_ICLR_Qian_ENTROFORMER,
  language  = {English},
  copyright = {Compendex},
  title     = {ENTROFORMER: A TRANSFORMER-BASED ENTROPY MODEL FOR LEARNED IMAGE COMPRESSION},
  booktitle = {ICLR 2022 - 10th International Conference on Learning Representations},
  author    = {Qian, Yichen and Lin, Ming and Sun, Xiuyu and Tan, Zhiyu and Jin, Rong},
  year      = {2022},
  address   = {Virtual, Online},
  abstract  = {<div data-language="eng" data-ev-field="abstract">One critical component in lossy deep image compression is the entropy model, which predicts the probability distribution of the quantized latent representation in the encoding and decoding modules. Previous works build entropy models upon convolutional neural networks which are inefficient in capturing global dependencies. In this work, we propose a novel transformer-based entropy model, termed Entroformer, to capture long-range dependencies in probability distribution estimation effectively and efficiently. Different from vision transformers in image classification, the Entroformer is highly optimized for image compression, including a top-k self-attention and a diamond relative position encoding. Meanwhile, we further expand this architecture with a parallel bidirectional context model to speed up the decoding process. The experiments show that the Entroformer achieves state-of-the-art performance on image compression while being time-efficient. Code is available at https://github.com/damo-cv/entroformer.<br/></div> &copy; 2022 ICLR 2022 - 10th International Conference on Learning Representationss. All rights reserved.},
  publisher = {ICLR},
  journal   = {ICLR 2022 - 10th International Conference on Learning Representations}
}

@inproceedings{2021_CVPR_Deng_Deep,
  language  = {English},
  copyright = {Compendex},
  title     = {Deep Homography for Efficient Stereo Image Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Deng, Xin and Yang, Wenzhe and Yang, Ren and Xu, Mai and Liu, Enpeng and Feng, Qianhan and Timofte, Radu},
  year      = {2021},
  pages     = {1492-1501},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In this paper, we propose HESIC, an end-to-end trainable deep network for stereo image compression (SIC). To fully explore the mutual information across two stereo images, we use a deep regression model to estimate the homography matrix, i.e., H matrix. Then, the left image is spatially transformed by the H matrix, and only the residual information between the left and right images is encoded to save bit-rates. A two-branch auto-encoder architecture is adopted in HESIC, corresponding to the left and right images, respectively. For entropy coding, we use two conditional stereo entropy models, i.e., Gaussian mixture model (GMM) based and context based entropy models, to fully explore the correlation between the two images to reduce the coding bit-rates. In decoding, a cross quality enhancement module is proposed to enhance the image quality based on inverse H matrix. Experimental results show that our HESIC outperforms state-of-the-art SIC methods on InStereo2K and KITTI datasets both quantitatively and qualitatively. Code is available at https://github.com/ywz978020607/HESIC.<br/></div> &copy; 2021 IEEE},
  url       = {http://dx.doi.org/10.1109/CVPR46437.2021.00154},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@article{2022_TCSVT_Guo_Causal,
  title     = {Causal contextual prediction for learned image compression},
  author    = {Guo, Zongyu and Zhang, Zhizheng and Feng, Runsen and Chen, Zhibo},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume    = {32},
  number    = {4},
  pages     = {2329-2341},
  year      = {2021},
  url       = {http://dx.doi.org/10.1109/TCSVT.2021.3089491},
  publisher = {IEEE}
}

@inproceedings{2021_CVPR_Yang_Slimmable,
  language  = {English},
  copyright = {Compendex},
  title     = {Slimmable Compressive Autoencoders for Practical Neural Image Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Yang, Fei and Herranz, Luis and Cheng, Yongmei and Mozerov, Mikhail G.},
  year      = {2021},
  pages     = {4996-5005},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Neural image compression leverages deep neural networks to outperform traditional image codecs in rate-distortion performance. However, the resulting models are also heavy, computationally demanding and generally optimized for a single rate, limiting their practical use. Focusing on practical image compression, we propose slimmable compressive autoencoders (SlimCAEs), where rate (R) and distortion (D) are jointly optimized for different capacities. Once trained, encoders and decoders can be executed at different capacities, leading to different rates and complexities. We show that a successful implementation of SlimCAEs requires suitable capacity-specific RD tradeoffs. Our experiments show that SlimCAEs are highly flexible models that provide excellent rate-distortion performance, variable rate, and dynamic adjustment of memory, computational cost and latency, thus addressing the main requirements of practical image compression.<br/></div> &copy; 2021 IEEE},
  url       = {http://dx.doi.org/10.1109/CVPR46437.2021.00496},
  publisher = {IEEE},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2016_ICLR_Toderici_Variable,
  language  = {English},
  copyright = {Compendex},
  title     = {Variable rate image compression with recurrent neural networks},
  booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
  publisher = {ICLR},
  author    = {Toderici, George and OMalley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
  year      = {2016},
  address   = {San Juan, Puerto rico},
  abstract  = {A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32&times;32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more.<br/> &copy; ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.},
  journal   = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings}
}

@inproceedings{2018_CVPR_Johnston_Improved,
  language  = {English},
  copyright = {Compendex},
  title     = {Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Jin Hwang, Sung and Shor, Joel and Toderici, George},
  year      = {2018},
  pages     = {4385-4393},
  issn      = {10636919},
  address   = {Salt Lake City, UT, United states},
  abstract  = {We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result using a single model. First, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network's hidden state. Second, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions. Finally, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to multiple metrics. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well as recently published methods based on deep neural networks.<br/> &copy; 2018 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR.2018.00461},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2020_ICIP_Minnen_Channel-Wise,
  language  = {English},
  copyright = {Compendex},
  title     = {Channel-Wise Autoregressive Entropy Models for Learned Image Compression},
  booktitle = {Proceedings - International Conference on Image Processing, ICIP},
  publisher = {IEEE},
  author    = {Minnen, David and Singh, Saurabh},
  volume    = {2020-October},
  year      = {2020},
  pages     = {3339-3343},
  issn      = {15224880},
  address   = {Virtual, Abu Dhabi, United arab emirates},
  abstract  = {In learning-based approaches to image compression, codecs are developed by optimizing a computational model to minimize a rate-distortion objective. Currently, the most effective learned image codecs take the form of an entropy-constrained autoencoder with an entropy model that uses both forward and backward adaptation. Forward adaptation makes use of side information and can be efficiently integrated into a deep neural network. In contrast, backward adaptation typically makes predictions based on the causal context of each symbol, which requires serial processing that prevents efficient GPU / TPU utilization. We introduce two enhancements, channel-conditioning and latent residual prediction, that lead to network architectures with better rate-distortion performance than existing context-adaptive models while minimizing serial processing. Empirically, we see an average rate savings of 6.7% on the Kodak image set and 11.4% on the Tecnick image set compared to a context-adaptive baseline model. At low bit rates, where the improvements are most effective, our model saves up to 18% over the baseline and outperforms hand-engineered codecs like BPG by up to 25%.<br/> &copy; 2020 IEEE.},
  url       = {http://dx.doi.org/10.1109/ICIP40778.2020.9190935},
  journal   = {Proceedings - International Conference on Image Processing, ICIP}
}

@article{2019_arXiv_Yang_Variable,
  language  = {English},
  copyright = {Compendex},
  title     = {Variable rate deep image compression with modulated autoencoder},
  booktitle = {IEEE Signal Processing Letters},
  author    = {Yang, Fei and Herranz, Luis and van de Weijer, Joost and Iglesias Guitin, Jos A. and Lpez, Antonio and Mozerov, Mikhail},
  year      = {2019},
  issn      = {23318422},
  volume    = {27},
  pages     = {331-335},
  publisher = {IEEE},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Variable rate is a requirement for flexible and adaptable image and video compression. However, deep image compression methods are optimized for a single fixed rate-distortion tradeoff. While this can be addressed by training multiple models for different tradeoffs, the memory requirements increase proportionally to the number of models. Scaling the bottleneck representation of a shared autoencoder can provide variable rate compression with a single shared autoencoder. However, the R-D performance using this simple mechanism degrades in low bitrates, and also shrinks the effective range of bit rates. Addressing these limitations, we formulate the problem of variable rate-distortion optimization for deep image compression, and propose modulated autoencoders (MAEs), where the representations of a shared autoencoder are adapted to the specific rate-distortion tradeoff via a modulation network. Jointly training this modulated autoencoder and modulation network provides an effective way to navigate the R-D operational curve. Our experiments show that the proposed method can achieve almost the same R-D performance of independent models with significantly fewer parameters.<br/></div> Copyright &copy; 2019, The Authors. All rights reserved.},
  url       = { https://doi.org/10.1109/lsp.2020.2970539 },
  journal   = {IEEE Signal Processing Letters}
}

@inproceedings{2021_CVPR_Cui_Asymmetric,
  language  = {English},
  copyright = {Compendex},
  title     = {Asymmetric Gained Deep Image Compression with Continuous Rate Adaptation},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {Cui, Ze and Wang, Jing and Gao, Shangyin and Guo, Tiansheng and Feng, Yihui and Bai, Bo},
  year      = {2021},
  pages     = {10527-10536},
  issn      = {10636919},
  address   = {Virtual, Online, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">With the development of deep learning techniques, the combination of deep learning with image compression has drawn lots of attention. Recently, learned image compression methods had exceeded their classical counterparts in terms of rate-distortion performance. However, continuous rate adaptation remains an open question. Some learned image compression methods use multiple networks for multiple rates, while others use one single model at the expense of computational complexity increase and performance degradation. In this paper, we propose a continuously rate adjustable learned image compression framework, Asymmetric Gained Variational Autoencoder (AG-VAE). AG-VAE utilizes a pair of gain units to achieve discrete rate adaptation in one single model with a negligible additional computation. Then, by using exponential interpolation, continuous rate adaptation is achieved without compromising performance. Besides, we propose the asymmetric Gaussian entropy model for more accurate entropy estimation. Exhaustive experiments show that our method achieves comparable quantitative performance with SOTA learned image compression methods and better qualitative performance than classical image codecs. In the ablation study, we confirm the usefulness and superiority of gain units and the asymmetric Gaussian entropy model.<br/></div> &copy; 2021 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR46437.2021.01039},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@article{2022_arXiv_Dupont_COIN++,
  language  = {English},
  copyright = {Compendex},
  title     = {COIN++: Neural Compression Across Modalities},
  booktitle = {arXiv preprint arXiv:2201.12904},
  author    = {Dupont, Emilien and Loya, Hrushikesh and Alizadeh, Milad and Goliski, Adam and Teh, Yee Whye and Doucet, Arnaud},
  year      = {2022},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the feasibility of our method by compressing various data modalities, from images and audio to medical and climate data.<br/></div> Copyright &copy; 2022, The Authors. All rights reserved.},
  url       = {http://dx.doi.org/10.48550/arXiv.2201.12904},
  journal   = {arXiv preprint arXiv:2201.12904}
}

@article{2022_arXiv_Ramirez_L0onie,
  language  = {English},
  copyright = {Compendex},
  title     = {{$L_0$}onie: Compressing COINs with {$L_0$}-constraints},
  booktitle = {arXiv preprint arXiv:2207.04144},
  author    = {Ramirez, Juan and Gallego-Posada, Jose},
  year      = {2022},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Advances in Implicit Neural Representations (INR) have motivated research on domain-agnostic compression techniques. These methods train a neural network to approximate an object, and then store the weights of the trained model. For example, given an image, a network is trained to learn the mapping from pixel locations to RGB values. In this paper, we propose L<inf>0</inf>onie, a sparsity-constrained extension of the COIN compression method. Sparsity allows to leverage the faster learning of overparameterized networks, while retaining the desirable compression rate of smaller models. Moreover, our constrained formulation ensures that the final model respects a pre-determined compression rate, dispensing of the need for expensive architecture search.<br/></div> &copy; 2022, CC BY.},
  url       = {http://dx.doi.org/10.48550/arXiv.2207.04144},
  journal   = {arXiv preprint arXiv:2207.04144}
}

@article{2023_arXiv_Wang_EVC,
  language  = {English},
  copyright = {Compendex},
  title     = {EVC: TOWARDS REAL-TIME NEURAL IMAGE COMPRESSION WITH MASK DECAY},
  booktitle = {arXiv preprint arXiv:2302.05071},
  author    = {Wang, Guo-Hua and Li, Jiahao and Li, Bin and Lu, Yan},
  year      = {2023},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Neural image compression has surpassed state-of-the-art traditional codecs (H.266/VVC) for rate-distortion (RD) performance, but suffers from large complexity and separate models for different rate-distortion trade-offs. In this paper, we propose an Efficient single-model Variable-bit-rate Codec (EVC), which is able to run at 30 FPS with 768x512 input images and still outperforms VVC for the RD performance. By further reducing both encoder and decoder complexities, our small model even achieves 30 FPS with 1920x1080 input images. To bridge the performance gap between our different capacities models, we meticulously design the mask decay, which transforms the large model's parameters into the small model automatically. And a novel sparsity regularization loss is proposed to mitigate shortcomings of L<inf>p</inf>regularization. Our algorithm significantly narrows the performance gap by 50% and 30% for our medium and small models, respectively. At last, we advocate the scalable encoder for neural image compression. The encoding complexity is dynamic to meet different latency requirements. We propose decaying the large encoder multiple times to reduce the residual representation progressively. Both mask decay and residual representation learning greatly improve the RD performance of our scalable encoder. Our code is at https://github.com/microsoft/DCVC.<br/></div> Copyright &copy; 2023, The Authors. All rights reserved.},
  url       = {http://dx.doi.org/10.48550/arXiv.2302.05071},
  journal   = {arXiv preprint arXiv:2302.05071}
}

@article{2022_TOM_Sheng_Temporal,
  language  = {English},
  copyright = {Compendex},
  title     = {Temporal Context Mining for Learned Video Compression},
  booktitle = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  author    = {Sheng, Xihua and Li, Jiahao and Li, Bin and Li, Li and Liu, Dong and Lu, Yan},
  year      = {2022},
  pages     = {1-12},
  issn      = {15209210},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Applying deep learning to video compression has attracted increasing attention in recent few years. In this work, we address end-to-end learned video compression with a special focus on better learning and utilizing temporal contexts. We propose to propagate not only the last reconstructed frame but also the feature before obtaining the reconstructed frame for temporal context mining. From the propagated feature, we learn multi-scale temporal contexts and re-fill the learned temporal contexts into the modules of our compression scheme, including the contextual encoder-decoder, the frame generator, and the temporal context encoder. We discard the parallelization-unfriendly auto-regressive entropy model to pursue a more practical encoding and decoding time. Experimental results show that our proposed scheme achieves a higher compression ratio than the existing learned video codecs. Our scheme also outperforms x264 and x265 (representing industrial software for H.264 and H.265, respectively) as well as the official reference software for H.264, H.265, and H.266 (JM, HM, and VTM, respectively). Specifically, when intra period is 32 and oriented to PSNR, our scheme outperforms H.265&#x2013;HM by 14.4&#x0025; bit rate saving; when oriented to MS-SSIM, our scheme outperforms H.266&#x2013;VTM by 21.1&#x0025; bit rate saving.<br/></div> IEEE},
  url       = {http://dx.doi.org/10.1109/TMM.2022.3220421},
  journal   = {IEEE Transactions on Multimedia}
}

@article{2021_arXiv_Ladune_CONDITIONAL,
  language  = {English},
  copyright = {Compendex},
  title     = {CONDITIONAL CODING for FLEXIBLE LEARNED VIDEO COMPRESSION},
  booktitle = {arXiv preprint arXiv:2104.07930},
  author    = {Ladune, Theo and Philippe, Pierrick and Hamidouche, Wassim and Zhang, Lu and Deforges, Olivier},
  year      = {2021},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">This paper introduces a novel framework for end-to-end learned video coding. Image compression is generalized through conditional coding to exploit information from reference frames, allowing to process intra and inter frames with the same coder. The system is trained through the minimization of a rate-distortion cost, with no pre-training or proxy loss. Its flexibility is assessed under three coding configurations (All Intra, Low-delay P and Random Access), where it is shown to achieve performance competitive with the state-of-the-art video codec HEVC.<br/></div> Copyright &copy; 2021, The Authors. All rights reserved.},
  journal   = {arXiv preprint arXiv:2104.07930}
}

@inproceedings{2022_LNAI_Mentzer_Neural,
  language  = {English},
  copyright = {Compendex},
  title     = {Neural Video Compression Using GANs forDetail Synthesis andPropagation},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  author    = {Mentzer, Fabian and Agustsson, Eirikur and Balle, Johannes and Minnen, David and Johnston, Nick and Toderici, George},
  volume    = {13686 LNCS},
  year      = {2022},
  pages     = {562-578},
  issn      = {03029743},
  address   = {Tel Aviv, Israel},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We present the first neural video compression method based on generative adversarial networks (GANs). Our approach significantly outperforms previous neural and non-neural video compression methods in a user study, setting a new state-of-the-art in visual quality for neural methods. We show that the GAN loss is crucial to obtain this high visual quality. Two components make the GAN loss effective: we i) synthesize detail by conditioning the generator on a latent extracted from the warped previous reconstruction to then ii) propagate this detail with high-quality flow. We find that user studies are required to compare methods, i.e., none of our quantitative metrics were able to predict all studies. We present the network design choices in detail, and ablate them with user studies.<br/></div> &copy; 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-031-19809-0_32},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2020_LNAI_Sun_High-Quality,
  language  = {English},
  copyright = {Compendex},
  title     = {High-Quality Single-Model Deep Video Compression with Frame-Conv3D and Multi-frame Differential Modulation},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  author    = {Sun, Wenyu and Tang, Chen and Li, Weigui and Yuan, Zhuqing and Yang, Huazhong and Liu, Yongpan},
  volume    = {12375 LNCS},
  year      = {2020},
  pages     = {239-254},
  issn      = {03029743},
  address   = {Glasgow, United kingdom},
  abstract  = {Deep learning (DL) methods have revolutionized the paradigm of computer vision tasks and DL-based video compression is becoming a hot topic. This paper proposes a deep video compression method to simultaneously encode multiple frames with Frame-Conv3D and differential modulation. We first adopt Frame-Conv3D instead of traditional Channel-Conv3D for efficient multi-frame fusion. When generating the binary representation, the multi-frame differential modulation is utilized to alleviate the effect of quantization noise. By analyzing the forward and backward computing flow of the modulator, we identify that this technique can make full use of past frames information to remove the redundancy between multiple frames, thus achieves better performance. A dropout scheme combined with the differential modulator is proposed to enable bit rate optimization within a single model. Experimental results show that the proposed approach outperforms the H.264 and H.265 codecs in the region of low bit rate. Compared with recent DL-based methods, our model also achieves competitive performance.<br/> &copy; 2020, Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-030-58577-8_15},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2021_ICLR_Yang_HIERARCHICAL,
  language  = {English},
  copyright = {Compendex},
  title     = {HIERARCHICAL AUTOREGRESSIVE MODELING FOR NEURAL VIDEO COMPRESSION},
  booktitle = {ICLR 2021 - 9th International Conference on Learning Representations},
  publisher = {ICLR},
  author    = {Yang, Ruihan and Yang, Yibo and Marino, Joseph and Mandt, Stephan},
  year      = {2021},
  pages     = {Amazon;DeepMind;etal.;FacebookAI;Microsoft;OpenAI-},
  address   = {Virtual, Online},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Recent work by Marino et al. (2020) showed improved performance in sequential density estimation by combining masked autoregressive flows with hierarchical latent variable models. We draw a connection between such autoregressive generative models and the task of lossy video compression. Specifically, we view recent neural video compression methods (Lu et al., 2019; Yang et al., 2020b; Agustsson et al., 2020) as instances of a generalized stochastic temporal autoregressive transform, and propose avenues for enhancement based on this insight. Comprehensive evaluations on large-scale video data show improved rate-distortion performance over both state-of-the-art neural and conventional video compression methods.<br/></div> &copy; 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.},
  journal   = {ICLR 2021 - 9th International Conference on Learning Representations}
}

@article{2020_TCSVT_Liu_Neural,
  language  = {English},
  copyright = {Compendex},
  title     = {Neural Video Coding Using Multiscale Motion Compensation and Spatiotemporal Context Model},
  booktitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  author    = {Liu, Haojie and Lu, Ming and Ma, Zhan and Wang, Fan and Xie, Zhihuang and Cao, Xun and Wang, Yao},
  volume    = {31},
  number    = {8},
  year      = {2021},
  pages     = {3182-3196},
  issn      = {10518215},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Over the past two decades, traditional block-based video coding has made remarkable progress and spawned a series of well-known standards such as MPEG-4, H.264/AVC and H.265/HEVC. On the other hand, deep neural networks (DNNs) have shown their powerful capacity for visual content understanding, feature extraction and compact representation. Some previous works have explored the learnt video coding algorithms in an end-to-end manner, which show the great potential compared with traditional methods. In this paper, we propose an end-to-end deep neural video coding framework (NVC), which uses variational autoencoders (VAEs) with joint spatial and temporal prior aggregation (PA) to exploit the correlations in intra-frame pixels, inter-frame motions and inter-frame compensation residuals, respectively. Novel features of NVC include: 1) To estimate and compensate motion over a large range of magnitudes, we propose an unsupervised multiscale motion compensation network (MS-MCN) together with a pyramid decoder in the VAE for coding motion features that generates multiscale flow fields, 2) we design a novel adaptive spatiotemporal context model for efficient entropy coding for motion information, 3) we adopt nonlocal attention modules (NLAM) at the bottlenecks of the VAEs for implicit adaptive feature extraction and activation, leveraging its high transformation capacity and unequal weighting with joint global and local information, and 4) we introduce multi-module optimization and a multi-frame training strategy to minimize the temporal error propagation among P-frames. NVC is evaluated for the low-delay causal settings and compared with H.265/HEVC, H.264/AVC and the other learnt video compression methods following the common test conditions, demonstrating consistent gains across all popular test sequences for both PSNR and MS-SSIM distortion metrics.<br/></div> &copy; 1991-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/TCSVT.2020.3035680},
  publisher = {IEEE},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology}
}

@article{2022_TCSVT_Zhang_Learning-based,
  language  = {English},
  copyright = {Compendex},
  title     = {Learning-based Compression for Noisy Images in the Wild},
  booktitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  author    = {Zhang, Pingping and Wang, Meng and Chen, Baoliang and Lin, Rongqun and Wang, Xu and Wang, Shiqi and Kwong, Sam},
  year      = {2022},
  pages     = {1-1},
  issn      = {10518215},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Digital images in real world applications typically undergo a wide variety of quality degradations before compression or re-compression. Existing learning based codecs are typically data-driven, relying on the predefined compression pipeline with pristine or high quality images as the input. However, the images in the wild may exhibit the substantially different characteristics compared to the high quality images, casting major challenges to the learning based image coding. In this paper, we propose a robust noisy image compression framework with the blind assumption on the specific noise type and level. The specifically designed encoder decomposes the representation of visual content into two types of features, including the Features that represent the Intrinsic Content (FIC) and the Features that account for Additive Degradation (FAD). As such, beyond the philosophy of faithfully reconstructing the given image with high fidelity, only FIC needs to be compactly represented and conveyed. The principled disentanglement strategy facilitates the removal of the redundancy from multiple perspectives (e.g., spatial, channel and content), ensuring the handling of a wide variety of noisy images in the wild. Extensive experimental results show that our model can achieve superior performance in terms of the ultimate quality and exhibit the strong generalizability across images degraded by a variety of means. The proposed scheme also points out a new research avenue on learning based compression for images in the wild, which is technically challenging but desirable in practice.<br/></div> IEEE},
  url       = {http://dx.doi.org/10.1109/TCSVT.2022.3200763},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology}
}

@inproceedings{2022_ICIP_Peng_TEXTURE-GUIDED,
  language  = {English},
  copyright = {Compendex},
  title     = {TEXTURE-GUIDED END-TO-END DEPTH MAP COMPRESSION},
  booktitle = {Proceedings - International Conference on Image Processing, ICIP},
  publisher = {IEEE},
  author    = {Peng, Bo and Jing, Yuying and Jin, Dengchao and Liu, Xiangrui and Pan, Zhaoqing and Lei, Jianjun},
  year      = {2022},
  pages     = {2386-2390},
  issn      = {15224880},
  address   = {Bordeaux, France},
  abstract  = {<div data-language="eng" data-ev-field="abstract">End-to-end compression methods designed for the texture image have achieved excellent coding performances. Due to the characteristic differences between the depth map and the texture image, the texture-oriented methods have limitations in depth map compression. To address this problem, this paper proposes a texture-guided end-to-end depth map compression network (TDMC-Net). Specifically, the proposed TDMC-Net is mainly composed of the texture-guided transform module (TTM) which performs the nonlinear transform with providing the textual context to reduce the redundancy in depth feature, and a texture-guided conditional entropy model (TCEM) which is designed to improve the entropy model by introducing the texture conditional prior. Experimental results show that the proposed TDMC-Net boosts the depth coding efficiency by utilizing the texture information and achieves superior performance.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/ICIP46576.2022.9897569},
  journal   = {Proceedings - International Conference on Image Processing, ICIP}
}

@inproceedings{2022_CVPR_Wang_Neural,
  language  = {English},
  copyright = {Compendex},
  title     = {Neural Data-Dependent Transform for Learned Image Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {Wang, Dezhao and Yang, Wenhan and Hu, Yueyu and Liu, Jiaying},
  volume    = {2022-June},
  year      = {2022},
  pages     = {17358-17367},
  issn      = {10636919},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Learned image compression has achieved great success due to its excellent modeling capacity, but seldom further considers the Rate-Distortion Optimization (RDO) of each input image. To explore this potential in the learned codec, we make the first attempt to build a neural data-dependent transform and introduce a continuous online mode decision mechanism to jointly optimize the coding efficiency for each individual image. Specifically, apart from the image content stream, we employ an additional model stream to generate the transform parameters at the decoder side. The pres-ence of a model stream enables our model to learn more abstract neural-syntax, which helps cluster the latent repre-sentations of images more compactly. Beyond the transform stage, we also adopt neural-syntax based post-processing for the scenarios that require higher quality reconstructions regardless of extra decoding overhead. Moreover, the in-volvement of the model stream further makes it possible to optimize both the representation and the decoder in an on-line way, i. e. RDO at the testing time. It is equivalent to a continuous online mode decision, like coding modes in the traditional codecs, to improve the coding efficiency based on the individual input image. The experimental results show the effectiveness of the proposed neural-syntax de-sign and the continuous online mode decision mechanism, demonstrating the superiority of our method in coding effi-ciency. Our project is available at: https://dezhao-wang.github.io/Neural-Syntax-Website/.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.01686},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2022_CVPRW_He_PO-ELIC,
  language  = {English},
  copyright = {Compendex},
  title     = {PO-ELIC: Perception-Oriented Efficient Learned Image Coding},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  publisher = {IEEE},
  author    = {He, Dailan and Yang, Ziming and Yu, Hongjiu and Xu, Tongda and Luo, Jixiang and Chen, Yuan and Gao, Chenjian and Shi, Xinjie and Qin, Hongwei and Wang, Yan},
  volume    = {2022-June},
  year      = {2022},
  pages     = {1763-1768},
  issn      = {21607508},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In the past years, learned image compression (LIC) has achieved remarkable performance. The recent LIC methods outperform VVC in both PSNR and MS-SSIM. However, the low bit-rate reconstructions of LIC suffer from artifacts such as blurring, color drifting and texture missing. Moreover, those varied artifacts make image quality metrics correlate badly with human perceptual quality. In this paper, we propose PO-ELIC, i.e., Perception-Oriented Efficient Learned Image Coding. To be specific, we adapt ELIC, one of the state-of-the-art LIC models, with adversarial training techniques. We apply a mixture of losses including hinge-form adversarial loss, Charbonnier loss, and style loss, to finetune the model towards better perceptual quality. Experimental results demonstrate that our method achieves comparable perceptual quality with HiFiC with much lower bitrate.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPRW56347.2022.00187},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}
}

@inproceedings{2022_CVPRW_Brand_RDONet,
  language  = {English},
  copyright = {Compendex},
  title     = {RDONet: Rate-Distortion Optimized Learned Image Compression with Variable Depth},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  publisher = {IEEE},
  author    = {Brand, Fabian and Fischer, Kristian and Kopte, Alexander and Windsheimer, Marc and Kaup, Andre},
  volume    = {2022-June},
  year      = {2022},
  pages     = {1758-1762},
  issn      = {21607508},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Rate-distortion optimization (RDO) is responsible for large gains in image and video compression. While RDO is a standard tool in traditional image and video coding, it is not yet widely used in novel end-to-end trained neural methods. The major reason is that the decoding function is trained once and does not have free parameters. In this paper, we present RDONet, a network containing state-of-the-art components, which is perceptually optimized and capable of rate-distortion optimization. With this network, we are able to outperform VVC Intra on MS-SSIM and two different perceptual LPIPS metrics. This paper is part of the CLIC challenge, where we participate under the team name RDONet FAU.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPRW56347.2022.00186},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}
}

@inproceedings{2021_CVPRW_Cheng_Perceptual,
  language  = {English},
  copyright = {Compendex},
  title     = {Perceptual image compression using relativistic average least squares gans},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  publisher = {IEEE},
  author    = {Cheng, Zhengxue and Fu, Ting and Hu, Jiapeng and Guo, Li and Wang, Shihao and Zhao, Xiongxin and Zhou, Dajiang and Song, Yang},
  year      = {2021},
  pages     = {1895-1900},
  issn      = {21607508},
  address   = {Virtual, Online, TN, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In this work, we provide a detailed description on our submitted methods ANTxNN and ANTxNN_SSIM to Workshop and Challenge on Learned Image Compression (CLIC) 2021. We propose to incorporate Relativistic average Least Squares GANs (RaLSGANs) into Rate-Distortion Optimization for end-to-end training, to achieve perceptual image compression. We also compare two types of discriminator networks and visualize their reconstructed images. Experimental results have validated our method optimized by RaLSGANs can achieve higher subjective quality compared to PSNR, MS-SSIM or LPIPS-optimized models.<br/></div> &copy; 2021 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPRW53098.2021.00213},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}
}

@inproceedings{2021_CVPRW_Brand_Rate-distortion,
  language  = {English},
  copyright = {Compendex},
  title     = {Rate-distortion optimized learning-based image compression using an adaptive hierachical autoencoder with conditional hyperprior},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  publisher = {IEEE},
  author    = {Brand, Fabian and Fischer, Kristian and Kaup, Andre},
  year      = {2021},
  pages     = {1885-1889},
  issn      = {21607508},
  address   = {Virtual, Online, TN, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Deep-learning-based compressive autoencoders consist of a single non-linear function mapping the image to a latent space which is quantized and transmitted. Afterwards, a second non-linear function transforms the received latent space back to a reconstructed image. This method achieves superior quality than many traditional image coders, which is due to a non-linear generalization of linear transforms used in traditional coders. However, modern image and video coder achieve large coding gains by applying rate-distortion optimization on dynamic block-partitioning. In this paper, we present RDONet, a novel approach to achieve similar effects in compression with full image autoencoders by using different hierarchical levels, which are transmitted adaptively after performing an external rate-distortion optimization. Using our model, we are able to save up to 20% rate over comparable non-hierarchical models while maintaining the same quality.<br/></div> &copy; 2021 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPRW53098.2021.00211},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}
}

@inproceedings{2021_ICLR_Qian_LEARNING,
  language  = {English},
  copyright = {Compendex},
  title     = {LEARNING ACCURATE ENTROPY MODEL WITH GLOBAL REFERENCE FOR IMAGE COMPRESSION},
  booktitle = {ICLR 2021 - 9th International Conference on Learning Representations},
  publisher = {ICLR},
  author    = {Qian, Yichen and Tan, Zhiyu and Sun, Xiuyu and Lin, Ming and Li, Dongyang and Sun, Zhenhong and Li, Hao and Jin, Rong},
  year      = {2021},
  address   = {Virtual, Online},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In recent deep image compression neural networks, the entropy model plays a critical role in estimating the prior distribution of deep image encodings. Existing methods combine hyperprior with local context in the entropy estimation function. This greatly limits their performance due to the absence of a global vision. In this work, we propose a novel Global Reference Model for image compression to effectively leverage both the local and the global context information, leading to an enhanced compression rate. The proposed method scans decoded latents and then finds the most relevant latent to assist the distribution estimating of the current latent. A by-product of this work is the innovation of a mean-shifting GDN module that further improves the performance. Experimental results demonstrate that the proposed model outperforms the rate-distortion performance of most of the state-of-the-art methods in the industry.<br/></div> &copy; 2021 ICLR 2021 - 9th International Conference on Learning Representations. All rights reserved.},
  journal   = {ICLR 2021 - 9th International Conference on Learning Representations}
}

@inproceedings{2020_ICASSP_Chen_Variable,
  language  = {English},
  copyright = {Compendex},
  title     = {Variable Bitrate Image Compression with Quality Scaling Factors},
  booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  author    = {Chen, Tong and Ma, Zhan},
  publisher = {IEEE},
  volume    = {2020-May},
  year      = {2020},
  pages     = {2163-2167},
  issn      = {15206149},
  address   = {Barcelona, Spain},
  abstract  = {Recently, learned image compression has emerged with significant coding efficiency improvement, and even shown noticeable gains over the state-of-the-art traditional codecs. In the mean time, most existing methods need to train separate models for different bitrate target. In this paper, we propose to embed a set of quality scaling factors (SFs) into learned image compression network, by which we can encode images across an entire bitrate range with a single model. This solution offers the comparable performance with those default approaches requiring multiple bitrate dependent models, and reduces the complexity significantly for practical implementation. Our work also demonstrates the generalization for various compression network structures, image contents, and training loss functions.<br/> &copy; 2020 IEEE.},
  url       = {http://dx.doi.org/10.1109/ICASSP40776.2020.9053885},
  journal   = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings}
}

@inproceedings{2019_arXiv_Choi_Variable,
  language  = {English},
  copyright = {Compendex},
  title     = {Variable rate deep image compression with a conditional autoencoder},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  author    = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  publisher = {IEEE},
  year      = {2019},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In this paper, we propose a novel variable-rate learned image compression framework with a conditional autoencoder. Previous learning-based image compression methods mostly require training separate networks for different compression rates so they can yield compressed images of varying quality. In contrast, we train and deploy only one variable-rate image compression network implemented with a conditional autoencoder. We provide two rate control parameters, i.e., the Lagrange multiplier and the quantization bin size, which are given as conditioning variables to the network. Coarse rate adaptation to a target is performed by changing the Lagrange multiplier, while the rate can be further fine-tuned by adjusting the bin size used in quantizing the encoded representation. Our experimental results show that the proposed scheme provides a better rate-distortion trade-off than the traditional variable-rate image compression codecs such as JPEG2000 and BPG. Our model also shows comparable and sometimes better performance than the state-of-the-art learned image compression models that deploy multiple networks trained for varying rates.<br/></div> Copyright &copy; 2019, The Authors. All rights reserved.},
  url       = {https://doi.org/10.1109/iccv.2019.00324 },
  journal   = {Proceedings of the IEEE/CVF International Conference on Computer Vision}
}

@inproceedings{2019_ICCV_Agustsson_Generative,
  language  = {English},
  copyright = {Compendex},
  title     = {Generative adversarial networks for extreme learned image compression},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  publisher = {IEEE},
  author    = {Agustsson, Eirikur and Tschannen, Michael and Mentzer, Fabian and Timofte, Radu and Van Gool, Luc},
  volume    = {2019-October},
  year      = {2019},
  pages     = {221-231},
  issn      = {15505499},
  address   = {Seoul, Korea, Republic of},
  abstract  = {We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.<br/> &copy; 2019 IEEE.},
  url       = {http://dx.doi.org/10.1109/ICCV.2019.00031},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision}
}

@inproceedings{2019_CVPRW_Zhou_End-to-end,
  language  = {English},
  copyright = {Compendex},
  title     = {End-to-end optimized image compression with attention mechanism},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  publisher = {IEEE},
  author    = {Zhou, Lei and Sun, Zhenhong and Wu, Xiangji and Wu, Junmin},
  volume    = {2019-June},
  year      = {2019},
  issn      = {21607508},
  address   = {Long Beach, CA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We present an end-to-end trainable image compression framework for low bit-rate and transparent image compression. Our method is based on variational autoencoder, which consists of a nonlinear encoder transformation, a soft quantizer, a nonlinear decoder transformation and an entropy estimation module. The prior probability of the latent representations is modeled by combining a hyperprior autoencoder and a Pixelcnn++ based context module and they are trained jointly with the transformation autoencoder with attention mechanism. In order to improve the compression performance, a non-local convolution based attention mechanism is designed for allocating bits adaptively. Finally, a novel rate allocation algorithm based on linear optimization is used to assign the bits for each image dynamically, considering the bits constraint of the challenge. Across the experimental results on validation and test sets, the optimized framework can generate the highest PSNR and MS-SSIM for low bit-rate compression competition, and cost the lowest bytes for transparent 40db competition.<br/></div> &copy; 2019 IEEE Computer Society. All rights reserved.},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}
}

@inproceedings{2019_arXiv_Cheng_Deep,
  language  = {English},
  copyright = {Compendex},
  title     = {Deep residual learning for image compression},
  booktitle = {CVPR Workshops},
  author    = {Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
  pages     = {0},
  year      = {2019},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In this paper, we provide a detailed description on our approach designed for CVPR 2019 Workshop and Challenge on Learned Image Compression (CLIC). Our approach mainly consists of two proposals, i.e. deep residual learning for image compression and sub-pixel convolution as up-sampling operations. Experimental results have indicated that our approaches, Kattolab, Kattolabv2 and KattolabSSIM, achieve 0.972 in MS-SSIM at the rate constraint of 0.15bpp with moderate complexity during the validation phase.<br/></div> Copyright &copy; 2019, The Authors. All rights reserved.},
  journal   = {CVPR Workshops},
  publisher = {IEEE}
}

@inproceedings{2018_CVPR_Li_Learning,
  language  = {English},
  copyright = {Compendex},
  title     = {Learning Convolutional Networks for Content-Weighted Image Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {Li, Mu and Zuo, Wangmeng and Gu, Shuhang and Zhao, Debin and Zhang, David},
  year      = {2018},
  pages     = {3214-3223},
  issn      = {10636919},
  address   = {Salt Lake City, UT, United states},
  abstract  = {Lossy image compression is generally formulated as a joint rate-distortion optimization problem to learn encoder, quantizer, and decoder. Due to the non-differentiable quantizer and discrete entropy estimation, it is very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that: (i) the bit rate of the different parts of the image is adapted to local content, and (ii) the content-aware bit rate is allocated under the guidance of a content-weighted importance map. The sum of the importance map can thus serve as a continuous alternative of discrete entropy estimation to control compression rate. The binarizer is adopted to quantize the output of encoder and a proxy function is introduced for approximating binary operation in backward propagation to make it differentiable. The encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner. And a convolutional entropy encoder is further presented for lossless compression of importance map and binary codes. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts.<br/> &copy; 2018 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR.2018.00339},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@article{2019_arXiv_Johnston_Computationally,
  language  = {English},
  copyright = {Compendex},
  title     = {Computationally efficient neural image compression},
  booktitle = {arXiv preprint arXiv:1912.08771},
  author    = {Johnston, Nick and Eban, Elad and Gordon, Ariel and Balle, Johannes},
  year      = {2019},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Image compression using neural networks have reached or exceeded non-neural methods (such as JPEG, WebP, BPG). While these networks are state of the art in rate-distortion performance, computational feasibility of these models remains a challenge. We apply automatic network optimization techniques to reduce the computational complexity of a popular architecture used in neural image compression, analyze the decoder complexity in execution runtime and explore the trade-offs between two distortion metrics, rate-distortion performance and run-time performance to design and research more computationally efficient neural image compression. We find that our method decreases the decoder run-time requirements by over 50% for a state-of-the-art neural architecture.<br/></div> Copyright &copy; 2019, The Authors. All rights reserved.},
  journal   = {arXiv preprint arXiv:1912.08771}
}

@article{2023_TCSVT_Yang_Advancing,
  language  = {English},
  copyright = {Compendex},
  title     = {Advancing Learned Video Compression With In-Loop Frame Prediction},
  booktitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  author    = {Yang, Ren and Timofte, Radu and Van Gool, Luc},
  volume    = {33},
  number    = {5},
  year      = {2023},
  pages     = {2410-2423},
  issn      = {10518215},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Recent years have witnessed an increasing interest in end-to-end learned video compression. Most previous works explore temporal redundancy by detecting and compressing a motion map to warp the reference frame towards the target frame. Yet, it failed to adequately take advantage of the historical priors in the sequential reference frames. In this paper, we propose an Advanced Learned Video Compression (ALVC) approach with the in-loop frame prediction module, which is able to effectively predict the target frame from the previously compressed frames, without consuming any bit-rate. The predicted frame can serve as a better reference than the previously compressed frame, and therefore it benefits the compression performance. The proposed in-loop prediction module is a part of the end-to-end video compression and is jointly optimized in the whole framework. We propose the recurrent and the bi-directional in-loop prediction modules for compressing P-frames and B-frames, respectively. The experiments show the state-of-the-art performance of our ALVC approach in learned video compression. We also outperform the default hierarchical B mode of x265 in terms of PSNR and beat the slowest mode of the SSIM-tuned x265 on MS-SSIM. The project page: https://github.com/RenYang-home/ALVC.<br/></div> &copy; 1991-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/TCSVT.2022.3222418},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology}
}

@inproceedings{2023_arXiv_Li_Neural,
  language  = {English},
  copyright = {Compendex},
  title     = {Neural Video Compression with Diverse Contexts},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author    = {Li, Jiahao and Li, Bin and Lu, Yan},
  publisher = {IEEE},
  year      = {2023},
  pages     = {22616-22626},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">For any video codecs, the coding efficiency highly relies on whether the current signal to be encoded can find the relevant contexts from the previous reconstructed signals. Traditional codec has verified more contexts bring substantial coding gain, but in a time-consuming manner. However, for the emerging neural video codec (NVC), its contexts are still limited, leading to low compression ratio. To boost NVC, this paper proposes increasing the context diversity in both temporal and spatial dimensions. First, we guide the model to learn hierarchical quality patterns across frames, which enriches long-term and yet high-quality temporal contexts. Furthermore, to tap the potential of optical flow-based coding framework, we introduce a group-based offset diversity where the cross-group interaction is proposed for better context mining. In addition, this paper also adopts a quadtree-based partition to increase spatial context diversity when encoding the latent representation in parallel. Experiments show that our codec obtains 23.5% bitrate saving over previous SOTA NVC. Better yet, our codec has surpassed the under-developing next generation traditional codec/ECM in both RGB and YUV420 colorspaces, in terms of PSNR. The codes are at https://github.com/microsoft/DCVC.<br/></div> Copyright &copy; 2023, The Authors. All rights reserved.},
  url       = {http://dx.doi.org/10.48550/arXiv.2302.14402},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2023_arXiv_He_Towards,
  language  = {English},
  copyright = {Compendex},
  title     = {Towards Scalable Neural Representation for Diverse Videos},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author    = {He, Bo and Yang, Xitong and Wang, Hanyu and Wu, Zuxuan and Chen, Hao and Huang, Shuaiyi and Ren, Yixuan and Lim, Ser-Nam and Shrivastava, Abhinav},
  year      = {2023},
  pages     = {6132-6142},
  issn      = {23318422},
  publisher = {IEEE},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Implicit neural representations (INR) have gained increasing attention in representing 3D scenes and images, and have been recently applied to encode videos (e.g., NeRV [1], E-NeRV [2]). While achieving promising results, existing INR-based methods are limited to encoding a handful of short videos (e.g., seven 5-second videos in the UVG dataset) with redundant visual content, leading to a model design that fits individual video frames independently and is not efficiently scalable to a large number of diverse videos. This paper focuses on developing neural representations for a more practical setup &ndash; encoding long and/or a large number of videos with diverse visual content. We first show that instead of dividing videos into small subsets and encoding them with separate models, encoding long and diverse videos jointly with a unified model achieves better compression results. Based on this observation, we propose D-NeRV, a novel neural representation framework designed to encode diverse videos by (i) decoupling clip-specific visual content from motion information, (ii) introducing temporal reasoning into the implicit neural network, and (iii) employing the task-oriented flow as intermediate output to reduce spatial redundancies. Our new model largely surpasses NeRV and traditional video compression techniques on UCF101 and UVG datasets on the video compression task. Moreover, when used as an efficient data-loader, D-NeRV achieves 3%-10% higher accuracy than NeRV on action recognition tasks on the UCF101 dataset under the same compression ratios.<br/></div> Copyright &copy; 2023, The Authors. All rights reserved.},
  url       = {http://dx.doi.org/10.48550/arXiv.2303.14124},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2023_WACV_Pourreza_Boosting,
  language  = {English},
  copyright = {Compendex},
  title     = {Boosting neural video codecs by exploiting hierarchical redundancy},
  booktitle = {Proceedings - 2023 IEEE Winter Conference on Applications of Computer Vision, WACV 2023},
  publisher = {IEEE},
  author    = {Pourreza, Reza and Le, Hoang and Said, Amir and Sautiere, Guillaume and Wiggers, Auke},
  year      = {2023},
  pages     = {5344-5353},
  address   = {Waikoloa, HI, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In video compression, coding efficiency is improved by reusing pixels from previously decoded frames via motion and residual compensation. We define two levels of hierarchical redundancy in video frames: 1) first-order: redundancy in pixel space, i.e., similarities in pixel values across neighboring frames, which is effectively captured using motion and residual compensation, 2) second-order: redundancy in motion and residual maps due to smooth motion in natural videos. While most of the existing neural video coding literature addresses first-order redundancy, we tackle the problem of capturing second-order redundancy in neural video codecs via predictors. We introduce generic motion and residual predictors that learn to extrapolate from previously decoded data. These predictors are lightweight, and can be employed with most neural video codecs in order to improve their rate-distortion performance. Moreover, while RGB is the dominant colorspace in neural video coding literature, we introduce general modifications for neural video codecs to embrace the YUV420 colorspace and report YUV420 results. Our experiments show that using our predictors with a well-known neural video codec leads to 38% and 34% bitrate savings in RGB and YUV420 colorspaces measured on the UVG dataset.<br/></div> &copy; 2023 IEEE.},
  url       = {http://dx.doi.org/10.1109/WACV56688.2023.00532},
  journal   = {Proceedings - 2023 IEEE Winter Conference on Applications of Computer Vision, WACV 2023}
}

@inproceedings{2023_WACV_Fathima_A,
  language  = {English},
  copyright = {Compendex},
  title     = {A neural video codec with spatial rate-distortion control},
  booktitle = {Proceedings - 2023 IEEE Winter Conference on Applications of Computer Vision, WACV 2023},
  publisher = {IEEE},
  author    = {Fathima, Noor and Petersen, Jens and Sautiere, Guillaume and Wiggers, Auke and Pourreza, Reza},
  year      = {2023},
  pages     = {5354-5363},
  address   = {Waikoloa, HI, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Neural video compression algorithms are nearly competitive with hand-crafted codecs in terms of rate-distortion performance and subjective quality. However, many neural codecs are inflexible black boxes, and give users little to no control over the reconstruction quality and bitrate. In this work, we present a flexible neural video codec that combines ideas from variable-bitrate codecs and region-of-interest-based coding. By conditioning our model on a global rate-distortion tradeoff parameter and a region-of-interest (ROI) mask, we obtain dynamic control over the per-frame bitrate and the reconstruction quality in the ROI at test time. The resulting codec enables practical use cases such as coding under bitrate constraints with fixed ROI quality, while taking a negligible hit in performance compared to a fixed-rate model. We find that our codec performs best on sequences with complex motion, where we substantially outperform non-ROI codecs in the region of interest with Bjontegaard-Delta rate savings exceeding 60%.<br/></div> &copy; 2023 IEEE.},
  url       = {http://dx.doi.org/10.1109/WACV56688.2023.00533},
  journal   = {Proceedings - 2023 IEEE Winter Conference on Applications of Computer Vision, WACV 2023}
}

@article{2023_TCSVT_Lan_Multisensor,
  language  = {English},
  copyright = {Compendex},
  title     = {Multisensor Collaboration Network for Video Compression Based on Wavelet Decomposition},
  booktitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  author    = {Lan, Hui and Ji, Zhe and Jung, Cheolkon and Zou, Dan and Li, Ming},
  volume    = {33},
  number    = {1},
  year      = {2023},
  pages     = {434-444},
  issn      = {10518215},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Multi-sensor data has significant advantages over single sensor data due to the unique property of each sensor. We discover that multi-sensor collaboration can save bits and achieve stable compression performance according to quantization parameter (QP). In this paper, we propose a multi-sensor collaboration network for video compression based on wavelet decomposition, called MSCN. We introduce MSCN into 3D video coding based on color and depth sensors. The images acquired by a color sensor represent color and texture of the scene, while the images obtained by a depth sensor represent 3D geometric shape of the scene objects. Two sensor data are complementary, and color images help to reconstruct their corresponding depth images. First, we perform uniform sampling on the input depth video. Then, we compress the color and downsampled depth videos using 3D-HEVC codec. Finally, we reconstruct the depth video from the decoded color and depth videos by color guided depth super-resolution (SR). Experimental results show that MSCN achieves average BD-rate reductions of {-9.3%, -65.6%, -66.3%} and {-6.2%, -67.7%, and -69.2%} on 3D-HEVC test datasets for sampling factors 1, 2 and 4 in Random Access (RA) and All Intra (AI) configurations, respectively. Moreover, they verify that multi-sensor collaboration remarkably saves bits in video compression.<br/></div> &copy; 1991-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/TCSVT.2022.3201697},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology}
}

@article{2023_JCVIR_Jeny_Optimized,
  language  = {English},
  copyright = {Compendex},
  title     = {Optimized video compression with residual split attention and swin-block artifact contraction},
  booktitle = {Journal of Visual Communication and Image Representation},
  author    = {Jeny, Afsana Ahsan and Islam, Md Baharul},
  volume    = {90},
  year      = {2023},
  issn      = {10473203},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Research in video compression has seen significant advancement in the last several years. However, the existing deep learning-based algorithms continue to be plagued by erroneous motion compression and ineffective motion compensation architectures, resulting in compression errors with a lower rate&ndash;distortion trade-off. To overcome these challenges, we present an end-to-end purely deep learning-based video compression method through a set of primary operations (e.g., motion estimation, motion compression, motion compensation, residual compression, and artifact contraction) differently. A deep residual attention split (DRAS) block is introduced for motion compression networks to pay more attention to certain image regions to create more effective features for the decoder while boosting the rate&ndash;distortion optimization (RDO) efficiency. A channel residual block (CRB) is proposed in motion compensation to yield a more accurate predicted frame, potentially improving the residual frame. To mitigate the compression errors, an artifact contraction module (ACM) by residual swin convolution UNet block is included in this model to improve the reconstruction quality. To improve the final frame, a buffer is added to fine-tune the previous reference frames. These modules combine with a loss function by assessing the trade-off and enhancing the decoded video quality. A comprehensive ablation study demonstrates the effectiveness of the proposed blocks and modules for video compression. Experimental results show the competitive performance of the proposed method on four benchmark datasets.<br/></div> &copy; 2022 Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.jvcir.2022.103737},
  journal   = {Journal of Visual Communication and Image Representation}
}

@article{2023_TCSVT_Lin_DMVC,
  language  = {English},
  copyright = {Compendex},
  title     = {DMVC: Decomposed Motion Modeling for Learned Video Compression},
  booktitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  author    = {Lin, Kai and Jia, Chuanmin and Zhang, Xinfeng and Wang, Shanshe and Ma, Siwei and Gao, Wen},
  volume    = {33},
  number    = {7},
  year      = {2023},
  pages     = {3502-3515},
  issn      = {10518215},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Inter prediction is the critical component in hybrid coding framework to deal with the temporal redundancy. Most of the neural video coding methods typically follow the motion compensation based inter coding scheme, establishing motion vector (MV) as the central role. In this paper, we innovatively propose an efficient motion modeling approach by inherently decomposing it into two components, the intrinsic motion and the compensatory motion. The intrinsic motion originates from the implicit spatiotemporal context hidden in the historical sequence, which can be intuitively captured free of bits. On the top of it, the compensatory motion acts a role of structural refinement and texture enhancement as a form of side information. In particular, the inter prediction is performed in the feature space as a manner of progressive temporal transition, conditioned on the decomposed motion. By the motion decomposition paradigm, we innovatively answer the question of motion representation, compensation and coding in the learned video compression framework. With the temporal prediction, the remaining pixel residue is signaled to obtain the reconstruction. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art coding performance on par with other end-to-end coding methods, and outperforms versatile video coding (VVC) under low-delay P (LDP) configuration in terms of MS-SSIM metric.<br/></div> &copy; 1991-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/TCSVT.2022.3233221},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology}
}

@article{2022_TCSVT_Huang_HMFVC,
  language  = {English},
  copyright = {Compendex},
  title     = {HMFVC: A Human-Machine Friendly Video Compression Scheme},
  booktitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  author    = {Huang, Zhimeng and Jia, Chuanmin and Wang, Shanshe and Ma, Siwei},
  year      = {2022},
  pages     = {1-1},
  issn      = {10518215},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Recent studies on Video Coding for Machine (VCM) have achieved remarkable results. However, in practical visual-analytic applications, except for transmitting the visual feature for machine analysis, video textures are also mandatory for human monitoring and decision-making. To this end, this paper proposes a human-machine friendly video compression scheme (HMFVC) which can satisfy both human viewing and machine analysis well. First, we propose a learned semantic representation (LSR) method to extract semantic information between temporal neighboring frames. LSR could be utilized in signal reconstruction for human viewing and visual analysis for machine understanding. Second, given the proposed LSR, we design an end-to-end optimized video compression framework to jointly optimize the visual quality for human perception, analysis accuracy for machines, and compression efficiency as well. Finally, an HMFVC codec is developed, which can achieve higher action recognition accuracy and better reconstruction quality than the traditional codecs and learned video compression approaches. Specifically, HMFVC saves 77% bitrate to achieve the same analysis performance with the original videos compared to x265. To our knowledge, HMFVC is the first end-to-end optimized video compression scheme to serve both humans and machines. It is a promising framework for human-machine friendly video compression approaches.<br/></div> IEEE},
  url       = {http://dx.doi.org/10.1109/TCSVT.2022.3207596},
  journal   = {IEEE Transactions on Circuits and Systems for Video Technology}
}

@inproceedings{2022_TCSVT_Zhang_Optimized,
  language  = {English},
  copyright = {Compendex},
  title     = {Optimized Bit Allocation for Learning-based Video Compression},
  booktitle = {Proceedings - IEEE International Symposium on Circuits and Systems},
  publisher = {IEEE},
  author    = {Zhang, Zhaobin and Li, Yue and Zhang, Kai and Zhang, Li and He, Yuwen},
  volume    = {2022-May},
  year      = {2022},
  pages     = {1938-1942},
  issn      = {02714310},
  address   = {Austin, TX, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">The optimized bit allocation among frames has been intensively explored and improved the compression performance significantly in conventional video coding. However, the optimized bit allocation is still in its infant stage for learning-based video coding. Most existing learning-based video compression methods either use uniform bit allocation or empirically determined bit allocation weights among frames. In this paper, we develop an optimized bit allocation scheme for learning-based end-to-end video compression. In particular, we realize a hierarchical quality-control mechanism based on the importance of different frames under random-access scenarios. Considering the varying importance of frames on different temporal layers, we propose an efficient yet simple scheme, in which a set of optimized bit allocation weights are introduced to the rate-distortion (R-D) loss function. Experimental results demonstrate the effectiveness of the proposed scheme. In addition, the proposed scheme can be easily applied to most existing learning-based video compression frameworks under random-access scenarios.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/ISCAS48785.2022.9937681},
  journal   = {Proceedings - IEEE International Symposium on Circuits and Systems}
}

@inproceedings{2022_TCSVT_Man_Deep,
  language  = {English},
  copyright = {Compendex},
  title     = {Deep Learning-Assisted Video Compression Framework},
  booktitle = {Proceedings - IEEE International Symposium on Circuits and Systems},
  publisher = {IEEE},
  author    = {Man, Hengyu and Yu, Chang and Xing, Feng and Cheng, Yang and Zheng, Bo and Fan, Xiaopeng},
  volume    = {2022-May},
  year      = {2022},
  pages     = {3210-3214},
  issn      = {02714310},
  address   = {Austin, TX, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Deep learning has shown its great potential in video coding. Lots of research has been done regarding deep tools used within traditional schemes or end-to-end deep codec. In this paper, we propose multiple novel deep tools focusing on intra prediction, motion compensation refinement, virtual reference frame, and post-processing and form a deep learning-assisted video compression framework. For intra prediction, a novel data clustering-driven neural network (DCDNN), which could learn deep features of the clustered data, is proposed to assist intra angular modes. For inter prediction, a neural network-based motion compensation refinement (NNMCR) algorithm is applied to enhance the motion compensation. Meanwhile, a V-RF network is employed to generate a more reliable reference frame that can characterize complex motions in natural video signals. For post-processing, a quality adaptive neural network-based in-loop filter (QANNLF) is designed to utilize Quality Parameter (QP) information in the encoder to adjust filter strength for reconstructed videos with various qualities. Our method can obtain on average 14.83% BD-rate saving for the luma component under the all intra configuration and 12.16% under the random access configuration compared with HEVC reference software 16.9.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/ISCAS48785.2022.9937452},
  journal   = {Proceedings - IEEE International Symposium on Circuits and Systems}
}

@inproceedings{2022_LNCS_Shi_AlphaVC,
  language  = {English},
  copyright = {Compendex},
  title     = {AlphaVC: High-Performance andEfficient Learned Video Compression},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  author    = {Shi, Yibo and Ge, Yunying and Wang, Jing and Mao, Jue},
  volume    = {13679 LNCS},
  year      = {2022},
  pages     = {616-631},
  issn      = {03029743},
  address   = {Tel Aviv, Israel},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Recently, learned video compression has drawn lots of attention and show a rapid development trend with promising results. However, the previous works still suffer from some critical issues and have a performance gap with traditional compression standards in terms of widely used PSNR metric. In this paper, we propose several techniques to effectively improve the performance. First, to address the problem of accumulative error, we introduce a conditional-I-frame as the first frame in the GoP, which stabilizes the reconstructed quality and saves the bit-rate. Second, to efficiently improve the accuracy of inter prediction without increasing the complexity of decoder, we propose a pixel-to-feature motion prediction method at encoder side that helps us to obtain high-quality motion information. Third, we propose a probability-based entropy skipping method, which not only brings performance gain, but also greatly reduces the runtime of entropy coding. With these powerful techniques, this paper proposes AlphaVC, a high-performance and efficient learned video compression scheme. To the best of our knowledge, AlphaVC is the first E2E AI codec that exceeds the latest compression standard VVC on all common test datasets for both PSNR (&minus;28.2% BD-rate saving) and MSSSIM (&minus;52.2% BD-rate saving), and has very fast encoding (0.001x VVC) and decoding (1.69x VVC) speeds.<br/></div> &copy; 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-031-19800-7_36},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2022_CVPRW_Yang_Learned,
  language  = {English},
  copyright = {Compendex},
  title     = {Learned Low Bitrate Video Compression with Space-Time Super-Resolution},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  publisher = {IEEE},
  author    = {Yang, Jiayu and Yang, Chunhui and Xiong, Fei and Wang, Feng and Wang, Ronggang},
  volume    = {2022-June},
  year      = {2022},
  pages     = {1785-1789},
  issn      = {21607508},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">This paper presents a learned low bitrate video compression framework that consists of pre-processing, compression and post-processing. In pre-processing stage, the source videos are optionally reduced to low-resolution or low-frame-rate ones to better meet with the limited band-width. In compression stage, inter-frame prediction is performed by deformable convolution (DCN). The predicted frame is then used as temporal conditions to compress the current frame. In post-processing stage, the decoded videos are fed into a Space-Time Super-Resolution module, in which the videos are restored to original spatial and temporal resolutions. Experimental results on CLIC22 video test conditions demonstrate that the proposed method shows better performance on both objective and subjective quality at low bitrate. Our team name is PKUSZ-LVC.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPRW56347.2022.00192},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}
}

@inproceedings{2022_CVPR_Lu_Learning,
  language  = {English},
  copyright = {Compendex},
  title     = {Learning based Multi-modality Image and Video Compression},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {Lu, Guo and Zhong, Tianxiong and Geng, Jing and Hu, Qiang and Xu, Dong},
  volume    = {2022-June},
  year      = {2022},
  pages     = {6073-6082},
  issn      = {10636919},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Multi-modality (i.e., multi-sensor) data is widely used in various vision tasks for more accurate or robust perception. However, the increased data modalities bring new challenges for data storage and transmission. The existing data compression approaches usually adopt individual codecs for each modality without considering the correlation between different modalities. This work proposes a multi-modality compression framework for infrared and visible image pairs by exploiting the cross-modality redun-dancy. Specifically, given the image in the reference modality (e.g., the infrared image), we use the channel-wise alignment module to produce the aligned features based on the affine transform. Then the aligned feature is used as the context information for compressing the image in the current modality (e.g., the visible image), and the corresponding affine coefficients are losslessly compressed at negligible cost. Furthermore, we introduce the Transformer-based spatial alignment module to exploit the correlation between the intermediate features in the decoding procedures for different modalities. Our framework is very flexible and easily extended for multi-modality video compression. Experimental results show our proposed framework outperforms the traditional and learning-based single modality compression methods on the FLIR and KAIST datasets.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.00599},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2022_CVPR_Chen_LSVC,
  language  = {English},
  copyright = {Compendex},
  title     = {LSVC: A Learning-based Stereo Video Compression Framework},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE},
  author    = {Chen, Zhenghao and Lu, Guo and Hu, Zhihao and Liu, Shan and Jiang, Wei and Xu, Dong},
  volume    = {2022-June},
  year      = {2022},
  pages     = {6063-6072},
  issn      = {10636919},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In this work, we propose the first end-to-end optimized framework for compressing automotive stereo videos (i.e., stereo videos from autonomous driving applications) from both left and right views. Specifically, when compressing the current frame from each view, our framework reduces temporal redundancy by performing motion compensation using the reconstructed intra-view adjacent frame and at the same time exploits binocular redundancy by conducting disparity compensation using the latest reconstructed cross-view frame. Moreover, to effectively compress the introduced motion and disparity offsets for better compensation, we further propose two novel schemes called motion residual compression and disparity residual compression to respectively generate the predicted motion offset and disparity offset from the previously compressed motion offset and disparity offset, such that we can more effectively compress residual offset information for better bit-rate saving. Overall, the entire framework is implemented by the fully-differentiable modules and can be optimized in an end-to-end manner. Our comprehensive experiments on three automotive stereo video benchmarks Cityscapes, KITTI 2012 and KITTI 2015 demonstrate that our proposed framework outperforms the learning-based single-view video codec and the traditional hand-crafted multi-view video codec.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.00598},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@article{2020_arXiv_Begaint_CompressAI,
  language  = {English},
  copyright = {Compendex},
  title     = {CompressAI: a PyTorch library and evaluation platform for end-to-end compression research},
  booktitle = {arXiv preprint arXiv:2011.03029},
  author    = {Begaint, Jean and Feltman, Simon and Racape, Fabien and Pushparaja, Akshay},
  year      = {2020},
  issn      = {23318422},
  journal   = {arXiv preprint arXiv:2011.03029}
}

@inproceedings{2022_ACMMM_Zhao_Learning-Based,
  language  = {English},
  copyright = {Compendex},
  title     = {Learning-Based Video Coding with Joint Deep Compression and Enhancement},
  booktitle = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia},
  publisher = {ACM},
  author    = {Zhao, Tiesong and Feng, Weize and Zeng, HongJi and Xu, Yiwen and Niu, Yuzhen and Liu, Jiaying},
  year      = {2022},
  pages     = {3045-3054},
  address   = {Lisboa, Portugal},
  abstract  = {<div data-language="eng" data-ev-field="abstract">End-to-end learning-based video coding has attracted substantial attentions by compressing video signals as stacked visual features. This paper proposes an end-to-end deep video codec with jointly optimized compression and enhancement modules (JCEVC). First, we propose a dual-path generative adversarial network (DPEG) to reconstruct video details after compression. An &alpha;-path and a &beta;-path concurrently reconstruct the structure information and local textures. Second, we reuse the DPEG network in both motion compensation and quality enhancement modules, which are further combined with other necessary modules to formulate our JCEVC framework. Third, we employ a joint training of deep video compression and enhancement that further improves the rate-distortion (RD) performance of compression. Compared with x265 LDP very fast mode, our JCEVC reduces the average bit-per-pixel (bpp) by 39.39%/54.92% at the same PSNR/MS-SSIM, which outperforms the state-of-the-art deep video codecs by a considerable margin. Sourcecode is available at: https://github.com/fwz1021/JCEVC.<br/></div> &copy; 2022 ACM.},
  url       = {http://dx.doi.org/10.1145/3503161.3548314},
  journal   = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia}
}

@inproceedings{2022_ACMMM_Shukor_Video,
  language  = {English},
  copyright = {Compendex},
  title     = {Video Coding using Learned Latent GAN Compression},
  booktitle = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia},
  publisher = {ACM},
  author    = {Shukor, Mustafa and Damodaran, Bharath Bhushan and Yao, Xu and Hellier, Pierre},
  year      = {2022},
  pages     = {2239-2248},
  address   = {Lisboa, Portugal},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We propose in this paper a new paradigm for facial video compression. We leverage the generative capacity of GANs such as StyleGAN to represent and compress a video, including intra and inter compression. Each frame is inverted in the latent space of StyleGAN, from which the optimal compression is learned. To do so, a diffeomorphic latent representation is learned using a normalizing flows model, where an entropy model can be optimized for image coding. In addition, we propose a new perceptual loss that is more efficient than other counterparts. Finally, an entropy model for video inter coding with residual is also learned in the previously constructed latent representation. Our method (SGANC) is simple, faster to train, and achieves better results for image and video coding compared to state-of-the-art codecs such as VTM, AV1, and recent deep learning techniques. In particular, it drastically minimizes perceptual distortion at low bit rates.<br/></div> &copy; 2022 ACM.},
  url       = {http://dx.doi.org/10.1145/3503161.3548219},
  journal   = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia}
}

@inproceedings{2022_ACMMM_Li_Hybrid,
  language  = {English},
  copyright = {Compendex},
  title     = {Hybrid Spatial-Temporal Entropy Modelling for Neural Video Compression},
  booktitle = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia},
  publisher = {ACM},
  author    = {Li, Jiahao and Li, Bin and Lu, Yan},
  year      = {2022},
  pages     = {1503-1511},
  address   = {Lisboa, Portugal},
  abstract  = {<div data-language="eng" data-ev-field="abstract">For neural video codec, it is critical, yet challenging, to design an efficient entropy model which can accurately predict the probability distribution of the quantized latent representation. However, most existing video codecs directly use the ready-made entropy model from image codec to encode the residual or motion, and do not fully leverage the spatial-temporal characteristics in video. To this end, this paper proposes a powerful entropy model which efficiently captures both spatial and temporal dependencies. In particular, we introduce the latent prior which exploits the correlation among the latent representation to squeeze the temporal redundancy. Meanwhile, the dual spatial prior is proposed to reduce the spatial redundancy in a parallel-friendly manner. In addition, our entropy model is also versatile. Besides estimating the probability distribution, our entropy model also generates the quantization step at spatial-channel-wise. This content-adaptive quantization mechanism not only helps our codec achieve the smooth rate adjustment in single model but also improves the final rate-distortion performance by dynamic bit allocation. Experimental results show that, powered by the proposed entropy model, our neural codec can achieve 18.2% bitrate saving on UVG dataset when compared with H.266 (VTM) using the highest compression ratio configuration. It makes a new milestone in the development of neural video codec. The codes are at https://github.com/microsoft/DCVC.<br/></div> &copy; 2022 ACM.},
  url       = {http://dx.doi.org/10.1145/3503161.3547845},
  journal   = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia}
}

@inproceedings{2022_ACMMM_Gao_Structure-Preserving,
  language  = {English},
  copyright = {Compendex},
  title     = {Structure-Preserving Motion Estimation for Learned Video Compression},
  booktitle = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia},
  publisher = {ACM},
  author    = {Gao, Han and Cui, Jinzhong and Ye, Mao and Li, Shuai and Zhao, Yu and Zhu, Xiatian},
  year      = {2022},
  pages     = {3055-3063},
  address   = {Lisboa, Portugal},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Following the conventional hybrid video coding framework, existing learned video compression methods rely on the decoded previous frame as the reference for motion estimation considering that it is available to the decoder. Diving into its essential advantage of strong representation capability with CNNs, however, we find this strategy is suboptimal due to two reasons: (1) Motion estimation based on the decoded (often distorted) frame would damage both the spatial structure of motion information inferred and the corresponding residual for each frame, making it difficult to be spatially encoded on the whole image basis using CNNs; (2) Typically, it would break the consistent nature across frames since the estimated motion information is no longer consistent with the movement in the original video due to the distortion in the decoded video, lowering the overall temporal coding efficiency. To overcome these problems, a novel asymmetric Structure-Preserving Motion Estimation (SPME) method is proposed, with the aim to fully explore the ignored original previous frame at the encoder side while complying with the decoded previous frame at the decoder side. Concretely, SPME estimates superior spatially structure-preserving and temporally consistent motion field by aggregating the motion prediction of both the original and the decoded reference frames w.r.t the current frame. Critically, our method can be universally applied to the existing feature prediction based video compression methods. Extensive experiments on several standard test datasets show that our SPME can significantly enhance the state-of-the-art methods.<br/></div> &copy; 2022 ACM.},
  url       = {http://dx.doi.org/10.1145/3503161.3548156},
  journal   = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia}
}

@inproceedings{2021_ICCV_Pourreza_Extending,
  language  = {English},
  copyright = {Compendex},
  title     = {Extending Neural P-frame Codecs for B-frame Coding},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  publisher = {IEEE},
  author    = {Pourreza, Reza and Cohen, Taco},
  year      = {2021},
  pages     = {6660-6669},
  issn      = {15505499},
  address   = {Virtual, Online, Canada},
  abstract  = {<div data-language="eng" data-ev-field="abstract">While most neural video codecs address P-frame coding (predicting each frame from past ones), in this paper we address B-frame compression (predicting frames using both past and future reference frames). Our B-frame solution is based on the existing P-frame methods. As a result, B-frame coding capability can easily be added to an existing neural codec. The basic idea of our B-frame coding method is to interpolate the two reference frames to generate a single reference frame and then use it together with an existing P-frame codec to encode the input B-frame. Our studies show that the interpolated frame is a much better reference for the P-frame codec compared to using the previous frame as is usually done. Our results show that using the proposed method with an existing P-frame codec can lead to 28.5% saving in bit-rate on the UVG dataset compared to the P-frame codec while generating the same video quality.<br/></div> &copy; 2021 IEEE},
  url       = {http://dx.doi.org/10.1109/ICCV48922.2021.00661},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision}
}

@inproceedings{2020_AAAI_Liu_Learned,
  language  = {English},
  copyright = {Compendex},
  title     = {Learned video compression via joint spatial-temporal correlation exploration},
  booktitle = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
  publisher = {AAAI},
  author    = {Liu, Haojie and Shen, Han and Huang, Lichao and Lu, Ming and Chen, Tong and Ma, Zhan},
  year      = {2020},
  pages     = {11580-11587},
  address   = {New York, NY, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Traditional video compression technologies have been developed over decades in pursuit of higher coding efficiency. Efficient temporal information representation plays a key role in video coding. Thus, in this paper, we propose to exploit the temporal correlation using both first-order optical flow and second-order flow prediction. We suggest an one-stage learning approach to encapsulate flow as quantized features from consecutive frames which is then entropy coded with adaptive contexts conditioned on joint spatial-temporal priors to exploit second-order correlations. Joint priors are embedded in autoregressive spatial neighbors, co-located hyper elements and temporal neighbors using ConvLSTM recurrently. We evaluate our approach for the low-delay scenario with High-Efficiency Video Coding (H.265/HEVC), H.264/AVC and another learned video compression method, following the common test settings. Our work offers the state-of-theart performance, with consistent gains across all popular test sequences.<br/></div> Copyright &copy; 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  url       = {https://doi.org/10.1609/aaai.v34i07.6825},
  journal   = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence}
}

@inproceedings{2019_NIPS_Han_Deep,
  language  = {English},
  copyright = {Compendex},
  title     = {Deep generative video compression},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {NIPS},
  author    = {Han, Jun and Lombardo, Salvator and Schroers, Christopher and Mandt, Stephan},
  volume    = {32},
  year      = {2019},
  issn      = {10495258},
  address   = {Vancouver, BC, Canada},
  abstract  = {The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy. Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video. Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression. The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE. Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved when training the model on specialized content.<br/> &copy; 2019 Neural information processing systems foundation. All rights reserved.},
  journal   = {Advances in Neural Information Processing Systems}
}

@inproceedings{2022_ICIP_Liu_Learned,
  title        = {Learned Video Compression With Residual Prediction And Feature-Aided Loop Filter},
  author       = {Liu, Chao and Sun, Heming and Zeng, Xiaoyang and Fan, Yibo},
  booktitle    = {2022 IEEE International Conference on Image Processing (ICIP)},
  pages        = {1321-1325},
  year         = {2022},
  publisher    = {IEEE},
  organization = {IEEE},
  address      = {Bordeaux, France},
  url          = {http://dx.doi.org/10.1109/ICIP46576.2022.9897989}
}

@inproceedings{2022_CVPR_Wodlinger_SASIC,
  language  = {English},
  copyright = {Compendex},
  title     = {SASIC: Stereo Image Compression with Latent Shifts and Stereo Attention},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Wodlinger, Matthias and Kotera, Jan and Xu, Jan and Sablatnig, Robert},
  volume    = {2022-June},
  year      = {2022},
  pages     = {651-660},
  issn      = {10636919},
  publisher = {IEEE},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">We propose a learned method for stereo image compression that leverages the similarity of the left and right images in a stereo pair due to overlapping fields of view. The left image is compressed by a learned compression method based on an autoencoder with a hyperprior entropy model. The right image uses this information from the previously encoded left image in both the encoding and decoding stages. In particular, for the right image, we encode only the residual of its latent representation to the optimally shifted latent of the left image. On top of that, we also employ a stereo attention module to connect left and right images during decoding. The performance of the proposed method is evaluated on two benchmark stereo image datasets (Cityscapes and InStereo2K) and outperforms previous stereo image compression methods while being significantly smaller in model size.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.00074},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2023_CVPR_Jiang_Adaptive,
  title     = {Adaptive Human-Centric Video Compression for Humans and Machines},
  author    = {Jiang, Wei and Choi, Hyomin and Racape, Fabien},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {1121-1129},
  year      = {2023},
  address   = {Vancouver, Canada},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher = {IEEE}
}

@article{2023_arXiv_Li_You,
  title     = {You Can Mask More For Extremely Low-Bitrate Image Compression},
  author    = {Li, Anqi and Li, Feng and Han, Jiaxin and Bai, Huihui and Cong, Runmin and Zhang, Chunjie   and Wang, Meng and Lin, Weisi and Zhao, Yao},
  booktitle = {arXiv preprint arXiv:2306.15561},
  year      = {2023},
  journal   = {arXiv preprint arXiv:2306.15561}
}

@inproceedings{2022_CVPR_Lei_Deep,
  language  = {English},
  copyright = {Compendex},
  title     = {Deep Stereo Image Compression via Bi-directional Coding},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  author    = {Lei, Jianjun and Liu, Xiangrui and Peng, Bo and Jin, Dengchao and Li, Wanqing and Gu, Jingxiao},
  volume    = {2022-June},
  year      = {2022},
  publisher = {IEEE},
  pages     = {19637-19646},
  issn      = {10636919},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Existing learning-based stereo compression methods usually adopt a unidirectional approach to encoding one image independently and the other image conditioned upon the first. This paper proposes a novel bidirectional coding-based end-to-end stereo image compression network (BCSIC-Net). BCSIC-Net consists of a novel bidirectional contextual transform module which performs nonlinear transform conditioned upon the inter-view context in a latent space to reduce inter-view redundancy, and a bidirectional conditional entropy model that employs interview correspondence as a conditional prior to improve coding efficiency. Experimental results on the InStereo2K and KITTI datasets demonstrate that the proposed BCSIC-Net can effectively reduce the inter-view redundancy and out-performs state-of-the-art methods.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPR52688.2022.01905},
  journal   = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition}
}

@inproceedings{2022_ACMMM_Zhai_Disparity-based,
  language  = {English},
  copyright = {Compendex},
  title     = {Disparity-based Stereo Image Compression with Aligned Cross-View Priors},
  booktitle = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia},
  author    = {Zhai, Yongqi and Tang, Luyang and Ma, Yi and Peng, Rui and Wang, Ronggang},
  year      = {2022},
  pages     = {2351-2360},
  address   = {Lisboa, Portugal},
  publisher = {ACM},
  abstract  = {<div data-language="eng" data-ev-field="abstract">With the wide application of stereo images in various fields, the research on stereo image compression (SIC) attracts extensive attention from academia and industry. The core of SIC is to fully explore the mutual information between the left and right images and reduce redundancy between views as much as possible. In this paper, we propose DispSIC, an end-to-end trainable deep neural network, in which we jointly train a stereo matching model to assist in the image compression task. Based on the stereo matching results (i.e. disparity), the right image can be easily warped to the left view, and only the residuals between the left and right views are encoded for the left image. A three-branch auto-encoder architecture is adopted in DispSIC, which encodes the right image, the disparity map and the residuals respectively. During training, the whole network can learn how to adaptively allocate bitrates to these three parts, achieving better rate-distortion performance at the cost of a lower disparity map bitrates. Moreover, we propose a conditional entropy model with aligned cross-view priors for SIC, which takes the warped latents of the right image as priors to improve the accuracy of the probability estimation for the left image. Experimental results demonstrate that our proposed method achieves superior performance compared to other existing SIC methods on the KITTI and InStereo2K datasets both quantitatively and qualitatively.<br/></div> &copy; 2022 ACM.},
  url       = {http://dx.doi.org/10.1145/3503161.3548136},
  journal   = {MM 2022 - Proceedings of the 30th ACM International Conference on Multimedia}
}

@inproceedings{2022_MMSP_Marie_Video,
  language  = {English},
  copyright = {Compendex},
  title     = {Video Coding for Machines: Large-Scale Evaluation of Deep Neural Networks Robustness to Compression Artifacts for Semantic Segmentation},
  booktitle = {2022 IEEE 24th International Workshop on Multimedia Signal Processing, MMSP 2022},
  author    = {Marie, Alban and Desnos, Karol and Morin, Luce and Zhang, Lu},
  year      = {2022},
  address   = {Shanghai, China},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In the Video Coding for Machines (VCM) context where visual content is compressed before being transmitted to a vision task algorithm, appropriate trade-off between the compression level and the vision task performance must be chosen. In this paper, a Deep Neural Networks (DNN) based semantic segmentation algorithm robustness to compression artifacts is evaluated with a total of 1486 different coding configurations. Results indicate the importance of using an appropriate image resolution to overcome the block-partitioning limitations in existing compression algorithms, allowing 58.3%, 49.8%, 33.5% and 24.3% bitrate savings at equivalent prediction accuracy for JPEG, JM, x265 and VVenC, respectively. Surprisingly, JPEG can achieve 73.41% bitrate reduction with the inclusion of compressed images at training time over VVC Test Model (VTM) with a DNN trained on pristine data, which implies that DNN generalization ability must not be overlooked.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/MMSP55362.2022.9949999},
  journal   = {2022 IEEE 24th International Workshop on Multimedia Signal Processing, MMSP 2022},
  publisher = {IEEE}
}

@inproceedings{2022_MMSP_Choi_Scalable,
  language  = {English},
  copyright = {Compendex},
  title     = {Scalable Video Coding for Humans and Machines},
  booktitle = {2022 IEEE 24th International Workshop on Multimedia Signal Processing, MMSP 2022},
  author    = {Choi, Hyomin and Bajic, Ivan V.},
  year      = {2022},
  address   = {Shanghai, China},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Video content is watched not only by humans, but increasingly also by machines. For example, machine learning models analyze surveillance video for security and traffic moni-toring, search through YouTube videos for inappropriate content, and so on. In this paper, we propose a scalable video coding framework that supports machine vision (specifically, object detection) through its base layer bitstream and human vision via its enhancement layer bitstream. The proposed framework includes components from both conventional and Deep Neural Network (DNN)-based video coding. The results show that on object detection, the proposed framework achieves 13-19% bit savings compared to state-of-the-art video codecs, while remaining competitive in terms of MS-SSIM on the human vision task.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/MMSP55362.2022.9949576},
  journal   = {2022 IEEE 24th International Workshop on Multimedia Signal Processing, MMSP 2022},
  publisher = {IEEE}
}

@article{2022_arXiv_Makowiak_Video,
  language  = {English},
  copyright = {Compendex},
  title     = {Video Coding for Machines: Partial transmission of SIFT features},
  booktitle = {arXiv preprint arXiv:2201.02689},
  author    = {Makowiak, Slawomir and Domanski, Marek and Roek, Slawomir and Cywiski, Dominik and Szkielda, Jakub},
  year      = {2022},
  issn      = {23318422},
  abstract  = {<div data-language="eng" data-ev-field="abstract">The paper deals with Video Coding for Machines that is a new paradigm in video coding related to consumption of decoded video by humans and machines. For such tasks, joint transmission of compressed video and features is considered. In this paper, we focus our considerations of features on SIFT keypoints. They can be extracted from the decoded video with losses in number of keypoints and their parameters as compared to the SIFT keypoints extracted from the original video. Such losses are studied for HEVC and VVC as functions of the quantization parameter and the bitrate. In the paper, we propose to transmit the residual feature data together with the compressed video. Therefore, even for strongly compressed video, the transmission of whole all SIFT keypoint information is avoided.<br/></div> Copyright &copy; 2022, The Authors. All rights reserved.},
  journal   = {arXiv preprint arXiv:2201.02689}
}

@inproceedings{2022_CVPRW_Wang_Perceptual,
  language  = {English},
  copyright = {Compendex},
  title     = {Perceptual in-Loop Filter for Image and Video Compression},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  author    = {Wang, Huairui and Ren, Guangjie and Ouyang, Tong and Zhang, Junxi and Han, Wenwei and Liu, Zizheng and Chen, Zhenzhong},
  volume    = {2022-June},
  year      = {2022},
  pages     = {1769-1772},
  issn      = {21607508},
  address   = {New Orleans, LA, United states},
  abstract  = {<div data-language="eng" data-ev-field="abstract">In this paper, we introduce our hybrid image and video compression scheme enhanced by CNN-optimized in-loop filter. Specifically, a Structure Preserving in-Loop Filter (SPiLF) is incorporated in the hybrid video codec Enhanced Compression Model (ECM), where two branches, i.e., gradient branch and pixel branch, are developed based on the dense residual unit (DRU). To provide pleasant visual quality, the Generative adversarial networks (GAN) loss and LPIPS loss are further considered. Therefore, the proposal is mainly focusing on perceptual-friendly image compression for human vision, whilst video compression could be further investigated. The experiments show that the proposed method achieves advanced visual quality when compared to the traditional methods.<br/></div> &copy; 2022 IEEE.},
  url       = {http://dx.doi.org/10.1109/CVPRW56347.2022.00188},
  journal   = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  publisher = {IEEE}
}

@article{2022_TOM_Yi_Task-Driven,
  language  = {English},
  copyright = {Compendex},
  title     = {Task-Driven Video Compression for Humans and Machines: Framework Design and Optimization},
  booktitle = {IEEE Transactions on Multimedia},
  author    = {Yi, Xiaokai and Wang, Hanli and Kwong, Sam and Kuo, C.-C. Jay},
  year      = {2022},
  pages     = {1-12},
  issn      = {15209210},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Learned video compression has developed rapidly and achieved impressive progress in recent years. Despite efficient compression performance, existing signal fidelity oriented or semantic fidelity oriented video compression methods limit the capability to meet the requirements of both machine and human vision. To address this problem, a task-driven video compression framework is proposed to flexibly support vision tasks for both human vision and machine vision. Specifically, to improve the compression performance, the backbone of the video compression framework is optimized by using three novel modules, including multi-scale motion estimation, multi-frame feature fusion, and reference based in-loop filters. Then, based on the proposed efficient compression backbone, a task-driven optimization approach is designed to achieve the trade-off between signal fidelity oriented compression and semantic fidelity oriented compression. Moreover, a post-filter module is employed for the framework to further improve the performance of the human vision branch. Finally, rate-distortion performance, rate-accuracy performance, and subjective quality are employed as the evaluation metrics, and experimental results show the superiority of the proposed framework for both human vision and machine vision. The source code of this work can be found in &lt;uri&gt;https://mic.tongji.edu.cn&lt;/uri&gt;.<br/></div> IEEE},
  url       = {http://dx.doi.org/10.1109/TMM.2022.3233245},
  journal   = {IEEE Transactions on Multimedia},
  publisher = {IEEE}
}

@inproceedings{2023_arXiv_Jiang_Multi-Modality,
  language  = {English},
  copyright = {Compendex},
  title     = {Multi-Modality Deep Network for Extreme Learned Image Compression},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author    = {Jiang, Xuhao and Tan, Weimin and Tan, Tian and Yan, Bo and Shen, Liquan},
  volume    = {37},
  number    = {1},
  pages     = {1033-1041},
  year      = {2023},
  issn      = {23318422},
  publisher = {AAAI press},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Image-based single-modality compression learning approaches have demonstrated exceptionally powerful encoding and decoding capabilities in the past few years, but suffer from blur and severe semantics loss at extremely low bitrates. To address this issue, we propose a multimodal machine learning method for text-guided image compression, in which the semantic information of text is used as prior information to guide image compression for better compression performance. We fully study the role of text description in different components of the codec, and demonstrate its effectiveness. In addition, we adopt the image-text attention module and image-request complement module to better fuse image and text features, and propose an improved multimodal semantic-consistent loss to produce semantically complete reconstructions. Extensive experiments, including a user study, prove that our method can obtain visually pleasing results at extremely low bitrates, and achieves a comparable or even better performance than state-of-the-art methods, even though these methods are at 2&times; to 4&times; bitrates of ours.<br/></div> Copyright &copy; 2023, The Authors. All rights reserved.},
  url       = {http://dx.doi.org/10.48550/arXiv.2304.13583},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence}
}

@article{2023_JSAC_Huang_Toward,
  language  = {English},
  copyright = {Compendex},
  title     = {Toward Semantic Communications: Deep Learning-Based Image Semantic Coding},
  booktitle = {IEEE Journal on Selected Areas in Communications},
  author    = {Huang, Danlan and Gao, Feifei and Tao, Xiaoming and Du, Qiyuan and Lu, Jianhua},
  volume    = {41},
  number    = {1},
  year      = {2023},
  pages     = {55-71},
  issn      = {07338716},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Semantic communications has received growing interest since it can remarkably reduce the amount of data to be transmitted without missing critical information. Most existing works explore the semantic encoding and transmission for text and apply techniques in Natural Language Processing (NLP) to interpret the meaning of the text. In this paper, we conceive the semantic communications for image data that is much more richer in semantics and bandwidth sensitive. We propose an reinforcement learning based adaptive semantic coding (RL-ASC) approach that encodes images beyond pixel level. Firstly, we define the semantic concept of image data that includes the category, spatial arrangement, and visual feature as the representation unit, and propose a convolutional semantic encoder to extract semantic concepts. Secondly, we propose the image reconstruction criterion that evolves from the traditional pixel similarity to semantic similarity and perce ptual performance. Thirdly, we design a novel RL-based semantic bit allocation model, whose reward is the increase in rate-semantic-perceptual performance after encoding a certain semantic concept with adaptive quantization level. Thus, the task-related information is preserved and reconstructed properly while less important data is discarded. Finally, we propose the Generative Adversarial Nets (GANs) based semantic decoder that fuses both locally and globally features via an attention module. Experimental results demonstrate that the proposed RL-ASC is noise robust and could reconstruct visually pleasant and semantic consistent image in low bit rate condition.<br/></div> &copy; 1983-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/JSAC.2022.3221999},
  journal   = {IEEE Journal on Selected Areas in Communications},
  publisher = {IEEE}
}

@article{2021_TOM_Yang_Towards,
  language  = {English},
  copyright = {Compendex},
  title     = {Towards Coding for Human and Machine Vision: Scalable Face Image Coding},
  booktitle = {IEEE Transactions on Multimedia},
  author    = {Yang, Shuai and Hu, Yueyu and Yang, Wenhan and Duan, Ling-Yu and Liu, Jiaying},
  volume    = {23},
  year      = {2021},
  pages     = {2957-2971},
  issn      = {15209210},
  abstract  = {<div data-language="eng" data-ev-field="abstract">The past decades have witnessed the rapid development of image and video coding techniques in the era of big data. However, the signal fidelity-driven coding pipeline design limits the capability of the existing image/video coding frameworks to fulfill the needs of both machine and human vision. In this paper, we come up with a novel face image coding framework by leveraging both the compressive and the generative models, to support machine vision and human perception tasks jointly. Given an input image, the feature analysis is first applied, and then the generative model is employed to reconstruct image with compact structure and color features, where sparse edges are extracted to connect both kinds of vision and a key reference pixel selection method is proposed to determine the priorities of the reference color pixels for scalable coding. The compact edge map serves as the basic layer for machine vision tasks, and the reference pixels act as an enhanced layer to guarantee signal fidelity for human vision. By introducing advanced generative models, we train a decoding network to reconstruct images from compact structure and color representations, which is flexible to accept inputs in a scalable way and to control the imagery effect of the outputs between signal fidelity and visual realism. Experimental results and comprehensive performance analysis over the face image dataset demonstrate the superiority of our framework in both human vision tasks and machine vision tasks, which provide useful evidence on the emerging standardization efforts on MPEG VCM (Video Coding for Machine).<br/></div> &copy; 1999-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/TMM.2021.3068580},
  journal   = {IEEE Transactions on Multimedia}
}

@inproceedings{20_LNCS_Liu_Conditional,
  language  = {English},
  copyright = {Compendex},
  title     = {Conditional Entropy Coding for Efficient Video Compression},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  author    = {Liu, Jerry and Wang, Shenlong and Ma, Wei-Chiu and Shah, Meet and Hu, Rui and Dhawan, Pranaab and Urtasun, Raquel},
  volume    = {12362 LNCS},
  year      = {2020},
  pages     = {453-468},
  issn      = {03029743},
  address   = {Glasgow, United kingdom},
  abstract  = {We propose a very simple and efficient video compression framework that only focuses on modeling the conditional entropy between frames. Unlike prior learning-based approaches, we reduce complexity by not performing any form of explicit transformations between frames and assume each frame is encoded with an independent state-of-the-art deep image compressor. We first show that a simple architecture modeling the entropy between the image latent codes is as competitive as other neural video compression works and video codecs while being much faster and easier to implement. We then propose a novel internal learning extension on top of this architecture that brings an additional &sim; 10% bitrate savings without trading off decoding speed. Importantly, we show that our approach outperforms H.265 and other deep learning baselines in MS-SSIM on higher bitrate UVG video, and against all video codecs on lower framerates, while being thousands of times faster in decoding than deep models utilizing an autoregressive entropy model.<br/> &copy; 2020, Springer Nature Switzerland AG.},
  url       = {http://dx.doi.org/10.1007/978-3-030-58520-4_27},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@inproceedings{2020_SiPS_Pessoa_End-to-end,
  language  = {English},
  copyright = {Compendex},
  title     = {End-to-End Learning of Video Compression using Spatio-Temporal Autoencoders},
  booktitle = {IEEE Workshop on Signal Processing Systems, SiPS: Design and Implementation},
  publisher = {IEEE},
  author    = {Pessoa, Jorge and Aidos, Helena and Tomas, Pedro and Figueiredo, Mario A. T.},
  volume    = {2020-October},
  year      = {2020},
  pages     = {1-6},
  issn      = {15206130},
  address   = {Virtual, Coimbra, Portugal},
  abstract  = {Deep learning (DL) is revolutionizing image and video processing and now holds state-of-the-art performance in many tasks. However, video compression has so far resisted the DL revolution. Current attempts rely on complex solutions, interconnecting multiple networks to mimic the different layers of conventional codecs. Since DL approaches usually excel when the models are allowed to learn their own feature set, a different solution is herein proposed: end-to-end learning of a single network, explicitly avoiding motion estimation/prediction. We formalize it as the rate-distortion optimization of a single spatio-temporal autoencoder, by jointly learning a latent-space projection transform, and a synthesis transform for low-bitrate video compression. The quantizer uses a rounding scheme, relaxed during training, and an entropy estimation technique to enforce an information bottleneck. The obtained video compression network shows competitive performance against standard codecs (MPEG-4 Part 2, H.264/AVC, H.265/HEVC), particularly for low bitrates, even while avoiding the use of any motion prediction/compensation method.<br/> &copy; 2020 IEEE.},
  url       = {http://dx.doi.org/10.1109/SiPS50750.2020.9195249},
  journal   = {IEEE Workshop on Signal Processing Systems, SiPS: Design and Implementation}
}

@inproceedings{2022_ICIP_Chen,
  language  = {English},
  copyright = {Compendex},
  title     = {END-TO-END DEPTH MAP COMPRESSION FRAMEWORK VIA RGB-TO-DEPTH STRUCTURE PRIORS LEARNING},
  booktitle = {Proceedings - International Conference on Image Processing, ICIP},
  publisher = {IEEE},
  author    = {Chen, Minghui and Zhang, Pingping and Chen, Zhuo and Zhang, Yun and Wang, Xu and Kwong, Sam},
  year      = {2022},
  pages     = {3206-3210},
  issn      = {15224880},
  address   = {Bordeaux, France},
  url       = {http://dx.doi.org/10.1109/ICIP46576.2022.9898073},
  journal   = {Proceedings - International Conference on Image Processing, ICIP}
}

@article{2022_TIP_Tang_TSA-SCC,
  language  = {English},
  copyright = {Compendex},
  title     = {TSA-SCC: Text Semantic-Aware Screen Content Coding with Ultra Low Bitrate},
  booktitle = {IEEE Transactions on Image Processing},
  publisher = {IEEE},
  author    = {Tang, Tong and Li, Ling and Wu, Xiaoyu and Chen, Ruizhi and Li, Haochen and Lu, Guo and Cheng, Limin},
  volume    = {31},
  year      = {2022},
  pages     = {2463-2477},
  issn      = {10577149},
  abstract  = {<div data-language="eng" data-ev-field="abstract">Due to the rapid growth of web conferences, remote screen sharing, and online games, screen content has become an important type of internet media information and over 90% of online media interactions are screen based. Meanwhile, as the main component in the screen content, textual information averagely takes up over 40% of the whole image on various commonly used screen content datasets. However, it is difficult to compress the textual information by using the traditional coding schemes as HEVC, which assumes strong spatial and temporal correlations within the image/video. State-of-the-art screen content coding (SCC) standard as HEVC-SCC still adopts a block-based coding framework and does not consider the text semantics for compression, thus inevitably blurring texts at a lower bitrate. In this paper, we propose a general text semantic-aware screen content coding scheme (TSA-SCC) for ultra low bitrate setting. This method detects the abrupt picture in a screen content video (or image), recognizes textual information (including word, position, font type, font size and font color) in the abrupt picture based on neural networks, and encodes texts with text coding tools. The other pictures as well as the background image after removing texts from the abrupt picture via inpainting, are encoded with HEVC-SCC. Compared with HEVC-SCC, the proposed method TSA-SCC reduces bitrate by up to 3 &times; at a similar compression quality. Moreover, TSA-SCC achieves much better visual quality with less bitrate consumption when encoding the screen content video/image at ultra low bitrates.<br/></div> &copy; 1992-2012 IEEE.},
  url       = {http://dx.doi.org/10.1109/TIP.2022.3152003},
  journal   = {IEEE Transactions on Image Processing}
}

